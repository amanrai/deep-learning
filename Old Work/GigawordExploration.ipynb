{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T03:50:16.940225Z",
     "start_time": "2019-04-25T03:50:16.935882Z"
    }
   },
   "outputs": [],
   "source": [
    "_articles = \"../../../Data/sumdata/train/train.article.txt\"\n",
    "_titles = \"../../../Data/sumdata/train/train.title.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T03:50:21.201873Z",
     "start_time": "2019-04-25T03:50:17.455607Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = []\n",
    "titles = []\n",
    "with open(_articles, \"r\") as f:\n",
    "    articles = f.read().split(\"\\n\")\n",
    "with open(_titles, \"r\") as f:\n",
    "    titles = f.read().split(\"\\n\")\n",
    "print(len(articles), len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T08:22:14.625520Z",
     "start_time": "2019-04-25T08:22:13.621791Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import numpy as np\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "cls_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"])[0]\n",
    "print(cls_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T03:58:22.827635Z",
     "start_time": "2019-04-25T03:58:22.824109Z"
    }
   },
   "outputs": [],
   "source": [
    "max_doc_len = 100\n",
    "max_sum_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T08:43:49.321097Z",
     "start_time": "2019-04-25T08:43:49.314479Z"
    }
   },
   "outputs": [],
   "source": [
    "dps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T08:57:59.283776Z",
     "start_time": "2019-04-25T08:43:56.772505Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(articles[:1000000])):\n",
    "    _article = None\n",
    "    _summary = None\n",
    "    _article_ids = None\n",
    "    _summary_ids = None\n",
    "    _article = tokenizer.tokenize(articles[i])\n",
    "    _summary = tokenizer.tokenize(titles[i])\n",
    "    _article_ids = tokenizer.convert_tokens_to_ids(_article)\n",
    "    _summary_ids = tokenizer.convert_tokens_to_ids(_summary)\n",
    "    _article_ids.insert(0, cls_token)\n",
    "    while (len(_article_ids) < max_doc_len):\n",
    "        _article_ids.append(0)\n",
    "        \n",
    "    _article_ids = _article_ids[:max_doc_len]\n",
    "    while (len(_summary_ids) < max_sum_len):\n",
    "        _summary_ids.append(0)\n",
    "    _summary_ids = _summary_ids[:max_sum_len]\n",
    "    pointers = []\n",
    "    for k in range(len(_summary)):\n",
    "        token = _summary[k]\n",
    "        if (any(char.isdigit() for char in token)):\n",
    "            pointers.append(k)\n",
    "    dp = {\n",
    "        \"story\":_article,\n",
    "        \"summary\":_summary,\n",
    "        \"story_tokens\":_article_ids,\n",
    "        \"summary_tokens\":_summary_ids,\n",
    "        \"pointers\":pointers\n",
    "    }\n",
    "    dps.append(dp)\n",
    "    _str = str(i+1) + \"/\" + str(len(articles))\n",
    "    print(_str, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T08:58:18.425296Z",
     "start_time": "2019-04-25T08:58:03.386420Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pprint\n",
    "step = 10\n",
    "\n",
    "f_name = \"../../../Data/sumdata/training_0.pickle\"\n",
    "pickle.dump(dps, open(f_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T08:41:43.548853Z",
     "start_time": "2019-04-25T08:41:30.250226Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(dps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T13:37:36.251121Z",
     "start_time": "2019-04-23T13:37:36.233482Z"
    }
   },
   "outputs": [],
   "source": [
    "def genBatch(bs = 64, max_article_len = 100, max_summary_len = 15):\n",
    "    indices = np.random.randint(0, len(articles), (bs,))\n",
    "    _docs = [tokenizer.tokenize(articles[index]) for index in indices]\n",
    "    _sums = [tokenizer.tokenize(titles[index]) for index in indices]\n",
    "    for doc in _docs:\n",
    "        doc.insert(0, \"[CLS]\")\n",
    "        doc.append(\"[SEP]\")\n",
    "        while (len(doc) < max_doc_len):\n",
    "            doc.append(\"[PAD]\")\n",
    "        doc = doc[:max_doc_len]\n",
    "    \n",
    "    copy = []\n",
    "    for summary in _sums:\n",
    "        doc = summary\n",
    "        while (len(doc) < max_sum_len):\n",
    "            doc.append(\"[PAD]\")\n",
    "        doc = doc[:max_sum_len]\n",
    "    \n",
    "    _docs = [tokenizer.convert_tokens_to_ids(doc) for doc in _docs]\n",
    "    _sums = [tokenizer.convert_tokens_to_ids(summary) for summary in _sums]\n",
    "        \n",
    "genBatch(bs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
