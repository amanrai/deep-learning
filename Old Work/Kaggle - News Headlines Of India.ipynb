{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#News Headlines of India Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Attribution:\n",
    "    https://www.kaggle.com/therohk/india-headlines-news-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {}\n",
    "with open(\"./india-news-headlines.csv\", \"r\", encoding=\"utf8\") as f:\n",
    "    text = f.read()\n",
    "    lines = text.split(\"\\n\")\n",
    "    print(len(lines[1:-1]))\n",
    "    for line in lines[1:-1]:\n",
    "        parts = line.split(\",\")\n",
    "        if (parts[1] not in categories):\n",
    "            categories[parts[1]] = []\n",
    "        categories[parts[1]].append(\", \".join(parts[2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim below (3 and 4) is to roll up sub-categories into broader categories. May be better to do this by hand eventually... should provide a better output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "final_cats = {}\n",
    "for key in categories:\n",
    "    arr.append((key, len(key.split(\".\"))))\n",
    "    \n",
    "arr = sorted(arr, key = lambda x:x[1])\n",
    "\n",
    "for i in range(len(arr)):\n",
    "    if (arr[i][1] < 3):\n",
    "        final_cats[arr[i][0]] = []\n",
    "\n",
    "for i in range(len(arr)):\n",
    "    if (arr[i][1] >= 3):\n",
    "        parts = arr[i][0].split(\".\")\n",
    "        base_phrase = \".\".join(parts[0:2])\n",
    "        num_found = 0\n",
    "        for k in range(i+1, len(arr)):\n",
    "            if (arr[k][1] >= 3 and k != i):\n",
    "                if (base_phrase in arr[k][0]):\n",
    "                    num_found += 1\n",
    "        if (num_found > 0):\n",
    "            if (base_phrase not in final_cats):\n",
    "                final_cats[base_phrase] = []\n",
    "            final_cats[base_phrase].append(arr[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exclude = [\"city\", \"times\", \"unknown\", \"removed\", \"cities\", \"top-headlines\", \"top-stories\"]\n",
    "count = 0\n",
    "redo = True\n",
    "while (redo):\n",
    "    redo = False\n",
    "    del_list = {}\n",
    "    for key in final_cats:\n",
    "        include = True\n",
    "        for i in range(len(exclude)):\n",
    "            if (exclude[i] in key):\n",
    "                include = False\n",
    "        if (include):\n",
    "            total = 0\n",
    "            if (key in categories):\n",
    "                total += len(categories[key])\n",
    "            if (len(final_cats[key]) > 0):\n",
    "                for i in range(len(final_cats[key])):\n",
    "                    if (final_cats[key][i] in categories):\n",
    "                        total += len(categories[final_cats[key][i]])\n",
    "            if (total > 5000):\n",
    "                count += 1\n",
    "            else:\n",
    "                parts = key.split(\".\")\n",
    "                use_key = parts[0]\n",
    "                if (\"cricket\" in key or \"ipl\" in key or \"icc\" in key): #this is hacky but it's the only way to keep the rest of the code clean\n",
    "                    use_key = \"cricket\"\n",
    "                if (parts[0] in final_cats and len(parts) > 1):\n",
    "                    del_list[key] = use_key\n",
    "                    redo = True\n",
    "    \n",
    "    print(\"Merging:\", len(del_list.keys()))\n",
    "    for key in del_list:\n",
    "        if (del_list[key] not in final_cats):\n",
    "            final_cats[del_list[key]] = []\n",
    "        final_cats[del_list[key]].extend(final_cats[key])\n",
    "        if (key in categories):\n",
    "            final_cats[del_list[key]].append(key)\n",
    "        del final_cats[key]\n",
    "\n",
    "good_cats = []\n",
    "for key in final_cats:\n",
    "    include = True\n",
    "    for i in range(len(exclude)):\n",
    "        if (exclude[i] in key):\n",
    "            include = False\n",
    "    if (include):\n",
    "        total = 0\n",
    "        if (key in categories):\n",
    "            total += len(categories[key])\n",
    "        if (len(final_cats[key]) > 0):\n",
    "            for i in range(len(final_cats[key])):\n",
    "                if (final_cats[key][i] in categories):\n",
    "                    total += len(categories[final_cats[key][i]])\n",
    "        if (total > 2500): #select categories with more than 2500 datapoints\n",
    "            good_cats.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again, hacky but required\n",
    "final_cats[\"cricket\"].append(\"sports.icc-world-cup-2015\")\n",
    "final_cats[\"cricket\"].extend(final_cats[\"sports.icc-world-cup-2015\"])\n",
    "del(good_cats[good_cats.index(\"sports.icc-world-cup-2015\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(good_cats, len(good_cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorg = {}\n",
    "\n",
    "for i in range(len(good_cats)):\n",
    "    key = good_cats[i]\n",
    "    if (key not in reorg):\n",
    "        reorg[key] = []\n",
    "    reorg[key].extend(categories[key])\n",
    "    for sub_key in final_cats[key]:\n",
    "        reorg[key].extend(categories[sub_key])\n",
    "        \n",
    "for key in reorg:\n",
    "    print(key, len(reorg[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_distribution = categories\n",
    "categories = reorg\n",
    "good_categories = list(categories.keys())\n",
    "num_cat = len(good_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def processHeadline_One(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_phrase = 0\n",
    "word_lengths = []\n",
    "vocab = {}\n",
    "for i in range(len(good_categories)):\n",
    "    t_cat = good_categories[i]\n",
    "    for k in range(len(categories[t_cat])):\n",
    "        headline = processHeadline_One(categories[t_cat][k])\n",
    "        words = headline.split(\" \")        \n",
    "        for j in range(len(words)):\n",
    "            if (words[j] not in vocab):\n",
    "                vocab[words[j]] = 0\n",
    "            vocab[words[j]] += 1\n",
    "        word_lengths.append(len(words))\n",
    "print(np.mean(word_lengths), np.percentile(word_lengths, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_arr = sorted([(word, vocab[word]) for word in vocab], key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab = {}\n",
    "rev_vocab = {}\n",
    "for i in range(len(vocab_arr)):\n",
    "    final_vocab[vocab_arr[i][0]] = i\n",
    "    rev_vocab[i] = vocab_arr[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab_arr))\n",
    "print(vocab_arr[75000:75020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dataset\n",
    "vocab_cutoff = 75000\n",
    "alt_words = {}\n",
    "alt_words[\"#UNKNOWN_WORD#\"] = vocab_cutoff + 1\n",
    "alt_words[\"#EOS\"] = vocab_cutoff + 2\n",
    "alt_words[\"#PADDING_WORD#\"] = vocab_cutoff + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_index(word): #returns the lowest possible index for a word, regardless of case\n",
    "    index = alt_words[\"#UNKNOWN_WORD#\"]\n",
    "    if (word in final_vocab):\n",
    "        index = final_vocab[word]\n",
    "    try:\n",
    "        if (final_vocab[word.lower()] < index):\n",
    "            index = final_vocab[word.lower()]\n",
    "        if (final_vocab[word.upper()] < index):\n",
    "            index = final_vocab[word.upper()]\n",
    "        n_word = word[0].upper() + word[1:]\n",
    "        if (final_vocab[n_word] < index):\n",
    "            index = final_vocab[n_word]\n",
    "    except:\n",
    "        if (word in final_vocab):\n",
    "            index = final_vocab[word]\n",
    "    if (index > vocab_cutoff):\n",
    "        index = alt_words[\"#UNKNOWN_WORD#\"]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in range(len(good_categories)):\n",
    "    headlines = categories[good_categories[i]]\n",
    "    for k in range(len(headlines)):\n",
    "        headline = processHeadline_One(headlines[k])\n",
    "        indices = []\n",
    "        words = headline.split(\" \")\n",
    "        if (len(words) <= 14):\n",
    "            for x in range(len(words)):\n",
    "                indices.append(word_index(words[x]))\n",
    "            indices.append(alt_words[\"#EOS\"])\n",
    "            while (len(indices) < 15):\n",
    "                indices.append(alt_words[\"#PADDING_WORD#\"])\n",
    "            dataset.append((indices,i))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "dataset = np.array(dataset)\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, GRU, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input((15,))\n",
    "embedding = Embedding(75005, 128)(input_layer)\n",
    "gru_1 = LSTM(128)(embedding)\n",
    "dropout = Dropout(0.1)(gru_1)\n",
    "d1 = Dense(128, activation=\"tanh\")(dropout)\n",
    "d2 = Dense(64, activation=\"relu\")(d1)\n",
    "output = Dense(num_cat, activation=\"sigmoid\")(d2) #sigmoid used here to allow for extraction of multiple topics\n",
    "\n",
    "model = Model(input_layer, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics = [\"categorical_accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.1\n",
    "val_cutoff = int(validation_split * len(dataset))\n",
    "train = dataset[val_cutoff:]\n",
    "validate = dataset[:val_cutoff]\n",
    "\n",
    "x_train = [train[i][0] for i in range(len(train))]\n",
    "y_train = [train[i][1] for i in range(len(train))]\n",
    "\n",
    "x_val = [validate[i][0] for i in range(len(validate))]\n",
    "y_val = [validate[i][1] for i in range(len(validate))]\n",
    "\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 50000\n",
    "num_full_epochs = 4\n",
    "for k in range(num_full_epochs): \n",
    "    print(\"Restarting on full dataset...\", k)\n",
    "    for i in range(0, 750000, step):\n",
    "        print(\"epoch:\", k, \",\", i, \":\", i+step)\n",
    "        x = [] \n",
    "        y = []\n",
    "        for j in range(i, i+step):\n",
    "            x.append(np.asarray(x_train[j]))\n",
    "            arr = np.zeros(num_cat, dtype=np.int8)\n",
    "            arr[y_train[j]] = 1\n",
    "            y.append(arr)\n",
    "        model.fit(np.asarray(x), np.asarray(y), batch_size=128, validation_split=0.1)\n",
    "    print(\"\\n\\n\\n\\n Predicting:\")\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "    for i in range(0, len(x_val)):\n",
    "        x_.append(np.asarray(x_val[i]))\n",
    "    output = model.predict(np.asarray(x_), verbose=True)\n",
    "    indices = np.argmax(output, axis=1)\n",
    "    acc = 0\n",
    "    for i in range(len(indices)):\n",
    "        if (indices[i] == y_val[i]):\n",
    "            acc += 1\n",
    "    accuracy = acc/len(indices)\n",
    "    if (accuracy > val_acc):\n",
    "        print(\"\\nValidation Accuracy went up to:\", accuracy, \"\\nSaving model.\\n\\n\\n\\n\")\n",
    "        model.save(\"./HeadlinesOfIndia.h5\")\n",
    "        val_acc = accuracy\n",
    "    else:\n",
    "        print(\"\\nNew validation accuracy is:\", accuracy, \"\\nNot saving.\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(x_val)):\n",
    "    x_.append(np.asarray(x_val[i]))\n",
    "output = model.predict(np.asarray(x_), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rev_vocab[75001] = \"#UNKNOWN_WORD#\"\n",
    "rev_vocab[75002] = \"#EOS\"\n",
    "for i in range(7000, 8000): #len(x_val)):\n",
    "    sent = x_val[i]\n",
    "    sent_str = \"\"\n",
    "    for word in sent:\n",
    "        if (word < 75002):\n",
    "            sent_str += rev_vocab[word] + \" \"\n",
    "    cats = output[i]\n",
    "    scores = sorted([(i,cats[i]) for i in range(len(cats))], key=lambda x: x[1], reverse=True)\n",
    "    potential_cats = [good_categories[scores[k][0]] for k in range(1)]\n",
    "    potential_cats = \"[ \" + \" ; \".join(potential_cats) + \" ]\"\n",
    "    print(sent_str, potential_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the full \"Unknown\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "test_categories = [\"unknown\"]\n",
    "for i in range(len(test_categories)):\n",
    "    headlines = original_distribution[test_categories[i]]\n",
    "    for k in range(len(headlines)):\n",
    "        headline = processHeadline_One(headlines[k])\n",
    "        indices = []\n",
    "        words = headline.split(\" \")\n",
    "        if (len(words) <= 14):\n",
    "            for x in range(len(words)):\n",
    "                indices.append(word_index(words[x]))\n",
    "            indices.append(alt_words[\"#EOS\"])\n",
    "            while (len(indices) < 15):\n",
    "                indices.append(alt_words[\"#PADDING_WORD#\"])\n",
    "            test_dataset.append((indices,i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [test_dataset[i][0] for i in range(len(test_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = []\n",
    "for i in range(0, len(x_test)):\n",
    "    x_.append(np.asarray(x_test[i]))\n",
    "output = model.predict(np.asarray(x_), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_categorised = []\n",
    "\n",
    "rev_vocab[75001] = \"#UNKNOWN_WORD#\"\n",
    "rev_vocab[75002] = \"#EOS\"\n",
    "for i in range(len(x_test)):\n",
    "    sent = x_test[i]\n",
    "    sent_str = \"\"\n",
    "    for word in sent:\n",
    "        if (word < 75002):\n",
    "            sent_str += rev_vocab[word] + \" \"\n",
    "    new_headline = {}\n",
    "    new_headline[\"headline\"] = sent_str\n",
    "\n",
    "    cats = output[i]\n",
    "    scores = sorted([(i,cats[i]) for i in range(len(cats))], key=lambda x: x[1], reverse=True)\n",
    "    potential_cats = [good_categories[scores[k][0]] for k in range(3)]\n",
    "    new_headline[\"categories\"] = potential_cats\n",
    "    unknown_categorised.append(new_headline)\n",
    "    #potential_cats = \"[ \" + \" ; \".join(potential_cats) + \" ]\"\n",
    "    #print(sent_str, potential_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json_str = json.dumps(unknown_categorised, indent=4)\n",
    "with open(\"./testing_category_unknown.json\", \"w\") as f:\n",
    "    json.dump(unknown_categorised, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
