{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:05.131649Z",
     "start_time": "2019-05-02T07:28:03.912520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:36.753223Z",
     "start_time": "2019-05-02T07:28:05.133256Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"../../training_processed.pickle\"\n",
    "all_data = pickle.load(open(data_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:36.769043Z",
     "start_time": "2019-05-02T07:28:36.754938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804874 144334 1660540\n"
     ]
    }
   ],
   "source": [
    "print(len(all_data[\"all_data\"]), len(all_data[\"positives\"]), len(all_data[\"negatives\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:36.833200Z",
     "start_time": "2019-05-02T07:28:36.770937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1744979, 0), (1707199, 0), (1797973, 0), (1727052, 0), (1635407, 0), (1770395, 0), (1740143, 0), (1709795, 0), (1685769, 0), (1740324, 0), (1678068, 0), (1755983, 0), (1721615, 0), (1689482, 0), (1742046, 0), (1783474, 0), (1743790, 0), (1692815, 0), (1774693, 0), (1754721, 0), (1761517, 0), (1730297, 1), (1705046, 0), (1707279, 0), (1641161, 0), (1658610, 0), (1734347, 0), (1674092, 0), (1624932, 0), (1783277, 0), (1662468, 0), (1683425, 0), (1670370, 0), (1636440, 0), (1742690, 0), (1695392, 1), (1724103, 1), (1722665, 0), (1659128, 0), (1647415, 0), (1633997, 0), (1701854, 0), (1793552, 0), (1708990, 0), (1692282, 0), (1778700, 0), (1795208, 0), (1717619, 0), (1778704, 0), (1753256, 0), (1726655, 1), (1689093, 0), (1747679, 0), (1782379, 0), (1752245, 0), (1734384, 0), (1675771, 0), (1649371, 1), (1718757, 0), (1664910, 1), (1734367, 0), (1695018, 0), (1749653, 0), (1719364, 0), (1700847, 0), (1772179, 0), (1681930, 0), (1662210, 0), (1664322, 0), (1774645, 0), (1662349, 0), (1677303, 0), (1738624, 0), (1771361, 0), (1754629, 0), (1685455, 0), (1785183, 0), (1764229, 0), (1701952, 1), (1729781, 0), (1796222, 0), (1639361, 0), (1803488, 0), (1798887, 0), (1631750, 0), (1677618, 0), (1767823, 0), (1715730, 0), (1643995, 0), (1700381, 0), (1733104, 1), (1740379, 0), (1746180, 0), (1704476, 0), (1798997, 0), (1635935, 0), (1674165, 0), (1788352, 0), (1780143, 0), (1716735, 0)]\n"
     ]
    }
   ],
   "source": [
    "positive_cutoff = len(all_data[\"positives\"])//10\n",
    "negative_cutoff = len(all_data[\"negatives\"])//10\n",
    "\n",
    "testing_positives = all_data[\"positives\"][-positive_cutoff:]\n",
    "testing_negatives = all_data[\"negatives\"][-negative_cutoff:]\n",
    "\n",
    "testing_data = [(index, 1) for index in testing_positives]\n",
    "testing_data.extend([(index, 0) for index in testing_negatives])\n",
    "\n",
    "np.random.shuffle(testing_data)\n",
    "print(testing_data[:100])\n",
    "\n",
    "training_positives = all_data[\"positives\"][:-positive_cutoff]\n",
    "training_negatives = all_data[\"negatives\"][:-negative_cutoff]\n",
    "\n",
    "_data = all_data[\"all_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:36.837504Z",
     "start_time": "2019-05-02T07:28:36.834494Z"
    }
   },
   "outputs": [],
   "source": [
    "max_comment_length = 120 #99th percentile of comment lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:38.785915Z",
     "start_time": "2019-05-02T07:28:36.839876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 120]) torch.Size([12, 120]) torch.Size([12, 120]) torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "def genBatch(bs=8, testing = False, cuda=True):\n",
    "    \n",
    "    _positive_data = training_positives\n",
    "    _negative_data = training_negatives\n",
    "    \n",
    "    _batch = None\n",
    "    if (not testing):\n",
    "        #during training always present a balanced training set\n",
    "        positive_samples = bs//2\n",
    "        negative_samples = bs - positive_samples\n",
    "        _p = np.random.randint(0, len(_positive_data), (positive_samples,))\n",
    "        _n = np.random.randint(0, len(_negative_data), (negative_samples,))\n",
    "\n",
    "        _batch = [(tokenizer.convert_tokens_to_ids(_data[index][\"comment_text\"]), 1) \n",
    "                  for index in _p]\n",
    "        _batch.extend([(tokenizer.convert_tokens_to_ids(_data[index][\"comment_text\"]), 0) \n",
    "                       for index in _n])\n",
    "    else:\n",
    "        _batch = []\n",
    "        _indices = np.random.randint(0, len(testing_data), (bs,))\n",
    "        for list_index in _indices:\n",
    "            _index = testing_data[list_index][0]\n",
    "            _class = testing_data[list_index][1]\n",
    "            _batch.append((tokenizer.convert_tokens_to_ids(_data[_index][\"comment_text\"]), _class))\n",
    "        \n",
    "    np.random.shuffle(_batch)\n",
    "    _docs = [dp[0] for dp in _batch]\n",
    "    _y = [dp[1] for dp in _batch]\n",
    "    \n",
    "    docs = []\n",
    "    for _doc in _docs:\n",
    "        _doc.insert(0, 101)\n",
    "        while (len(_doc) < max_comment_length):\n",
    "            _doc.append(0)\n",
    "        docs.append(_doc[:max_comment_length])\n",
    "        \n",
    "    docs = np.asarray(docs)\n",
    "    segments = np.zeros(docs.shape)\n",
    "    y = np.asarray(_y)\n",
    "    if (cuda):\n",
    "        docs = torch.LongTensor(docs).cuda()\n",
    "        segments = torch.LongTensor(segments).cuda()\n",
    "        y = torch.FloatTensor(y).cuda()\n",
    "    else:\n",
    "        docs = torch.LongTensor(docs)\n",
    "        segments = torch.LongTensor(segments)\n",
    "        y = torch.FloatTensor(y)\n",
    "    mask = docs > 0\n",
    "    \n",
    "    return docs, segments, mask, y\n",
    "\n",
    "d, se, m, y = genBatch(bs=12, testing=False, cuda=True)\n",
    "print(d.size(), se.size(), m.size(), y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:44.181370Z",
     "start_time": "2019-05-02T07:28:44.176066Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, dim=64, bert_model = \"bert-base-uncased\"):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.bert_dim = 768\n",
    "        if (\"large\" in bert_model):\n",
    "            self.bert_dim = 1024\n",
    "        self.w = torch.nn.Linear(self.bert_dim, dim)\n",
    "        self.v = torch.nn.Linear(dim,1)\n",
    "        self.o = torch.nn.Linear(self.bert_dim, dim)\n",
    "    \n",
    "    def forward(self, _d):\n",
    "        _att = torch.tanh(self.w(_d))\n",
    "        _att = self.v(_att)\n",
    "        _att = F.softmax(_att, dim=1)\n",
    "        _o = _d * _att\n",
    "        _o = torch.sum(_o, dim=1)\n",
    "        return self.o(_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:44.195807Z",
     "start_time": "2019-05-02T07:28:44.183539Z"
    }
   },
   "outputs": [],
   "source": [
    "class multiHeadedClassifier(torch.nn.Module):\n",
    "    def __init__(self, attention_heads = 1, \n",
    "                 attention_head_dim=512,\n",
    "                 bert_model = \"bert-base-uncased\",\n",
    "                 output_dims = 1\n",
    "                ):\n",
    "        super(multiHeadedClassifier, self).__init__()\n",
    "        self.attentions = torch.nn.ModuleList([])\n",
    "        self.bert_dim = 768\n",
    "        if (\"large\" in bert_model):\n",
    "            self.bert_dim = 1024\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\").cuda()\n",
    "        for i in range(attention_heads):\n",
    "            self.attentions.append(AttentionHead(dim=attention_head_dim, bert_model=bert_model))\n",
    "        self.lin = torch.nn.Linear(self.bert_dim + attention_heads*attention_head_dim, attention_head_dim)\n",
    "        self.output = torch.nn.Linear(attention_head_dim, 1)\n",
    "    \n",
    "    def forward(self, d, se, m):\n",
    "        _d, _ = self.bert(d, se, m, output_all_encoded_layers=False)\n",
    "        \n",
    "        att_outs = []\n",
    "        for i in range(len(self.attentions)):\n",
    "            head_out = self.attentions[i](_d)\n",
    "            att_outs.append(head_out)\n",
    "        att_outs = torch.cat(att_outs, dim=-1).unsqueeze(1)\n",
    "        _out = torch.cat([_d[:,0,:].unsqueeze(1), att_outs], dim=-1)\n",
    "        _out = torch.tanh(self.lin(_out))\n",
    "        return self.output(_out).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:49.596830Z",
     "start_time": "2019-05-02T07:28:44.196890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiHeadedClassifier(\n",
       "  (attentions): ModuleList(\n",
       "    (0): AttentionHead(\n",
       "      (w): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (v): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (o): Linear(in_features=768, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lin): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = multiHeadedClassifier(attention_heads=1, attention_head_dim=256)\n",
    "network.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:52.218625Z",
     "start_time": "2019-05-02T07:28:51.973475Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "def validate(network, batches=10, bs=64):\n",
    "    print(\"\\tValidating...\")\n",
    "    y_preds = []\n",
    "    y_acts = []\n",
    "    for i in range(batches):\n",
    "        with torch.no_grad():\n",
    "            d, se, m, y = genBatch(bs=12, testing=True, cuda=True)\n",
    "            y_pred = network.forward(d, se, m)\n",
    "            if (len(y_preds) == 0):\n",
    "                y_preds = y_pred\n",
    "                y_acts = y\n",
    "            else:\n",
    "                y_preds = torch.cat([y_preds, y_pred], dim=0)\n",
    "                y_acts = torch.cat([y_acts, y])\n",
    "\n",
    "    loss = loss_fn(y_preds, y_acts.unsqueeze(-1))\n",
    "    score = roc(y_acts.cpu().numpy(), y_preds.cpu().numpy())\n",
    "    print(\"\\t\", np.round(loss.data.item(), 5), np.round(score,2))\n",
    "    return loss.data.item(), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:53.203733Z",
     "start_time": "2019-05-02T07:28:53.193357Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_losses = []\n",
    "epoch_losses = []\n",
    "validation_rocs = []\n",
    "\n",
    "def _save(network, cause):\n",
    "    print(\"\\tSaving model for cause:\", cause)\n",
    "    torch.save(network.state_dict(), \"./UnbiasedToxicity_\" + cause + \".h5\")\n",
    "    _trgcycle = {\n",
    "        \"training_losses\":epoch_losses,\n",
    "        \"validation_losses\":validation_losses,\n",
    "        \"roc_auc\":validation_rocs\n",
    "    }\n",
    "    with open(\"./TrainingCycle_UbiasedToxicity_\" + cause + \".json\", \"w\") as f:\n",
    "        f.write(json.dumps(_trgcycle))\n",
    "        f.close()\n",
    "    \n",
    "def saveModel(network):\n",
    "    loss, roc_auc = validate(network)\n",
    "    validation_losses.append(loss)\n",
    "    validation_rocs.append(roc_auc)\n",
    "    \n",
    "    if (np.min(epoch_losses) == epoch_losses[-1]):\n",
    "        _save(network,\"BestTrainingLoss\")\n",
    "    \n",
    "    if (np.min(validation_losses) == validation_losses[-1]):\n",
    "        _save(network, \"BestValidationLoss\")\n",
    "    \n",
    "    if (np.max(validation_rocs) == validation_rocs[-1]):\n",
    "        _save(network, \"BestRoCAUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:54.272032Z",
     "start_time": "2019-05-02T07:28:54.258302Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer=None, loss_function=None, epochs=2, batches_per_epoch=10, bs=12):\n",
    "    for k in range(epochs):\n",
    "        batch_losses = []\n",
    "        batch_rocs = []\n",
    "        for j in range(batches_per_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            d, se, m, y = genBatch(bs=bs, testing=False, cuda=True)\n",
    "            y_pred = network.forward(d, se, m)\n",
    "            loss = loss_function(y_pred, y.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y_act = y.detach().cpu().numpy()\n",
    "            y_ = torch.sigmoid(y_pred).detach().cpu().numpy()\n",
    "            batch_losses.append(loss.data.item())\n",
    "            roc_ = roc(y_act,y_)\n",
    "            batch_rocs.append(roc_)\n",
    "            _str = \"Epoch: \" + str(k + 1) + \"; Batch: (\" + str(j+1) + \"/\" + str(batches_per_epoch) + \")\"\n",
    "            _str = _str + \"\\tLoss: \" + str(np.round(np.mean(batch_losses), 5)) + \\\n",
    "                    \"; AUC:\" + str(np.round(np.mean(batch_rocs), 2))\n",
    "            print(_str, end=\"\\r\")\n",
    "        print(\"\\n\")\n",
    "        epoch_losses.append(np.mean(batch_losses))\n",
    "        saveModel(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:49.602046Z",
     "start_time": "2019-05-02T07:28:49.598264Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optim = torch.optim.SGD(network.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:36:19.271029Z",
     "start_time": "2019-05-02T07:28:55.171539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Batch: (9/500)\tLoss: 0.69299; AUC:0.51\r"
     ]
    }
   ],
   "source": [
    "train(network, optimizer=optim, loss_function=loss_fn, epochs=10, bs=64, batches_per_epoch=500)\n",
    "#train(network, optimizer=optim, loss_function=loss_fn, epochs=2, bs=12, batches_per_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
