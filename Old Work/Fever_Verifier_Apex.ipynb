{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T16:39:48.166545Z",
     "start_time": "2019-04-03T16:39:47.850902Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from apex import amp\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first 512 tokens of the concatenated evidences as shown in the dataset are used. Substitute this for a sentence formation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T16:39:52.122110Z",
     "start_time": "2019-04-03T16:39:48.963010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72000 data points.\n"
     ]
    }
   ],
   "source": [
    "all_data = pickle.load(open(\"./usable_verifiable_fever_data.pickle\", \"rb\"))\n",
    "data = all_data[:72000]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(str(len(data)) + \" data points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_precision = True\n",
    "num_layers_to_take_from_bert = 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# def simple_attention_in_two_directions(claims_, evidences_, c2e, e2c):\n",
    "    claims_last_ts = claims_[:,-1,:]\n",
    "    evidences_last_ts = evidences_[:,-1,:]\n",
    "\n",
    "    #do attention between the last time step of the claims, and all the timesteps of the evidences\n",
    "    att = c2e(claims_last_ts).unsqueeze(dim=1)\n",
    "    att = torch.matmul(evidences_, att.transpose(1, 2))\n",
    "    att = F.softmax(att, dim=-2)\n",
    "    #reduce the evidences into a single context vector after applying attention\n",
    "    evidences_out = torch.sum(evidences_ * att, dim=1)\n",
    "\n",
    "    #do attention between the last time step of the evidences, and all the timesteps of the claims\n",
    "    att2 = e2c(evidences_last_ts).unsqueeze(dim=1)\n",
    "    att2 = torch.matmul(claims_, att2.transpose(1, 2))\n",
    "    att2 = F.softmax(att2, dim=-2)\n",
    "    #reduce the evidences into a single context vector after applying attention\n",
    "    claims_out = torch.sum(claims_*att2, dim=1)\n",
    "\n",
    "    #concatenate the two to produce the output vector\n",
    "    out_ = torch.cat([claims_out, evidences_out], dim=-1)\n",
    "    return out_, (att, att2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBERT(claims, evidences, bert, extract_layers=4):\n",
    "    seq_tokens = torch.zeros(claims.size(), dtype=torch.long).cuda()\n",
    "    claims_, _ = bert(claims, seq_tokens)\n",
    "    claims_ = torch.cat(claims_[-extract_layers:], dim=-1)\n",
    "    evidences_, _ = bert(evidences)\n",
    "    evidences_ = torch.cat(evidences_[-extract_layers:], dim=-1)\n",
    "    return claims_, evidences_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biDAF(claims, evidences, af_vector, output_perceptron):\n",
    "    #modified the alignment matrix computation because it's more conventient this way\n",
    "    align = torch.matmul(claims, af_vector)\n",
    "    align = torch.matmul(align, evidences.transpose(2,1)) #bs, claims_ts, evidences_ts\n",
    "    \n",
    "    #context-to-evidence alignment\n",
    "    att_c2e = F.softmax(align, dim=-1)\n",
    "    att_c2e = torch.matmul(att_c2e, evidences) #(bs, ts, dim)\n",
    "\n",
    "    #evidence-to-context alignment\n",
    "    maxes = torch.max(align, dim=-1)\n",
    "    att_e2c = torch.matmul(maxes[0], claims)\n",
    "    att_e2c = torch.sum(att_e2c, dim=-2) #(bs, dim)\n",
    "    att_e2c = att_e2c.unsqueeze(1).repeat(1, claims.size()[1], 1) #(bs, ts, dim)\n",
    "    \n",
    "    #concat and return after push through perceptron\n",
    "    out_ = torch.cat([claims, att_c2e, att_e2c], dim=-1)\n",
    "    out_ = output_perceptron(out_)\n",
    "    return out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePostBiDAF(bidaf_out, lstm, perceptron, out):\n",
    "    out_, _ = lstm(bidaf_out)\n",
    "    out_ = torch.tanh(perceptron(out_[:,-1,:]))\n",
    "    out_ = out(out_)\n",
    "    return out_.squeeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T16:39:52.134797Z",
     "start_time": "2019-04-03T16:39:52.123397Z"
    }
   },
   "outputs": [],
   "source": [
    "class Verifier(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers_to_take_from_bert=4, \n",
    "                 biDAF_out = 1024,\n",
    "                 backprop_thru_bert = False):\n",
    "        super(Verifier, self).__init__()\n",
    "        self.num_bert_layers = num_layers_to_take_from_bert\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        if (not backprop_thru_bert):\n",
    "            self.bert.embeddings.requires_grad = False\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(biDAF_out, biDAF_out, bidirectional=True)\n",
    "        self.biDAF_perceptron = torch.nn.Linear(num_layers_to_take_from_bert*768*3,biDAF_out)\n",
    "        self.biDAF_alignment_vector = torch.nn.Parameter(torch.zeros(num_layers_to_take_from_bert*768,num_layers_to_take_from_bert*768))\n",
    "        self.perceptron = torch.nn.Linear(biDAF_out*2, 256)\n",
    "        self.out = torch.nn.Linear(256, 1)\n",
    "        self.dropout = torch.nn.Dropout(0.15)\n",
    "        print(\"remember to use BCEWithLogitsLoss since the output is not put through sigmoid\")\n",
    "    \n",
    "    def forward(self, claims, evidences):\n",
    "        #put both the claims and the evidences thru BERT; concat the last n layers\n",
    "        claims_, evidences_ = getBERT(claims, evidences, self.bert, extract_layers=self.num_bert_layers)\n",
    "        out_ = biDAF(self.dropout(claims_), self.dropout(evidences_), self.biDAF_alignment_vector, self.biDAF_perceptron)\n",
    "        return computePostBiDAF(out_, self.lstm, self.perceptron, self.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T16:40:00.239286Z",
     "start_time": "2019-04-03T16:39:52.926125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remember to use BCEWithLogitsLoss since the output is not put through sigmoid\n",
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "verifier = Verifier(num_layers_to_take_from_bert=2).cuda()\n",
    "optimizer = torch.optim.Adam(verifier.parameters(), lr=0.001)\n",
    "if (mixed_precision):\n",
    "    verifier, optimizer = amp.initialize(verifier, optimizer, opt_level=\"O2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T16:40:00.244933Z",
     "start_time": "2019-04-03T16:40:00.241035Z"
    }
   },
   "outputs": [],
   "source": [
    "def getBatch(bs = 5, max_len=512, claim_len=30):\n",
    "    indices = np.random.randint(0, len(data), (bs,))\n",
    "    batch_evidences = np.asarray([data[index][\"evidence\"] for index in indices])\n",
    "    batch_claims = np.asarray([data[index][\"claim\"] for index in indices])\n",
    "    batch_evidences = batch_evidences[:,:200]\n",
    "    batch_claims = batch_claims[:,:10]\n",
    "    y = [data[index][\"class\"] for index in indices]\n",
    "    return batch_claims, batch_evidences, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16_Optimizer processing param group 0:\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([1536, 1536])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([30522, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([2, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 3072])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768, 768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([768])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([4096, 1024])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([4096, 1024])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([4096])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([4096])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([4096, 1024])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([4096, 1024])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([4096])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([4096])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([1024, 4608])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([1024])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([256, 2048])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([256])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([1, 256])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from apex.fp16_utils import FP16_Optimizer\n",
    "optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T16:40:01.886076Z",
     "start_time": "2019-04-03T16:40:01.875798Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epochs = 1, batch_size=4, batches_per_epoch=100):\n",
    "    for k in range(epochs):\n",
    "        losses = []\n",
    "        for i in range(batches_per_epoch):            \n",
    "            claim, evidence, y = getBatch(bs=batch_size)\n",
    "            claim = torch.LongTensor(claim).cuda()\n",
    "            evidence = torch.LongTensor(evidence).cuda()\n",
    "            y = torch.FloatTensor(y).cuda()\n",
    "            #print(y)\n",
    "            output = verifier(claim, evidence)\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                print(F.sigmoid(output))\n",
    "            \"\"\"\n",
    "            loss = loss_fn(output, y)\n",
    "            losses.append(loss.data.item())\n",
    "            if (mixed_precision):\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses = losses[-100:]\n",
    "            print(\"Epoch:\", str(k), \n",
    "                  \"; Batch:\", str(i), \n",
    "                  \"; Average Loss:\", str(np.round(np.mean(losses), 7)), \n",
    "                  end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T16:44:24.127838Z",
     "start_time": "2019-04-03T16:40:02.640822Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aman/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/rnn.py:179: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 ; Batch: 162 ; Average Loss: 0.6908445\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e7f17f6e774f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmixed_precision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-f6399d8dc59b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, batches_per_epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmixed_precision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                     \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "bpe = 1000\n",
    "batch_size = 16\n",
    "if (mixed_precision):\n",
    "    batch_size = 70\n",
    "train(epochs= epochs, batches_per_epoch=bpe, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
