{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T16:34:43.459290Z",
     "start_time": "2019-04-23T16:34:40.609261Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = pickle.load(open(\"../../../Data/DMQA/cnn_tokenized.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T17:07:06.609155Z",
     "start_time": "2019-04-23T17:07:06.593047Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from Attentions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T16:35:27.142335Z",
     "start_time": "2019-04-23T16:35:27.137836Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "max_doc_length = 512\n",
    "max_summary_length = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T16:35:27.430310Z",
     "start_time": "2019-04-23T16:35:27.416392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512]) torch.Size([5, 512]) torch.Size([5, 512]) torch.Size([5, 75])\n"
     ]
    }
   ],
   "source": [
    "def genBatch(bs = 5):\n",
    "    indices = np.random.randint(0, len(data), (bs,))\n",
    "    docs = [data[index][\"story_tokens\"] for index in indices]\n",
    "    documents = []\n",
    "    summaries = []\n",
    "    for doc in docs:\n",
    "        doc.insert(0, 101) #<- 101 is the token id for the CLS token\n",
    "        while (len(doc) < max_doc_length):\n",
    "            doc.append(0)\n",
    "        doc = doc[:max_doc_length]\n",
    "        documents.append(doc)\n",
    "        \n",
    "    sums = [data[index][\"summary_tokens\"] for index in indices]\n",
    "    for summ in sums:\n",
    "        summ.insert(0, 101)\n",
    "        while (len(summ) < max_summary_length):\n",
    "            summ.append(0)\n",
    "        summ = summ[:max_summary_length]\n",
    "        summaries.append(summ)\n",
    "    documents = torch.LongTensor(documents)\n",
    "    summaries = torch.LongTensor(summaries)\n",
    "    segments = torch.zeros_like(documents)\n",
    "    mask = documents > 0\n",
    "    \n",
    "    return documents, segments, mask, summaries\n",
    "    \n",
    "d, se, m, su = genBatch()\n",
    "print(d.size(), se.size(), m.size(), su.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T17:12:59.794268Z",
     "start_time": "2019-04-23T17:12:59.787910Z"
    }
   },
   "outputs": [],
   "source": [
    "def resolvePreviouslyGeneratedText(arr, innerAttentionMatrix, resolutionMatrix):\n",
    "    _allPrev = torch.cat(arr, dim=1)\n",
    "    prev_ = InnerAttention(_allPrev, innerAttentionMatrix)\n",
    "    prev_ = torch.sum(prev_, dim=1)\n",
    "    return torch.matmul(prev_, resolutionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T17:12:14.290127Z",
     "start_time": "2019-04-23T17:12:14.273745Z"
    }
   },
   "outputs": [],
   "source": [
    "class Summarizer(torch.nn.Module):\n",
    "    def __init__(self, bert_model = \"bert-base-uncased\"):\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.bert_width = 768\n",
    "        self.bert_model = bert_model\n",
    "        if (\"-large-\" in self.bert_model):\n",
    "            self.bert_width = 1024\n",
    "        \n",
    "        self.attention_accumulation = torch.nn.Parameter(torch.zeros((512,)))\n",
    "        self.all_generated = []\n",
    "        \n",
    "        self.wz = torch.nn.Parameter(torch.zeros((self.bert_width*2, self.bert_width)))\n",
    "        self.wr = torch.nn.Parameter(torch.zeros((self.bert_width*2, self.bert_width)))\n",
    "        self.w_cand = torch.nn.Parameter(torch.zeros((self.bert_width*2, self.bert_width)))\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        \n",
    "        self.innerPrevAttention = torch.nn.Parameter(torch.zeros((30000, 1024)))\n",
    "        self.prevToWidth = torch.nn.Parameter(torch.zeros((30000, self.bert_width)))\n",
    "        self.attention_weights = torch.nn.Parameter(torch.zeros((self.bert_width, self.bert_width)))\n",
    "        self.output_ = torch.nn.Parameter(torch.zeros(self.bert_width, 30000))\n",
    "        \n",
    "    def init_hidden_state(self, size):\n",
    "        _prev_word = self.output_[101] #<- this is basically the cls marker\n",
    "        _prev_word = _prev_word.repeat(size[0], 1).unsqueeze(1)\n",
    "        return torch.zeros(size), _prev_word\n",
    "    \n",
    "    def forward(self, docs, segments, masks, output_ts = 75):\n",
    "        \n",
    "        hs, generated_words = self.init_hidden_state((docs.size()[0],self.bert_width, self.bert_width))\n",
    "        print(hs.size(), start_word.size())\n",
    "        \n",
    "        for i in range(output_ts):\n",
    "            _gen = resolvePreviouslyGeneratedText(generated_words, \n",
    "                                                  self.innerPrevAttention, \n",
    "                                                  self.prevToWidth)\n",
    "            \n",
    "            _\n",
    "        #_docs, _ = self.bert(docs, segments, masks, output_all_encoded_layers = False)\n",
    "        #_docs = _docs * masks\n",
    "        \n",
    "        \n",
    "        \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T17:12:20.108676Z",
     "start_time": "2019-04-23T17:12:14.760760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 768, 768]) torch.Size([5, 1, 30000])\n",
      "Time step: 0\n",
      "Time step: 1\n",
      "Time step: 2\n",
      "Time step: 3\n",
      "Time step: 4\n",
      "Time step: 5\n",
      "Time step: 6\n",
      "Time step: 7\n",
      "Time step: 8\n",
      "Time step: 9\n",
      "Time step: 10\n",
      "Time step: 11\n",
      "Time step: 12\n",
      "Time step: 13\n",
      "Time step: 14\n",
      "Time step: 15\n",
      "Time step: 16\n",
      "Time step: 17\n",
      "Time step: 18\n",
      "Time step: 19\n",
      "Time step: 20\n",
      "Time step: 21\n",
      "Time step: 22\n",
      "Time step: 23\n",
      "Time step: 24\n",
      "Time step: 25\n",
      "Time step: 26\n",
      "Time step: 27\n",
      "Time step: 28\n",
      "Time step: 29\n",
      "Time step: 30\n",
      "Time step: 31\n",
      "Time step: 32\n",
      "Time step: 33\n",
      "Time step: 34\n",
      "Time step: 35\n",
      "Time step: 36\n",
      "Time step: 37\n",
      "Time step: 38\n",
      "Time step: 39\n",
      "Time step: 40\n",
      "Time step: 41\n",
      "Time step: 42\n",
      "Time step: 43\n",
      "Time step: 44\n",
      "Time step: 45\n",
      "Time step: 46\n",
      "Time step: 47\n",
      "Time step: 48\n",
      "Time step: 49\n",
      "Time step: 50\n",
      "Time step: 51\n",
      "Time step: 52\n",
      "Time step: 53\n",
      "Time step: 54\n",
      "Time step: 55\n",
      "Time step: 56\n",
      "Time step: 57\n",
      "Time step: 58\n",
      "Time step: 59\n",
      "Time step: 60\n",
      "Time step: 61\n",
      "Time step: 62\n",
      "Time step: 63\n",
      "Time step: 64\n",
      "Time step: 65\n",
      "Time step: 66\n",
      "Time step: 67\n",
      "Time step: 68\n",
      "Time step: 69\n",
      "Time step: 70\n",
      "Time step: 71\n",
      "Time step: 72\n",
      "Time step: 73\n",
      "Time step: 74\n"
     ]
    }
   ],
   "source": [
    "s = Summarizer()\n",
    "s.forward(d, se, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
