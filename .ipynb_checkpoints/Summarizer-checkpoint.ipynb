{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:07:40.088605Z",
     "start_time": "2019-04-28T04:07:39.784824Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from Attentions import *\n",
    "import json\n",
    "import pickle\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:07:59.244077Z",
     "start_time": "2019-04-28T04:07:40.089797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "all_data = pickle.load(open(\"./training_0.pickle\", \"rb\"))\n",
    "print(len(all_data))\n",
    "training = all_data[:-len(all_data)//10]\n",
    "testing = all_data[-len(all_data)//10:]\n",
    "\n",
    "max_doc_length = 100\n",
    "max_summary_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:07:59.275578Z",
     "start_time": "2019-04-28T04:07:59.245759Z"
    }
   },
   "outputs": [],
   "source": [
    "_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:08:01.128899Z",
     "start_time": "2019-04-28T04:07:59.277170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100]) torch.Size([5, 100]) torch.Size([5, 100]) torch.Size([5, 20]) torch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "def genBatch(bs = 5, validation = False):\n",
    "    data = training\n",
    "    if (validation):\n",
    "        data = testing\n",
    "    indices = np.random.randint(0, len(data), (bs,))\n",
    "    docs = [data[index][\"story_tokens\"] for index in indices]\n",
    "    _pointers = [data[index][\"pointers\"] for index in indices]\n",
    "    \n",
    "    documents = []\n",
    "    summaries = []\n",
    "    pointers = []\n",
    "    for doc in docs:\n",
    "        doc = doc[1:]\n",
    "        doc.insert(0, 101) #<- 101 is the token id for the CLS token\n",
    "        while (len(doc) < max_doc_length):\n",
    "            doc.append(0)\n",
    "        doc = doc[:max_doc_length]\n",
    "        documents.append(doc)\n",
    "    #print(documents)  \n",
    "    \n",
    "    #print(indices)\n",
    "    sums = [data[index][\"summary_tokens\"] for index in indices]\n",
    "    #print(sums)\n",
    "    for k in range(len(sums)):\n",
    "        summ = sums[k]\n",
    "        _point = _pointers[k]\n",
    "        while (len(summ) < max_summary_length):\n",
    "            summ.append(0)\n",
    "        summ = summ[:max_summary_length]\n",
    "        summaries.append(summ)\n",
    "        points = np.zeros((len(summ),))\n",
    "        _point_choice = np.asarray(_point) < max_summary_length\n",
    "        _point = np.asarray(_point)[_point_choice]\n",
    "        if (len(_point) > 0):\n",
    "            points[_point] = 1\n",
    "        pointers.append(points)\n",
    "        \n",
    "    if _cuda:\n",
    "        documents = torch.LongTensor(documents).cuda()\n",
    "        summaries = torch.LongTensor(summaries).cuda()\n",
    "        segments = torch.zeros_like(documents).cuda()\n",
    "        pointers = torch.FloatTensor(pointers).cuda()\n",
    "    else:\n",
    "        documents = torch.LongTensor(documents)\n",
    "        summaries = torch.LongTensor(summaries)\n",
    "        segments = torch.zeros_like(documents)\n",
    "        pointers = torch.FloatTensor(pointers)\n",
    "    mask = documents > 0\n",
    "    \n",
    "    return documents, segments, mask, summaries, pointers\n",
    "    \n",
    "d, se, m, su, po = genBatch()\n",
    "print(d.size(), se.size(), m.size(), su.size(), po.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:08:01.133982Z",
     "start_time": "2019-04-28T04:08:01.130359Z"
    }
   },
   "outputs": [],
   "source": [
    "def resolvePreviouslyGeneratedText(arr, innerAttentionMatrix, resolutionMatrix):\n",
    "    #_allPrev = torch.cat(arr, dim=1)\n",
    "    _allPrev = arr\n",
    "    prev_ = InnerAttention(_allPrev, innerAttentionMatrix)\n",
    "    if (len(prev_.size()) == 2):\n",
    "        prev_ = prev_.unsqueeze(1)\n",
    "    prev_ = torch.sum(prev_, dim=1)\n",
    "    return torch.matmul(prev_, resolutionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:14:17.935263Z",
     "start_time": "2019-04-28T04:14:17.907582Z"
    }
   },
   "outputs": [],
   "source": [
    "class Summarizer(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 bert_model = \"bert-base-uncased\",\n",
    "                 attention_dim = 512,\n",
    "                 tf = True,\n",
    "                 isCuda = True):\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.bert_width = 768\n",
    "        self.bert_model = bert_model\n",
    "        self.iscuda = isCuda\n",
    "        self.teacherForcing = tf\n",
    "        if (\"-large-\" in self.bert_model):\n",
    "            self.bert_width = 1024\n",
    "        \n",
    "        self.wz = torch.nn.Parameter(torch.zeros((self.bert_width*3, self.bert_width)))\n",
    "        self.wr = torch.nn.Parameter(torch.zeros((self.bert_width*3, self.bert_width*3)))\n",
    "        self.w_cand = torch.nn.Parameter(torch.zeros((self.bert_width*3, self.bert_width)))\n",
    "        \n",
    "        if (self.iscuda):\n",
    "            self.bert = BertModel.from_pretrained(bert_model).cuda()\n",
    "        else:\n",
    "            self.bert = BertModel.from_pretrained(bert_model)\n",
    "            \n",
    "        self.innerPrevAttention = torch.nn.Parameter(torch.ones((self.bert_width, attention_dim)))\n",
    "        self.prevToWidth = torch.nn.Parameter(torch.ones((self.bert_width, self.bert_width)))\n",
    "        \n",
    "        \"\"\" UaHj, Wa, Va\"\"\"\n",
    "        self.attention_weights = torch.nn.Parameter(torch.ones((self.bert_width, self.bert_width)))\n",
    "        self.ua = torch.nn.Parameter(torch.ones((self.bert_width, 128)))\n",
    "        self.wa = torch.nn.Parameter(torch.ones((self.bert_width, 128)))\n",
    "        self.va = torch.nn.Parameter(torch.ones((128,)))\n",
    "        \n",
    "        self.output_ = torch.nn.Parameter(torch.ones(self.bert_width, 30000))\n",
    "        self.output_to_network_embedding = torch.nn.Embedding(30000, self.bert_width)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        \n",
    "    def init_hidden_state(self, size):\n",
    "        if (self.iscuda):\n",
    "            _prev_word = torch.LongTensor([[101]]).cuda()#<- this is basically the cls marker\n",
    "        else:\n",
    "            _prev_word = torch.LongTensor([[101]])\n",
    "        _prev_word = _prev_word.repeat(size[0], 1)\n",
    "        if (self.cuda):\n",
    "            return torch.ones(size).cuda(), [_prev_word.cuda()]\n",
    "        else:\n",
    "            return torch.ones(size), [_prev_word]\n",
    "    \n",
    "    def forward(self, docs, segments, masks, output_ts = 75, y = None, tf_prob = 0.25):\n",
    "        pointers = []\n",
    "        atts = []\n",
    "        hs, _output_words = self.init_hidden_state((docs.size()[0],1, self.bert_width))\n",
    "        \n",
    "        _docs, _ = self.bert(docs, segments, masks, output_all_encoded_layers = False)\n",
    "        _docs = _docs * masks.unsqueeze(-1).float()\n",
    "        _docs = self.dropout(_docs)\n",
    "        _x = _docs\n",
    "        _uahj = torch.matmul(_docs, self.ua)\n",
    "        generated_words = []\n",
    "        #coverage = torch.zeros((docs.size()[0],docs.size()[1])).cuda() \n",
    "        \n",
    "        for i in range(output_ts):\n",
    "            _generatedContext = None\n",
    "            _ow = torch.stack(_output_words, dim=1)\n",
    "            _ow = self.output_to_network_embedding(_ow.squeeze(-1))\n",
    "            \n",
    "            y_in = _ow[:,-1,:]\n",
    "            #self attention and context vector generation of all previously generated words\n",
    "            \"\"\"\n",
    "            _generatedContext = resolvePreviouslyGeneratedText(_ow, \n",
    "                                                  self.innerPrevAttention, \n",
    "                                                  self.prevToWidth).unsqueeze(1)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            _stwa = torch.matmul(hs, self.wa)\n",
    "            _xatt = bahdanauAttention(_uahj, _stwa, self.va)\n",
    "            \n",
    "            #context vector generation for the doc space\n",
    "            \"\"\"\n",
    "            att = dotProductAttention(_docs, hs, self.attention_weights).transpose(-2,-1)\n",
    "            \"\"\"\n",
    "            _dcv = _docs * _xatt.unsqueeze(-1)\n",
    "            doc_context_vector = torch.sum(_dcv, dim=1).unsqueeze(1)\n",
    "\n",
    "\n",
    "            #gru gating\n",
    "            _gru_in = torch.cat([y_in.unsqueeze(1), hs, doc_context_vector], dim=-1)            \n",
    "            z = torch.sigmoid(torch.matmul(_gru_in, self.wz))\n",
    "            r = torch.sigmoid(torch.matmul(_gru_in, self.wr))\n",
    "            \n",
    "            #candidate hidden state and final hidden state for the gru\n",
    "            _cand_in = _gru_in*r\n",
    "            _cand_in = self.dropout(_cand_in)\n",
    "            h_cand = torch.tanh(torch.matmul(_cand_in, self.w_cand))\n",
    "            hs = (1-z)*hs + z*h_cand\n",
    "            \n",
    "            #generate the output word\n",
    "            word = torch.matmul(hs, self.output_)\n",
    "            generated_words.append(word)\n",
    "            \n",
    "            _word = F.softmax(word, dim=-1)\n",
    "            _word = torch.max(_word, dim=-1)[1]\n",
    "            if (self.teacherForcing and (y is not None)):\n",
    "                choice = np.random.randint(0, 100, (1,))[0]\n",
    "                if (choice < tf_prob*100):\n",
    "                    _word = y[:,i].unsqueeze(-1)\n",
    "\n",
    "            _output_words.append(_word)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            #pointer architecture\n",
    "            _pointer_in = torch.cat([hs, doc_context_vector, _generatedContext.unsqueeze(1)], dim=-1)\n",
    "            pointer = self.pointer_out(_pointer_in)            \n",
    "            pointers.append(pointer)\n",
    "            \n",
    "            coverage = coverage + att.squeeze(-1)\n",
    "            add_cov = coverage.clone().unsqueeze(1)\n",
    "            coverages.append(add_cov)\n",
    "            \"\"\"\n",
    "            atts.append(_xatt.unsqueeze(1))\n",
    "        \n",
    "        return torch.cat(generated_words, dim=1), torch.cat(atts, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:14:18.663295Z",
     "start_time": "2019-04-28T04:14:18.660153Z"
    }
   },
   "outputs": [],
   "source": [
    "continue_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:14:18.914206Z",
     "start_time": "2019-04-28T04:14:18.901752Z"
    }
   },
   "outputs": [],
   "source": [
    "network = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:14:24.567480Z",
     "start_time": "2019-04-28T04:14:19.128829Z"
    }
   },
   "outputs": [],
   "source": [
    "network = Summarizer(isCuda = _cuda).cuda()\n",
    "\n",
    "continueForCause = \"BestTrainingLoss\"\n",
    "if (_cuda):\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPU(s)...\")\n",
    "        network = torch.nn.DataParallel(network)\n",
    "\n",
    "    epoch_losses = []\n",
    "    epoch_vals = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    if (continue_training):\n",
    "        network.load_state_dict(torch.load(\"./summarizer_\" + continueForCause + \".h5\"))\n",
    "        _training_cylce = None\n",
    "        with open(\"./Summarizer_training_cycle_\" + continueForCause + \".json\") as f:\n",
    "            _training_cylce = json.loads(f.read())\n",
    "            epoch_losses = _training_cylce[\"training_losses\"]\n",
    "            epoch_vals = _training_cylce[\"validation_losses\"]\n",
    "            print(epoch_losses, epoch_vals)\n",
    "\n",
    "    network.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "alpha = 1.\n",
    "beta = 1.\n",
    "gamma = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:14:24.577601Z",
     "start_time": "2019-04-28T04:14:24.568720Z"
    }
   },
   "outputs": [],
   "source": [
    "def _save(cause):\n",
    "    print(\"\\t\\t...saving model for cause\", cause)\n",
    "    torch.save(network.state_dict(), \"./summarizer_\" + cause + \".h5\")\n",
    "    with open(\"./Summarizer_training_cycle_\"  + cause + \".json\", \"w\") as f:            \n",
    "        f.write(json.dumps(\n",
    "            {\n",
    "                \"training_losses\":epoch_losses,\n",
    "                \"validation_losses\":epoch_vals,\n",
    "                \"validation_accuracy\":epoch_accs,\n",
    "            }\n",
    "        ))\n",
    "        f.close()\n",
    "\n",
    "def ValidateModel(validation_bs = 2):\n",
    "    all_su = None\n",
    "    all_pred = None\n",
    "    \n",
    "    print(\"\\n\\tValidating...\")\n",
    "    for i in range(10):\n",
    "        with torch.no_grad():\n",
    "            d, se, m, su, po = genBatch(bs=validation_bs, validation = True)\n",
    "            words, atts = network.forward(d, se, m, output_ts=ts)\n",
    "            if (all_su is None):\n",
    "                all_su = su\n",
    "                all_pred = words\n",
    "            else:\n",
    "                all_su = torch.cat([all_su, su], dim=0)\n",
    "                all_pred = torch.cat([all_pred, words], dim=0)\n",
    "    val_loss = WordLoss(all_su, all_pred)\n",
    "    print(\"\\tValidation Loss:\", np.round(val_loss.data.item(), 5))\n",
    "    epoch_vals.append(val_loss.data.item())\n",
    "\n",
    "    with open(\"./Summarizer_training_cycle.json\", \"w\") as f:            \n",
    "        f.write(json.dumps(\n",
    "            {\n",
    "                \"training_losses\":epoch_losses,\n",
    "                \"validation_losses\":epoch_vals,\n",
    "                \"validation_accuracy\":epoch_accs,\n",
    "            }\n",
    "        ))\n",
    "        f.close()\n",
    "    \n",
    "    if (np.min(epoch_vals) == epoch_vals[-1]):\n",
    "        _save(\"BestValidationLoss\")        \n",
    "    \n",
    "    if (np.min(epoch_losses) == epoch_losses[-1]):\n",
    "        _save(\"BestTrainingLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:14:24.587243Z",
     "start_time": "2019-04-28T04:14:24.579100Z"
    }
   },
   "outputs": [],
   "source": [
    "pointerCriterion = torch.nn.BCEWithLogitsLoss()\n",
    "wordCriterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def CoverageLoss(attentions):\n",
    "    \"\"\"\n",
    "    :param attentions: (b, yt, xt)\n",
    "    :param coverages: (b, yt, xt)\n",
    "    \"\"\"\n",
    "    coverage = None\n",
    "    losses = None\n",
    "    if (_cuda):\n",
    "        coverage = torch.zeros((attentions.size()[0], attentions.size()[-1])).cuda()\n",
    "    else:\n",
    "        coverage = torch.zeros((attentions.size()[0], attentions.size()[-1]))\n",
    "\n",
    "    losses = []\n",
    "    for i in range(attentions.size()[1]):\n",
    "        cov = torch.min(coverage, attentions[:,i,:])\n",
    "        _ts_loss = torch.sum(cov, dim=1)\n",
    "        losses.append(_ts_loss)\n",
    "        coverage = coverage + attentions[:,i,:]\n",
    "    \n",
    "    losses = torch.stack(losses)\n",
    "\n",
    "    _loss = (torch.sum(losses)/len(losses))/attentions.size()[0]\n",
    "    return _loss\n",
    "\n",
    "def PointerLoss(yPointers, y_Pointers):\n",
    "    return pointerCriterion(y_Pointers, yPointers)\n",
    "\n",
    "def WordLoss(yWords, y_Words):\n",
    "    return wordCriterion(y_Words.view(-1,30000), yWords.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T03:48:14.636625Z",
     "start_time": "2019-04-28T03:48:14.619532Z"
    }
   },
   "outputs": [],
   "source": [
    "def Train(epochs, batches_per_epoch, bs):\n",
    "    for k in range(epochs):\n",
    "        batch_losses = []\n",
    "        b_wl = []\n",
    "        b_pl = []\n",
    "        b_cl = []\n",
    "        for j in range(batches_per_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            d, se, m, su, po = genBatch(bs=bs)\n",
    "            loss_mask = su > 0\n",
    "            words, atts = network.forward(d, se, m, output_ts=ts, y=su)\n",
    "            _word_loss = alpha * WordLoss(su, words)\n",
    "            _coverage_loss = CoverageLoss(atts)\n",
    "            total_loss = _word_loss + _coverage_loss\n",
    "            \n",
    "            if (torch.sum(po) > 1):\n",
    "                l2 = beta * PointerLoss(po, pointers)\n",
    "                b_pl.append(l2.data.item())\n",
    "                b_cl.append(l.item())\n",
    "                total_loss = total_loss + l2\n",
    "            \n",
    "            b_wl.append(_word_loss.data.item())\n",
    "            b_cl.append(_coverage_loss.detach().cpu().numpy())\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(total_loss.data.item())\n",
    "            batch_losses.append(total_loss.data.item())\n",
    "            \n",
    "            _str = \"Epoch: \" + str(k+1) + \\\n",
    "            \";  Batch: \" + str(j+1)  + \"/\" + str(batches_per_epoch) + \\\n",
    "            \"; Loss: \" + str(np.round(np.mean(batch_losses), 5)) \n",
    "            \n",
    "            if (len(b_pl) > 0):\n",
    "                _str = _str + \" (\" + str(np.round(np.mean(b_wl),5))  + \\\n",
    "                \",\" + str(np.round(np.mean(b_pl),5)) + \\\n",
    "                \",\" + str(np.round(np.mean(b_cl), 5)) + \")\"\n",
    "            \n",
    "            print(_str, end = \"\\r\")\n",
    "        epoch_losses.append(np.mean(batch_losses))\n",
    "        ValidateModel(validation_bs = bs)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:13:01.280564Z",
     "start_time": "2019-04-28T04:13:01.274606Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = max_summary_length\n",
    "epochs = 1\n",
    "batches_per_epoch = 200\n",
    "\n",
    "if (_cuda):\n",
    "    if (torch.cuda.device_count() > 1):\n",
    "        bs = 56 * torch.cuda.device_count()\n",
    "    else:\n",
    "        bs = 16\n",
    "else:\n",
    "    bs = 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T03:53:03.501835Z",
     "start_time": "2019-04-28T03:51:18.926723Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train(epochs,batches_per_epoch,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:14:25.361088Z",
     "start_time": "2019-04-28T04:14:25.356354Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:14:26.266874Z",
     "start_time": "2019-04-28T04:14:26.179281Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(350.9231, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    d, se, m, su, po = genBatch(bs=8)\n",
    "    words, atts = network.forward(d, se, m, output_ts=20)\n",
    "    l = CoverageLoss(atts)\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T03:44:02.243075Z",
     "start_time": "2019-04-28T03:43:51.479411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "data = atts.squeeze(-1).cpu().numpy()\n",
    "for i in range(2):\n",
    "    plt.figure(figsize=(105, 15))\n",
    "    sns.heatmap(data[i], annot=True, vmin=0.01, vmax = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T14:02:02.645137Z",
     "start_time": "2019-04-25T14:02:02.007399Z"
    }
   },
   "outputs": [],
   "source": [
    "_save(\"LastForcedSave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T03:39:46.125254Z",
     "start_time": "2019-04-27T03:39:45.991794Z"
    }
   },
   "outputs": [],
   "source": [
    "network = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T03:40:26.602740Z",
     "start_time": "2019-04-27T03:40:20.990763Z"
    }
   },
   "outputs": [],
   "source": [
    "network = Summarizer()\n",
    "if (torch.cuda.device_count() > 1):\n",
    "    network = torch.nn.DataParallel(network)\n",
    "network.load_state_dict(torch.load(\"./summarizer_BestTrainingLoss.h5\"))\n",
    "network.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T03:44:03.567563Z",
     "start_time": "2019-04-28T03:44:02.518890Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "with torch.no_grad():\n",
    "    d, se, m, su, po = genBatch(bs=8)\n",
    "    words, atts = network.forward(d, se, m, output_ts=20)\n",
    "    print(words.topk(5, dim=-1)[1][0])\n",
    "    print(su[0])\n",
    "    #print(su)\n",
    "    #print(words.size())\n",
    "    words2 = F.softmax(words, dim=1)\n",
    "    w = torch.max(words2, dim=-1)[1]\n",
    "    #print(w)\n",
    "    _pred = tokenizer.convert_ids_to_tokens(w.cpu().numpy()[0])\n",
    "    _act = tokenizer.convert_ids_to_tokens(su.cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T03:40:39.681194Z",
     "start_time": "2019-04-27T03:40:39.676705Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\" \".join(_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T03:40:43.107281Z",
     "start_time": "2019-04-27T03:40:43.102802Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\" \".join(_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
