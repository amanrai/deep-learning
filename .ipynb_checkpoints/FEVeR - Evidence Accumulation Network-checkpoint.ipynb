{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:51:55.764208Z",
     "start_time": "2019-04-17T08:51:45.065341Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "processed_data = pickle.load(open(\"../fever_processed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep for Evidence Accumulation\n",
    "\n",
    "* The significance of a given statement as evidence to a claim/question is modelled as a classification problem\n",
    "* Any length of text (such as a sentence from a document), is appended to the claim/question in the usual way. \"CLS\" <claim/question tokens> \"SEP\" <potential/evidence tokens> \"SEP\"\n",
    "* A class is awarded to the combined string based on the following:\n",
    "    - Class 0, if the evidence tokens do not contribute to answering the question\n",
    "    - Class 1, if the evidence tokens partially answer the question\n",
    "    - Class 2, if the evidence tokens completely answer the question\n",
    "* len(claim) + len(evidence) + 3 should be <= 96 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:51:55.768240Z",
     "start_time": "2019-04-17T08:51:55.765930Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len_claims = 30\n",
    "max_len_evid = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:24.363410Z",
     "start_time": "2019-04-17T08:51:55.770123Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "classes = []\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def make_data(claim, evidence):\n",
    "    _fctokens = [\"[CLS]\"]\n",
    "    _fctokens.extend(claim)\n",
    "    _fetokens = [\"[CLS]\"]\n",
    "    _fetokens.extend(evidence) \n",
    "    while len(_fctokens) < max_len_claims:\n",
    "        _fctokens.append(\"[PAD]\")\n",
    "    while len(_fetokens) < max_len_evid:\n",
    "        _fetokens.append(\"[PAD]\")\n",
    "    _fctokens = _fctokens[:max_len_claims]\n",
    "    _fetokens = _fetokens[:max_len_evid]\n",
    "    _csegments = np.zeros((max_len_claims,))\n",
    "    _esegments = np.zeros((max_len_evid,))\n",
    "    ctokens = tokenizer.convert_tokens_to_ids(_fctokens)\n",
    "    etokens = tokenizer.convert_tokens_to_ids(_fetokens)\n",
    "    return (ctokens, etokens, _esegments)\n",
    "\n",
    "counter = 0\n",
    "for line in processed_data:\n",
    "    counter += 1\n",
    "    print(counter, \"/\", len(processed_data), end=\"\\r\")\n",
    "    for evidence in line[\"processed\"][\"evidentiary\"]:\n",
    "        lines.append(make_data(line[\"processed\"][\"claim\"], evidence))\n",
    "        if (len(line[\"processed\"][\"evidentiary\"]) == 1):\n",
    "            classes.append(1)\n",
    "        else:\n",
    "            classes.append(2)\n",
    "    for evidence in line[\"processed\"][\"non_evidentiary\"]:\n",
    "        lines.append(make_data(line[\"processed\"][\"claim\"], evidence))\n",
    "        classes.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:24.851950Z",
     "start_time": "2019-04-17T08:53:24.365274Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total data points = \", len(classes))\n",
    "print(\"Of which evidentiary:\", np.count_nonzero(classes))\n",
    "\n",
    "training_lines = lines[:-len(classes)//10]\n",
    "training_classes = classes[:-len(classes)//10]\n",
    "\n",
    "print(\"Total training data points = \", len(training_classes))\n",
    "print(\"Of which evidentiary:\", np.count_nonzero(training_classes))\n",
    "\n",
    "training_evidentiary_indices = [i for i in range(len(training_classes)) if training_classes[i] > 0 ]\n",
    "training_nonevidentiary_indices = [i for i in range(len(training_classes)) if training_classes[i] == 0]\n",
    "\n",
    "testing_lines = lines[-len(classes)//10:]\n",
    "testing_classes = classes[-len(classes)//10:]\n",
    "\n",
    "testing_evidentiary_indices = [i for i in range(len(testing_classes)) if testing_classes[i] > 0 ]\n",
    "testing_nonevidentiary_indices = [i for i in range(len(testing_classes)) if testing_classes[i] == 0]\n",
    "\n",
    "print(\"Total testing data points = \", len(testing_classes))\n",
    "print(\"Of which evidentiary:\", np.count_nonzero(testing_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Fact Extraction (ReFE)\n",
    "\n",
    "* Going by the paper and the documentation, when fine-tuning bert for classification tasks, only the output of the CLS tag (index 0) needs to be used. We can ignore the rest\n",
    "* Aim here is to make it look to BERT like an entailment task. Since we are not really worried about the truth value of the claim, all we need to do is decide if a given sentence provides evidence to the claim or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:24.856981Z",
     "start_time": "2019-04-17T08:53:24.853962Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:24.864951Z",
     "start_time": "2019-04-17T08:53:24.858628Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "epoch_vals = []\n",
    "epoch_accs = []\n",
    "epoch_evid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:26.932264Z",
     "start_time": "2019-04-17T08:53:24.867645Z"
    }
   },
   "outputs": [],
   "source": [
    "def getTrainingBatch(bs = 64, validation = False):\n",
    "    \n",
    "    evidentiary = training_evidentiary_indices\n",
    "    non_evidentiary = training_nonevidentiary_indices\n",
    "    source = training_lines\n",
    "    source_classes = training_classes\n",
    "    \n",
    "    if (validation):\n",
    "        evidentiary = testing_evidentiary_indices\n",
    "        non_evidentiary = testing_nonevidentiary_indices\n",
    "        source = testing_lines\n",
    "        source_classes = testing_classes\n",
    "    \n",
    "    #control the number of positive samples being\n",
    "    #seen by the dataset, since these will be much rarer in real life. \n",
    "    \n",
    "    min_divisor = 2\n",
    "    divisor = min_divisor\n",
    "    divisor = divisor + (len(epoch_losses) % 10)\n",
    "        \n",
    "    ev_total = bs // divisor\n",
    "    if (ev_total < bs//4):\n",
    "        ev_total = bs//4\n",
    "    nev_total = bs - ev_total\n",
    "    x = np.random.randint(0, len(evidentiary), (ev_total))\n",
    "    x = np.asarray(evidentiary)[x]\n",
    "    _base_ctokens = [source[index][0] for index in x]\n",
    "    _base_etokens = [source[index][1] for index in x]\n",
    "    #_segment_ctokens = [source[index][2] for index in x]\n",
    "    #_segment_etokens = [source[index][3] for index in x]\n",
    "    _classes = [source_classes[index] for index in x]\n",
    "    \n",
    "    x = np.random.randint(0, len(non_evidentiary), (nev_total))\n",
    "    x = np.asarray(non_evidentiary)[x]\n",
    "    _base_ctokens_ne = [source[index][0] for index in x]\n",
    "    _base_etokens_ne = [source[index][1] for index in x]\n",
    "    #_segment_ctokens_ne = [source[index][2] for index in x]\n",
    "    #_segment_etokens_ne = [source[index][2] for index in x]\n",
    "    _classes_ne = [source_classes[index] for index in x]\n",
    "    \n",
    "    _base_ctokens.extend(_base_ctokens_ne)\n",
    "    _base_etokens.extend(_base_etokens_ne)\n",
    "    #_segment_ctokens.extend(_segment_ctokens_ne)\n",
    "    #_segment_etokens.extend(_segment_etokens_ne)\n",
    "    _classes.extend(_classes_ne)\n",
    "        \n",
    "    final_seq = [i for i in range(bs)]\n",
    "    np.random.shuffle(final_seq)\n",
    "    \n",
    "    ctokens = []\n",
    "    etokens = []\n",
    "    csegments = np.zeros((bs, max_len_claims))\n",
    "    esegments = np.zeros((bs, max_len_evid))\n",
    "    classes = []\n",
    "    for index in final_seq:\n",
    "        ctokens.append(_base_ctokens[index])\n",
    "        etokens.append(_base_etokens[index])\n",
    "        #csegments.append(_segment_ctokens[index])\n",
    "        #esegments.append(_segment_etokens[index])\n",
    "        classes.append(_classes[index])\n",
    "    \n",
    "    \n",
    "    \"\"\" \n",
    "    #Two class vs Three class output\n",
    "    classes = np.asarray(classes)\n",
    "    twos = classes == 2\n",
    "    classes[twos] = 1\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ctokens = torch.LongTensor(ctokens).cuda()\n",
    "    csegments = torch.LongTensor(csegments).cuda()\n",
    "    etokens = torch.LongTensor(etokens).cuda()\n",
    "    esegments = torch.LongTensor(esegments).cuda()\n",
    "    classes = torch.LongTensor(classes).cuda()\n",
    "    catt_mask = ctokens != 0\n",
    "    eatt_mask = etokens != 0\n",
    "    \n",
    "    return ctokens, csegments, catt_mask, etokens, esegments, eatt_mask, classes\n",
    "    \n",
    "ctokens, csegments, catt_mask, etokens, esegments, eatt_mask, classes = getTrainingBatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:26.939616Z",
     "start_time": "2019-04-17T08:53:26.934281Z"
    }
   },
   "outputs": [],
   "source": [
    "print(ctokens.size(), csegments.size(), catt_mask.size())\n",
    "print(etokens.size(), esegments.size(), eatt_mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:26.950841Z",
     "start_time": "2019-04-17T08:53:26.941580Z"
    }
   },
   "outputs": [],
   "source": [
    "def getLoss(pred, actual, lossFn, e_weight=0.6, ne_weight=0.4):\n",
    "    evidences = actual >= 1\n",
    "    non_evidences = actual == 0\n",
    "    \n",
    "    loss1 = lossFn(F.log_softmax(pred[evidences], dim=-1), actual[evidences].cuda())\n",
    "    loss2 = lossFn(F.log_softmax(pred[non_evidences], dim = -1), actual[non_evidences].cuda())\n",
    "    \"\"\"\n",
    "    e_trg_losses.append(loss1)\n",
    "    ne_training_losses.append(loss2)\n",
    "    \"\"\"\n",
    "    \n",
    "    return (loss1 + loss2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:26.963241Z",
     "start_time": "2019-04-17T08:53:26.952075Z"
    }
   },
   "outputs": [],
   "source": [
    "#from QA_Attentions import biDAF as biDAF\n",
    "import torch\n",
    "from QA_Attentions import *\n",
    "\n",
    "class ReFE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReFE, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.wd = torch.nn.Parameter(torch.FloatTensor(np.random.uniform(0, 1, (3*768,))))\n",
    "        self.innerAttQuery = torch.nn.Parameter(torch.FloatTensor(np.random.uniform(0, 1, (768, 256))))\n",
    "        self.innerAttDoc = torch.nn.Parameter(torch.FloatTensor(np.random.uniform(0, 1, (768*4, 256))))\n",
    "        self.out = torch.nn.Linear((768*5),3)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, dt, ds, da, qt, qs, qa):\n",
    "        queries, pooled = self.bert(qt, \n",
    "                         token_type_ids=qs, \n",
    "                         attention_mask=qa, \n",
    "                         output_all_encoded_layers=False)\n",
    "        \n",
    "        documents, pooled = self.bert(dt, \n",
    "                         token_type_ids=ds, \n",
    "                         attention_mask=da, \n",
    "                         output_all_encoded_layers=False)\n",
    "        \n",
    "        bdaf, ad2q, aq2d = biDAF(documents, queries, self.wd)\n",
    "        q = InnerAttention(queries, self.innerAttQuery)\n",
    "        d = InnerAttention(bdaf, self.innerAttDoc)\n",
    "        _f = torch.cat([q,d],dim=-1)\n",
    "        out_ = self.out(_f)\n",
    "        return out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:42.779821Z",
     "start_time": "2019-04-17T08:53:42.776560Z"
    }
   },
   "outputs": [],
   "source": [
    "continue_from_prev = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:48.540785Z",
     "start_time": "2019-04-17T08:53:42.976400Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "network = None\n",
    "if (continue_from_prev):\n",
    "    network = torch.load(\"./ReFE_BestValidationLoss_save.h5\")\n",
    "    tcycle = None\n",
    "    with open(\"./saved_model_training_cycle.json\", \"r\") as f:\n",
    "        tcycle = json.loads(f.read())\n",
    "    epoch_losses = tcycle[\"training_losses\"][:-2]\n",
    "    epoch_vals = tcycle[\"validation_losses\"][:-2]\n",
    "    epoch_accs = tcycle[\"validation_accuracy\"][:-2]\n",
    "    epoch_evid = tcycle[\"evidence_accuracy\"][:-2]\n",
    "else:\n",
    "    network = ReFE()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Friends dont let friends use batch sizes > 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:49.604974Z",
     "start_time": "2019-04-17T08:53:49.598526Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "lossFn = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:50.337845Z",
     "start_time": "2019-04-17T08:53:50.328101Z"
    }
   },
   "outputs": [],
   "source": [
    "e_trg_losses = []\n",
    "ne_training_losses = []\n",
    "\n",
    "def _save(cause, network):\n",
    "    print(\"\\tSaving Model for Cause:\", cause)\n",
    "    torch.save(network, \"./ReFE_\" + cause + \"_save.h5\")\n",
    "    with open(\"./\" + cause + \"_training_cycle.json\", \"w\") as f:            \n",
    "        f.write(json.dumps(\n",
    "            {\n",
    "                \"training_losses\":epoch_losses,\n",
    "                \"validation_losses\":epoch_vals,\n",
    "                \"validation_accuracy\":epoch_accs,\n",
    "                \"evidence_accuracy\":epoch_evid        \n",
    "            }\n",
    "        ))\n",
    "        f.close()\n",
    "    \n",
    "def chooseModelSave(network):\n",
    "    save = False\n",
    "    if (np.min(epoch_vals) == epoch_vals[-1]):\n",
    "        cause = \"BestValidationLoss\"\n",
    "        _save(cause, network)\n",
    "    \n",
    "    if (np.max(epoch_accs) == epoch_accs[-1]):\n",
    "        cause = \"BestValidationOverallAccuracy\"\n",
    "        _save(cause, network)\n",
    "    \n",
    "    if (np.max(epoch_evid) == epoch_evid[-1]):\n",
    "        cause = \"BestValidationEvidentiaryAccuracy\"\n",
    "        _save(cause, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:51.160509Z",
     "start_time": "2019-04-17T08:53:51.147473Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(network, bs=100, num_batches=5):\n",
    "    \n",
    "    classes = torch.LongTensor([]).cuda()\n",
    "    preds = torch.FloatTensor([]).cuda()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            ct, cs, ca, et, es, ea, classes_ = getTrainingBatch(bs=bs, validation=True)\n",
    "            y_ = network.forward(et, es, ea, ct, cs, ca)\n",
    "            classes = torch.cat([classes, classes_], dim=0)\n",
    "            preds = torch.cat([preds, y_], dim=0)\n",
    "        \n",
    "        evidences = classes >= 1\n",
    "        f_loss = getLoss(preds, classes, lossFn)\n",
    "        pred = torch.max(preds, dim=-1)[1]\n",
    "        acc = torch.sum(pred == classes)\n",
    "        acc = acc.cpu().numpy()/(bs*num_batches)\n",
    "        positives = torch.sum(pred[evidences] == classes[evidences])\n",
    "        \n",
    "        return f_loss.data.item(), acc, positives.cpu().numpy()/torch.sum(evidences).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:55.662283Z",
     "start_time": "2019-04-17T08:53:52.083816Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def chooseLossWeights():\n",
    "    \n",
    "    if (random.randint(0,100) % 5 == 0):\n",
    "        return 1.0, 0.0\n",
    "    elif (random.randint(0,100) % 20 == 0):\n",
    "        return 0.25, 0.75\n",
    "    \n",
    "    return 0.5, 0.5\n",
    "\n",
    "def Train(network, bs = 24, epochs=30, batches_per_epoch=10000):\n",
    "    val_min = 1000\n",
    "    if (continue_from_prev):\n",
    "        val_min = np.min(epoch_vals)\n",
    "    for k in range(epochs):\n",
    "        batch_losses = []\n",
    "        for i in range(batches_per_epoch):\n",
    "            ct, cs, ca, et, es, ea, classes = getTrainingBatch(bs=bs)\n",
    "            y_ = network.forward(et, es, ea, ct, cs, ca)\n",
    "            optimizer.zero_grad()\n",
    "            e_w, ne_w = chooseLossWeights()\n",
    "            f_loss = getLoss(y_, classes, lossFn, e_weight=e_w, ne_weight=ne_w)\n",
    "            batch_losses.append(f_loss.data.item())\n",
    "            print(\"Epoch:\", k+1, \n",
    "                  \"Batch:\", i+1, \n",
    "                  \"Loss:\", np.round(np.mean(batch_losses),5), \n",
    "                  end=\"\\r\")\n",
    "            f_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(np.mean(batch_losses))\n",
    "        val_loss, acc, evid_acc = validate(network, num_batches=10)\n",
    "        \n",
    "        epoch_vals.append(val_loss)\n",
    "        epoch_accs.append(acc)\n",
    "        epoch_evid.append(evid_acc)\n",
    "        \n",
    "        print(\"\\n\\tValidation Loss:\", np.round(val_loss,5))\n",
    "        print(\"\\tOverall Validation Accuracy:\", np.round(acc,2), \"; and for evidence only:\", np.round(evid_acc,2))\n",
    "        \n",
    "        if (val_loss < val_min):\n",
    "            val_min = val_loss\n",
    "            \n",
    "        chooseModelSave(network)\n",
    "        \n",
    "        with open(\"./training_cycle.json\", \"w\") as f:            \n",
    "            f.write(json.dumps(\n",
    "                {\n",
    "                    \"training_losses\":epoch_losses,\n",
    "                    \"validation_losses\":epoch_vals,\n",
    "                    \"validation_accuracy\":epoch_accs,\n",
    "                    \"evidence_accuracy\":epoch_evid        \n",
    "                }\n",
    "            ))\n",
    "            f.close()\n",
    "\n",
    "tot_epochs = 100\n",
    "\"\"\"\n",
    "if (continue_from_prev):\n",
    "    tot_epochs = 75 - len(epoch_losses)\n",
    "\"\"\"\n",
    "Train(network.to(\"cuda\"), bs=75, epochs=tot_epochs, batches_per_epoch=750)\n",
    "#Train(network.to(\"cuda\"), bs=75, epochs=5, batches_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.load(\"./ReFE_BestValidationLoss_save.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingBatch(bs = 100):\n",
    "    source = testing_lines\n",
    "    source_classes = testing_classes\n",
    "    x = np.random.randint(0, len(testing_lines), (bs,))\n",
    "    _ctokens = [source[index][0] for index in x]\n",
    "    _etokens = [source[index][1] for index in x]\n",
    "    csegments = np.zeros((bs, max_len_claims))\n",
    "    esegments = np.zeros((bs, max_len_evid))\n",
    "    \n",
    "    _classes = [source_classes[index] for index in x]\n",
    "        \n",
    "    \"\"\"\n",
    "    _classes = np.asarray(_classes)\n",
    "    twos = _classes == 2\n",
    "    _classes[twos] = 1\n",
    "    \"\"\"\n",
    "    \n",
    "    ctokens = torch.LongTensor(_ctokens).cuda()\n",
    "    csegments = torch.LongTensor(csegments).cuda()\n",
    "    etokens = torch.LongTensor(_etokens).cuda()\n",
    "    esegments = torch.LongTensor(esegments).cuda()\n",
    "    classes = torch.LongTensor(_classes).cuda()\n",
    "    catt_mask = ctokens != 0\n",
    "    eatt_mask = etokens != 0\n",
    "    \n",
    "    return ctokens, csegments, catt_mask, etokens, esegments, eatt_mask, classes\n",
    "    \n",
    "ct, cs, ca, et, es, ea, classes_ = testingBatch()\n",
    "print(ct.size())\n",
    "print(cs.size())\n",
    "print(ca.size())\n",
    "print(et.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(network, bs=100, num_batches=10):\n",
    "    \n",
    "    classes = torch.LongTensor([]).cuda()\n",
    "    preds = torch.FloatTensor([]).cuda()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            ct, cs, ca, et, es, ea, classes_ = testingBatch(bs=bs)\n",
    "            y_ = network.forward(ct, cs, ca, et, es, ea)\n",
    "            classes = torch.cat([classes, classes_], dim=0)\n",
    "            preds = torch.cat([preds, y_], dim=0)\n",
    "        \n",
    "        evidences = classes >= 1\n",
    "        f_loss = getLoss(preds, classes, lossFn)\n",
    "        pred = torch.max(preds, dim=-1)[1]\n",
    "        acc = torch.sum(pred == classes)\n",
    "        acc = acc.cpu().numpy()/(bs*num_batches)\n",
    "        positives = torch.sum(pred[evidences] == classes[evidences])\n",
    "        \n",
    "        return f_loss.data.item(), acc, positives.cpu().numpy()/torch.sum(evidences).cpu().numpy(), preds, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, acc, p_acc, y_, y = validate(network)\n",
    "y_ = F.softmax(y_, dim=-1)\n",
    "y_ = torch.max(y_, dim=-1)[1]\n",
    "\n",
    "act_1 = y >= 1\n",
    "act_2 = y_ >= 1\n",
    "#print(act_1)\n",
    "#print(act_2)\n",
    "print(torch.sum(act_1 != act_2), torch.sum(act_1))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
