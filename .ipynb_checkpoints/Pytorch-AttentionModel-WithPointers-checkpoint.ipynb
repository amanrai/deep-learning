{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline Generation - PyTorch Implementation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pwd\n",
    "!ls ./sumdata/train\n",
    "!gunzip ./sumdata/train/train.article.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*Not really using fastai for this particular notebook. Import to explore Fastai options to the same things.*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "from fastai.fastai.imports import *\n",
    "from fastai.fastai.torch_imports import *\n",
    "from fastai.fastai.core import *\n",
    "from fastai.fastai.model import fit\n",
    "from fastai.fastai.dataset import *\n",
    "from fastai.fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pprint\n",
    "from random import randint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "config = []\n",
    "with open(\"./gigaword_attn_config.json\") as f:\n",
    "    config = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "val_inputs = []\n",
    "val_outputs = []\n",
    "counter = 0\n",
    "_dict = {}\n",
    "\n",
    "path = config[\"data_path\"]\n",
    "input_path = config[\"training\"][\"inputs\"]\n",
    "output_path = config[\"training\"][\"outputs\"]\n",
    "\n",
    "validation_input_path = config[\"validation\"][\"inputs\"]\n",
    "validation_output_path = config[\"validation\"][\"outputs\"]\n",
    "\n",
    "with open(path + input_path, \"r\") as f:\n",
    "    inputs = f.readlines()\n",
    "with open(path + output_path, \"r\") as f:\n",
    "    outputs = f.readlines()\n",
    "with open(path + validation_input_path, \"r\") as f:\n",
    "    val_inputs = f.readlines()\n",
    "with open(path + validation_output_path, \"r\") as f:\n",
    "    val_outputs = f.readlines()\n",
    "        \n",
    "print(\"Training Samples (x,y):\",len(inputs), len(outputs))\n",
    "print(\"Validation Samples (x,y):\", len(val_inputs), len(val_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Rebuild Dictionary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "def buildDictWordTokens(all_sources):\n",
    "    _dict = {}\n",
    "    for k in range(len(all_sources)):\n",
    "        counter = 0\n",
    "        for line in all_sources[k]:\n",
    "            counter += 1\n",
    "            if (counter %1000 == 0):\n",
    "                _str = str(k) + \"\\t\" + str(counter) + \"/\" + str(len(all_sources[k])) + \"\\r\"\n",
    "                sys.stdout.write(_str)\n",
    "                sys.stdout.flush()\n",
    "            for word in line.split():\n",
    "                if (word not in _dict):\n",
    "                    _dict[word] = 0\n",
    "                _dict[word] += 1\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "_dict = buildDictWordTokens([inputs, outputs])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "pickle.dump(_dict, open(config[\"dictionary_path\"], \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ...or load from disk instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "_dict = pickle.load(open(config[\"dictionary_path\"], \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_dict = sorted([(word, _dict[word]) for word in _dict], key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "set config[\"vocab_size\"]:-1 to use all words in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "add_tokens = config[\"dictionary_tokens\"]\n",
    "for i in range(len(add_tokens)):\n",
    "    _dict.insert(i, (add_tokens[i], 1))\n",
    "f_dict = {}\n",
    "for i in range(len(_dict)):\n",
    "    f_dict[_dict[i][0]] = i \n",
    "r_dict = [_dict[i][0] for i in range(len(_dict))]\n",
    "vocab_size = len(r_dict) if config[\"vocab_size\"] == -1 else config[\"vocab_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaffolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([r_dict[w] for w in range(34)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_token = config[\"oov_token\"]\n",
    "padding_token = config[\"padding_token\"]\n",
    "\n",
    "def generateBatch(x_source, y_source, input_ts=30, output_ts = 10, bs=64):\n",
    "    dont_copy_index = input_ts-1\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "    u_ = []\n",
    "    u_ind = []\n",
    "    while len(x_) < bs:\n",
    "        _u = []\n",
    "        _u_ind = []\n",
    "        l = randint(0, len(x_source)-1)\n",
    "        x = [f_dict[t] for t in x_source[l].split()]\n",
    "        y = [f_dict[t] for t in y_source[l].split()]\n",
    "        x = [t if t < vocab_size else f_dict[oov_token] for t in x]\n",
    "        y = [t if t < vocab_size else f_dict[oov_token] for t in y] \n",
    "        x = x[:input_ts]\n",
    "        while (len(x) < input_ts):\n",
    "            x.insert(0,f_dict[padding_token])\n",
    "        for i in range(len(y)):\n",
    "            word = y[i]   \n",
    "            if (word in x and word != oov_token):\n",
    "                _u.append(1)\n",
    "                _u_ind.append(x.index(word))\n",
    "                #y[i] = vocab_size + x.index(word)\n",
    "            else:\n",
    "                _u.append(0)\n",
    "                _u_ind.append(dont_copy_index)\n",
    "                \n",
    "        while (len(y) < output_ts):\n",
    "            y.append(f_dict[padding_token])\n",
    "            _u.append(0)\n",
    "            _u_ind.append(dont_copy_index)\n",
    "            \n",
    "        _u_ind = _u_ind[:output_ts]\n",
    "        _u = _u[:output_ts]\n",
    "        y = y[:output_ts]\n",
    "\n",
    "        \n",
    "        x_.append(x)\n",
    "        y_.append(y)\n",
    "        u_ind.append(_u_ind)\n",
    "        u_.append(_u)\n",
    "\n",
    "    return np.array(x_), np.array(y_), np.array(u_), np.array(u_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x,y,u,ind = generateBatch(val_inputs, val_outputs, input_ts=20, output_ts=10)\n",
    "x.shape, y.shape, u.shape, ind.shape\n",
    "np.max(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function **flattens along the batch and timesteps dimensions** and computes a Loss for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfn_multi(outputs, acts, criterion, input_ts = 30):\n",
    "    acts = acts.transpose(0,1).contiguous().view(-1)\n",
    "    outputs = outputs.view(-1, vocab_size)\n",
    "    return criterion(outputs, acts.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, lossfn, criterion, num_batches = 10, bs = 128, output_ts=10):\n",
    "    t_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        x, y, u, u_indices = generateBatch(val_inputs, val_outputs, output_ts = output_ts)\n",
    "        y = torch.LongTensor(y).cuda()\n",
    "        h = m.reinitInputHiddenState(1)\n",
    "        w,h,u = model(torch.from_numpy(x).cuda(), h, output_ts)\n",
    "        l = lossfn(w,y,criterion)\n",
    "        t_loss += l.item()\n",
    "    return t_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBatch(x,y,u,u_indices, \n",
    "               model, \n",
    "               optimizer, \n",
    "               criterion, \n",
    "               bs, \n",
    "               use_tf = False, \n",
    "               output_ts=10):\n",
    "    loss = 0\n",
    "\n",
    "    #print(y) \n",
    "    y = torch.LongTensor(y).cuda()\n",
    "    u = torch.LongTensor(u).cuda()\n",
    "    #u_app = torch.zeros_like(u).unsqueeze(0).cuda().permute(1,2,0)\n",
    "    \n",
    "    u_indices = torch.LongTensor(u_indices).cuda()\n",
    "    h = m.reinitInputHiddenState(bs)\n",
    "    w,h,u_pred = model(torch.from_numpy(x).cuda(), h, output_ts, y_acts=y, use_tf=True)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    \"\"\"\n",
    "    l1 = lossfn_u(u, u_pred)\n",
    "    loss += l1\n",
    "    l1.backward(retain_graph=True)\n",
    "    #optimizer.step()\n",
    "    \"\"\"\n",
    "    #optimizer.zero_grad()\n",
    "    l2 = lossfn_uind(u_indices, h, criterion)\n",
    "    loss += l2\n",
    "    l2.backward(retain_graph=True)\n",
    "    #optimizer.step()\n",
    "    \n",
    "    #optimizer.zero_grad()\n",
    "    l = lossfn_multi(w,y,criterion)\n",
    "    loss += l\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), l.item(), 0, l2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, generate_words = 10, print_attn=False):\n",
    "    x, y, u, u_ind = generateBatch(val_inputs, val_outputs, output_ts =generate_words)\n",
    "    h = m.reinitInputHiddenState(1)\n",
    "    outputs, atts, uts = model(torch.from_numpy(x).cuda(), h, generate_words, print_attn=print_attn)\n",
    "    outputs = outputs.exp()\n",
    "    words = torch.max(outputs, -1)[1].view(-1,outputs.size()[1]).permute(1,0)\n",
    "    samples = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_words = \" \".join([r_dict[word.item()] for word in x[i]])\n",
    "        y_act_words = \" \".join([r_dict[word.item()] for word in y[i]])\n",
    "        y_words = \" \".join([r_dict[word] for word in words[i]])\n",
    "        \"\"\"\n",
    "        if (print_attn):\n",
    "            print(\"ATTNS:\")\n",
    "            print(atts[i])\n",
    "        \"\"\"\n",
    "        f_ = {\n",
    "            \"text\": {\n",
    "                \"source\":x_words, \n",
    "                \"actual\":y_act_words, \n",
    "                \"predicted\":y_words\n",
    "            },\n",
    "            \"attention\":F.softmax(atts[i], dim=-1).cpu().detach().numpy().tolist()\n",
    "        }\n",
    "        samples.append(f_)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "network_losses = []\n",
    "ut_losses = []\n",
    "att_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=1, \n",
    "          batches=128, \n",
    "          optim=None, \n",
    "          criterion = None, \n",
    "          bs = 64, \n",
    "          output_ts=20, \n",
    "          use_tf=False, \n",
    "          lr=1e-3, \n",
    "          num_valid_batches=10):\n",
    "    \n",
    "    if optim == None:\n",
    "        optim = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    if criterion == None:\n",
    "        criterion = nn.NLLLoss()\n",
    "    for e in range(epochs):\n",
    "        rolling_loss = 0\n",
    "        rolling_time = 0\n",
    "        rolling_network = 0\n",
    "        rolling_ut = 0\n",
    "        rolling_att = 0\n",
    "        print(\"\\n\")\n",
    "        for b in range(batches):\n",
    "            b_start = time.time()\n",
    "            loss, network_loss, ut_loss, att_loss = trainBatch(*generateBatch(inputs, outputs, output_ts = output_ts, bs=bs), \n",
    "                              model, \n",
    "                              optim, \n",
    "                              criterion, \n",
    "                              bs, \n",
    "                              output_ts=output_ts,\n",
    "                              use_tf = use_tf)\n",
    "            rolling_loss += loss\n",
    "            rolling_network += network_loss\n",
    "            rolling_ut += ut_loss\n",
    "            rolling_att += att_loss\n",
    "            b_stop = time.time()\n",
    "            rolling_time += b_stop-b_start\n",
    "            avg_time = rolling_time/(b+1)\n",
    "            eta = (batches-b)*avg_time\n",
    "            _str = \"e\" + str(e+1) + \", batch: \" + \\\n",
    "                    str(b+1) + \"\\tloss:\" + \\\n",
    "                    \"{:10.3f}\".format(rolling_loss/(b+1)) + \\\n",
    "                    \" (\" + \\\n",
    "                    \"{:10.3f}\".format(rolling_network/(b+1)) + \\\n",
    "                    \",\" + \\\n",
    "                    \"{:10.3f}\".format(rolling_ut/(b+1)) + \\\n",
    "                    \",\" + \\\n",
    "                    \"{:10.3f}\".format(rolling_att/(b+1)) + \\\n",
    "                    \") \" + \\\n",
    "                    \" \\t\\teta: \" +  \\\n",
    "                    \"{:5.1f}\".format(eta) + \"s\\t\" + \\\n",
    "                    \"{:1.2f}\".format(avg_time) + \"s/batch\\r\"\n",
    "            sys.stdout.write(_str)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        losses.append(rolling_loss/batches)\n",
    "        #validate\n",
    "        valid_loss = validate(model, lossfn_multi, criterion, bs=bs, num_batches=num_valid_batches, output_ts=output_ts)        \n",
    "        print(\"\\n\")\n",
    "        print(\"validation loss:\", \"{:3.2f}\".format(valid_loss))\n",
    "\n",
    "        doSave = False        \n",
    "        if (len(val_losses) == 0):\n",
    "            doSave = True\n",
    "        elif (np.min(val_losses) > valid_loss):\n",
    "            doSave = True            \n",
    "        if (doSave):\n",
    "            print(\"Saving Model:\", config[\"save_model_path\"])\n",
    "            torch.save(model, config[\"save_model_path\"])    \n",
    "        val_losses.append(valid_loss)\n",
    "        network_losses.append(network_loss)\n",
    "        ut_losses.append(ut_loss)\n",
    "        att_losses.append(att_loss)\n",
    "        \n",
    "        with open(config[\"save_training_cycle_path\"], \"w\") as f:\n",
    "            f.write(json.dumps( {                \n",
    "                \"training_loss\":losses,\n",
    "                \"validation_loss\":val_losses,\n",
    "                \"network_loss\":network_losses,\n",
    "                \"ut_loss\":ut_losses,\n",
    "                \"att_loss\":att_losses\n",
    "            }))\n",
    "            f.close()\n",
    "        \n",
    "        #sample\n",
    "        samples = sample(m, generate_words=output_ts)  \n",
    "        _l = rolling_loss/batches\n",
    "        _samples = {\n",
    "            \"epochs\":len(losses),\n",
    "            \"used_tf\":use_tf,\n",
    "            \"loss\":_l,\n",
    "            \"val_loss\":valid_loss,\n",
    "            \"samples\":samples\n",
    "        }\n",
    "        with open(config[\"save_samples_path\"] + str(time.time()) + \"_.json\", \"w\") as f:\n",
    "            f.write(json.dumps(_samples, indent=4))\n",
    "            f.close()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_token = f_dict[config[\"start_token\"]]\n",
    "class customGRU(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size=128, \n",
    "                 embed_dim = 100, \n",
    "                 lstm_dim= 90, \n",
    "                 hidden_dim=64, \n",
    "                 bidirec=False, \n",
    "                 lstm_layers = 3,\n",
    "                 start_token = start_token):\n",
    "        super(customGRU, self).__init__()\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx = f_dict[padding_token])\n",
    "        self.start_token = start_token\n",
    "        self.input_lstm = nn.GRU(embed_dim, lstm_dim, num_layers=lstm_layers, dropout=0.1, bidirectional=True)\n",
    "        self.attn_W = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.w2i = nn.Parameter(torch.randn(hidden_dim + lstm_dim*2, hidden_dim))\n",
    "        self.dec_lstm = nn.GRU(embed_dim, hidden_dim)\n",
    "        self.decoder = nn.Parameter(torch.randn(self.hidden_dim + self.lstm_dim*2, embed_dim))\n",
    "        self.ut = nn.Parameter(torch.randn(self.hidden_dim + self.lstm_dim*2, 1))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.norm_softmax = nn.Softmax(dim=-1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, hidden, output_ts, use_tf=False, y_acts=None, train=True, print_attn=False):\n",
    "        bs, ts = x.size()\n",
    "        x_ = x.permute(1,0)\n",
    "        o = self.embed(x_) #b,ts,embed\n",
    "        y_ = None\n",
    "        if (y_acts is not None):\n",
    "            y_ = y_acts.permute(1,0)\n",
    "        i_lh, i_h = self.input_lstm(o)\n",
    "        h = i_lh[-1, :, :].unsqueeze(0)\n",
    "        attn_ = torch.zeros(1, 1, self.lstm_dim*2)\n",
    "        pw = np.zeros((bs,1))\n",
    "        pw[:,0] = start_token\n",
    "        pw = torch.LongTensor(pw).cuda().permute(1,0)\n",
    "        o_wh = []\n",
    "        atts_ = []\n",
    "        uts_ = []\n",
    "        for i in range(output_ts):\n",
    "            pw = self.embed(pw)\n",
    "            h,h_ = self.dec_lstm(pw, h)    \n",
    "            #Attention Calculations\n",
    "            a = torch.matmul(h, self.attn_W) #1, b, hidden_dim\n",
    "            a = a.permute(1,0,2) #b,n,m <- b, 1, hidden_dim\n",
    "            b = i_lh.permute(1,2,0) #b,m,p <- b, hidden_dim, ts\n",
    "            e = torch.bmm(a,b) #b,n,p <= b, 1, ts\n",
    "            alpha = F.softmax(e, dim=-1)\n",
    "            \n",
    "            if (print_attn):\n",
    "                print(\"ALPHA:\", alpha.size())\n",
    "                print(\"ALPHA[0] SUM:\",torch.sum(alpha,-1)[0])\n",
    "                print(\"ALPHA[0]:\",alpha[0])\n",
    "                print(\"E:\", e.size())\n",
    "                print(\"E[0]:\",e[0])\n",
    "                print(\"I_LH_PERM:\", b.size())\n",
    "                print(\"A_PERM:\", a.size())\n",
    "                print(\"a_matmul\", a.size())\n",
    "                print(\"w_attn:\", self.attn_W)\n",
    "                print(\"h[0]\", h[0])\n",
    "                print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "                #print(alpha)\n",
    "                \n",
    "            #atts_.append(e.view(-1, ts+1)) \n",
    "            atts_.append(e.view(-1,ts))\n",
    "            #alpha = alpha #ts, b\n",
    "            \n",
    "            #output creation\n",
    "            #alpha_mult = alpha[:,:,:ts]\n",
    "            att_out = torch.sum(i_lh*alpha.view(-1, x.size()[0], 1),0) #b, hidden_dim\n",
    "            att_out = att_out.unsqueeze(0) #1, b, hidden_dim\n",
    "            h_att = torch.cat([h, att_out], -1) \n",
    "            \n",
    "            \n",
    "            ut = torch.matmul(h_att, self.ut)\n",
    "            uts_.append(self.sigmoid(ut))\n",
    "            \n",
    "            \n",
    "            w_proj = torch.matmul(h_att, self.decoder)\n",
    "            w_ = self.log_softmax(torch.matmul(w_proj, torch.transpose(self.embed.weight, 0, 1)))\n",
    "            #f_out = torch.cat([w_, alpha.permute(1,0,2)], -1)\n",
    "\n",
    "            o_wh.append(w_)\n",
    "            \n",
    "            #GRU State Management\n",
    "            pw = torch.max(w_,-1)[1]\n",
    "            if ((use_tf and torch.randint(11, (1,))[0] > 3)):\n",
    "                pw = y_[i,:].unsqueeze(0)\n",
    "            \n",
    "        o_wh = torch.stack(o_wh, 0).squeeze(1)\n",
    "        atts_ = torch.stack(atts_, 0)\n",
    "        atts_ = atts_.permute(1, 0, 2)\n",
    "        uts_ = torch.stack(uts_, 0).squeeze(1).permute(1,0,2)\n",
    "        return o_wh, atts_, uts_\n",
    "    \n",
    "    def reinitInputHiddenState(self,bs):\n",
    "        return torch.zeros((self.lstm_layers, bs, self.lstm_dim)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m = customGRU(vocab_size = vocab_size, \n",
    "              hidden_dim = 256, \n",
    "              embed_dim=300, \n",
    "              lstm_dim = 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples = sample(m, print_attn=False)    # test that sampling works without errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfn_u(outputs_u, acts):\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "    acts = acts.contiguous().view(-1, acts.size(-1))\n",
    "    outputs = outputs_u.view(-1,1)\n",
    "    l = loss(outputs.float(), acts)\n",
    "    return l\n",
    "    \n",
    "def lossfn_uind(acts_ind, outputs_ind, criterion):\n",
    "    loss2 = nn.CrossEntropyLoss()\n",
    "    acts_ind = acts_ind.transpose(0,1).contiguous().view(-1)\n",
    "    outputs_ind = outputs_ind.permute(1,0,2)\n",
    "    outputs_ind = outputs_ind.contiguous().view(-1, outputs_ind.size()[-1])\n",
    "    l2 = loss2(outputs_ind, acts_ind)\n",
    "    return l2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = [10] \n",
    "batches = [2500, 1000]\n",
    "tf = [True, True]\n",
    "lrs = [1e-3, 1e-4]\n",
    "\n",
    "_output_ts = 10\n",
    "_bs = 128\n",
    "optim = torch.optim.Adam(m.parameters(), lr=1e-3) \n",
    "\n",
    "\n",
    "while True:\n",
    "    for i in range(len(epochs)):\n",
    "        print(\"\\n\")\n",
    "        print(str(epochs[i]) + \" epoch(s) (\" + str(batches[i]) + \" batches of \" + str(_bs) + \" samples each.) Teacher Forcing:\", tf[i])\n",
    "        e = epochs[i]\n",
    "        b = batches[i]\n",
    "        _losses = train(m, \n",
    "                        epochs=e, \n",
    "                        batches=b, \n",
    "                        optim = optim, \n",
    "                        output_ts=_output_ts, \n",
    "                        use_tf=tf[i], \n",
    "                        bs=_bs)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
