{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:05.131649Z",
     "start_time": "2019-05-02T07:28:03.912520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:36.753223Z",
     "start_time": "2019-05-02T07:28:05.133256Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"../../../Data/jigsaw/training_processed.pickle\"\n",
    "all_data = pickle.load(open(data_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:36.769043Z",
     "start_time": "2019-05-02T07:28:36.754938Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(all_data[\"all_data\"]), len(all_data[\"positives\"]), len(all_data[\"negatives\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:36.833200Z",
     "start_time": "2019-05-02T07:28:36.770937Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_cutoff = len(all_data[\"positives\"])//10\n",
    "negative_cutoff = len(all_data[\"negatives\"])//10\n",
    "\n",
    "testing_positives = all_data[\"positives\"][-positive_cutoff:]\n",
    "testing_negatives = all_data[\"negatives\"][-negative_cutoff:]\n",
    "\n",
    "testing_data = [(index, 1) for index in testing_positives]\n",
    "testing_data.extend([(index, 0) for index in testing_negatives])\n",
    "\n",
    "np.random.shuffle(testing_data)\n",
    "print(testing_data[:100])\n",
    "\n",
    "training_positives = all_data[\"positives\"][:-positive_cutoff]\n",
    "training_negatives = all_data[\"negatives\"][:-negative_cutoff]\n",
    "\n",
    "_data = all_data[\"all_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:36.837504Z",
     "start_time": "2019-05-02T07:28:36.834494Z"
    }
   },
   "outputs": [],
   "source": [
    "max_comment_length = 230 #99th percentile of comment lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:38.785915Z",
     "start_time": "2019-05-02T07:28:36.839876Z"
    }
   },
   "outputs": [],
   "source": [
    "def genBatch(bs=8, testing = False, cuda=True):\n",
    "    \n",
    "    _positive_data = training_positives\n",
    "    _negative_data = training_negatives\n",
    "    \n",
    "    _batch = None\n",
    "    if (not testing):\n",
    "        #during training always present a balanced training set\n",
    "        positive_samples = bs//2\n",
    "        negative_samples = bs - positive_samples\n",
    "        _p = np.random.randint(0, len(_positive_data), (positive_samples,))\n",
    "        _n = np.random.randint(0, len(_negative_data), (negative_samples,))\n",
    "\n",
    "        _batch = [(tokenizer.convert_tokens_to_ids(_data[index][\"comment_text\"]), 1) \n",
    "                  for index in _p]\n",
    "        _batch.extend([(tokenizer.convert_tokens_to_ids(_data[index][\"comment_text\"]), 0) \n",
    "                       for index in _n])\n",
    "    else:\n",
    "        _batch = []\n",
    "        _indices = np.random.randint(0, len(testing_data), (bs,))\n",
    "        for list_index in _indices:\n",
    "            _index = testing_data[list_index][0]\n",
    "            _class = testing_data[list_index][1]\n",
    "            _batch.append((tokenizer.convert_tokens_to_ids(_data[_index][\"comment_text\"]), _class))\n",
    "        \n",
    "    np.random.shuffle(_batch)\n",
    "    _docs = [dp[0] for dp in _batch]\n",
    "    _y = [dp[1] for dp in _batch]\n",
    "    \n",
    "    docs = []\n",
    "    for _doc in _docs:\n",
    "        _doc.insert(0, 101)\n",
    "        while (len(_doc) < max_comment_length):\n",
    "            _doc.append(0)\n",
    "        docs.append(_doc[:max_comment_length])\n",
    "    \n",
    "    docs = np.asarray(docs)\n",
    "    segments = np.zeros(docs.shape)\n",
    "    y = np.asarray(_y)\n",
    "    if (cuda):\n",
    "        docs = torch.LongTensor(docs).cuda()\n",
    "        segments = torch.LongTensor(segments).cuda()\n",
    "        y = torch.FloatTensor(y).cuda()\n",
    "    else:\n",
    "        docs = torch.LongTensor(docs)\n",
    "        segments = torch.LongTensor(segments)\n",
    "        y = torch.FloatTensor(y)\n",
    "    mask = docs > 0\n",
    "    \n",
    "    return docs, segments, mask, y\n",
    "\n",
    "d, se, m, y = genBatch(bs=12, testing=False, cuda=True)\n",
    "print(d.size(), se.size(), m.size(), y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:44.174537Z",
     "start_time": "2019-05-02T07:28:38.787205Z"
    }
   },
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:44.181370Z",
     "start_time": "2019-05-02T07:28:44.176066Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, dim=64, bert_model = \"bert-base-uncased\"):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.bert_dim = 768\n",
    "        if (\"large\" in bert_model):\n",
    "            self.bert_dim = 1024\n",
    "        self.w = torch.nn.Linear(self.bert_dim, dim)\n",
    "        self.v = torch.nn.Linear(dim,1)\n",
    "        self.o = torch.nn.Linear(self.bert_dim, dim)\n",
    "    \n",
    "    def forward(self, _d):\n",
    "        _att = torch.tanh(self.w(_d))\n",
    "        _att = self.v(_att)\n",
    "        _att = F.softmax(_att, dim=1)\n",
    "        _o = _d * _att\n",
    "        _o = torch.sum(_o, dim=1)\n",
    "        return self.o(_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:44.195807Z",
     "start_time": "2019-05-02T07:28:44.183539Z"
    }
   },
   "outputs": [],
   "source": [
    "class multiHeadedClassifier(torch.nn.Module):\n",
    "    def __init__(self, attention_heads = 8, \n",
    "                 attention_head_dim=64,\n",
    "                 bert_model = \"bert-base-uncased\",\n",
    "                 output_dims = 1\n",
    "                ):\n",
    "        super(multiHeadedClassifier, self).__init__()\n",
    "        self.attentions = torch.nn.ModuleList([])\n",
    "        self.bert_dim = 768\n",
    "        if (\"large\" in bert_model):\n",
    "            self.bert_dim = 1024\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\").cuda()\n",
    "        for i in range(attention_heads):\n",
    "            self.attentions.append(AttentionHead(dim=attention_head_dim, bert_model=bert_model))\n",
    "        self.output = torch.nn.Linear(self.bert_dim + attention_heads*attention_head_dim, 1)\n",
    "    \n",
    "    def forward(self, d, se, m):\n",
    "        _d, _ = self.bert(d, se, m, output_all_encoded_layers=False)\n",
    "        att_outs = []\n",
    "        for i in range(len(self.attentions)):\n",
    "            head_out = self.attentions[i](_d)\n",
    "            att_outs.append(head_out)\n",
    "        att_outs = torch.cat(att_outs, dim=-1).unsqueeze(1)\n",
    "        return self.output(torch.cat([_d[:,0,:].unsqueeze(1), att_outs], dim=-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:49.596830Z",
     "start_time": "2019-05-02T07:28:44.196890Z"
    }
   },
   "outputs": [],
   "source": [
    "network = multiHeadedClassifier()\n",
    "network.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:49.602046Z",
     "start_time": "2019-05-02T07:28:49.598264Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optim = torch.optim.SGD(network.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:52.218625Z",
     "start_time": "2019-05-02T07:28:51.973475Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "def validate(network, batches=10, bs=12):\n",
    "    print(\"\\tValidating...\")\n",
    "    y_preds = []\n",
    "    y_acts = []\n",
    "    for i in range(batches):\n",
    "        with torch.no_grad():\n",
    "            d, se, m, y = genBatch(bs=12, testing=True, cuda=True)\n",
    "            y_pred = network.forward(d, se, m)\n",
    "            if (len(y_preds) == 0):\n",
    "                y_preds = y_pred\n",
    "                y_acts = y\n",
    "            else:\n",
    "                y_preds = torch.cat([y_preds, y_pred], dim=0)\n",
    "                y_acts = torch.cat([y_acts, y])\n",
    "\n",
    "    loss = loss_fn(y_preds, y_acts.unsqueeze(-1))\n",
    "    score = roc(y_acts.cpu().numpy(), y_preds.cpu().numpy())\n",
    "    print(\"\\t\", np.round(loss.data.item(), 5), np.round(score,2))\n",
    "    return loss.data.item(), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:53.203733Z",
     "start_time": "2019-05-02T07:28:53.193357Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_losses = []\n",
    "epoch_losses = []\n",
    "validation_rocs = []\n",
    "\n",
    "def _save(network, cause):\n",
    "    print(\"\\tSaving model for cause:\", cause)\n",
    "    torch.save(network.state_dict(), \"./UnbiasedToxicity_\" + cause + \".h5\")\n",
    "    _trgcycle = {\n",
    "        \"training_losses\":epoch_losses,\n",
    "        \"validation_losses\":validation_losses,\n",
    "        \"roc_auc\":validation_rocs\n",
    "    }\n",
    "    with open(\"./TrainingCycle_UbiasedToxicity_\" + cause + \".json\", \"w\") as f:\n",
    "        f.write(json.dumps(_trgcycle))\n",
    "        f.close()\n",
    "    \n",
    "def saveModel(network):\n",
    "    loss, roc_auc = validate(network)\n",
    "    validation_losses.append(loss)\n",
    "    validation_rocs.append(roc_auc)\n",
    "    \n",
    "    if (np.min(epoch_losses) == epoch_losses[-1]):\n",
    "        _save(network,\"BestTrainingLoss\")\n",
    "    \n",
    "    if (np.min(validation_losses) == validation_losses[-1]):\n",
    "        _save(network, \"BestValidationLoss\")\n",
    "    \n",
    "    if (np.max(validation_rocs) == validation_rocs[-1]):\n",
    "        _save(network, \"BestRoCAUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:54.272032Z",
     "start_time": "2019-05-02T07:28:54.258302Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer=None, loss=None, epochs=2, batches_per_epoch=10, bs=12):\n",
    "    for k in range(epochs):\n",
    "        batch_losses = []\n",
    "        for j in range(batches_per_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            d, se, m, y = genBatch(bs=bs, testing=False, cuda=True)\n",
    "            y_pred = network.forward(d, se, m)\n",
    "            loss = loss_fn(y_pred, y.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y = y.detach().cpu().numpy()\n",
    "            y_ = torch.sigmoid(y_pred).detach().cpu().numpy()\n",
    "            batch_losses.append(loss.data.item())\n",
    "            _str = \"Epoch: \" + str(k + 1) + \"; Batch: (\" + str(j+1) + \"/\" + str(batches_per_epoch) + \")\"\n",
    "            _str = _str + \"\\tLoss (AUC last batch): \" + str(np.round(np.mean(batch_losses), 5)) + \\\n",
    "                    \"(\" + str(np.round(roc(y,y_), 2)) + \")\"\n",
    "            print(_str, end=\"\\r\")\n",
    "        print(\"\\n\")\n",
    "        epoch_losses.append(np.mean(batch_losses))\n",
    "        saveModel(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:36:19.271029Z",
     "start_time": "2019-05-02T07:28:55.171539Z"
    }
   },
   "outputs": [],
   "source": [
    "train(network, optimizer=optim, loss=loss_fn, epochs=25, bs=8, batches_per_epoch=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
