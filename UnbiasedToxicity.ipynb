{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:05.131649Z",
     "start_time": "2019-05-02T07:28:03.912520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:36.753223Z",
     "start_time": "2019-05-02T07:28:05.133256Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"../../training_processed.pickle\"\n",
    "all_data = pickle.load(open(data_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:36.769043Z",
     "start_time": "2019-05-02T07:28:36.754938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804874 144334 1660540\n"
     ]
    }
   ],
   "source": [
    "print(len(all_data[\"all_data\"]), len(all_data[\"positives\"]), len(all_data[\"negatives\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:36.833200Z",
     "start_time": "2019-05-02T07:28:36.770937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1682435, 0), (1707091, 0), (1713097, 0), (1708607, 0), (1752428, 0), (1656033, 0), (1656156, 0), (1707064, 0), (1789557, 0), (1701123, 0), (1650975, 0), (1777440, 0), (1732504, 0), (1653565, 1), (1772303, 0), (1770677, 0), (1673426, 0), (1689119, 0), (1803687, 0), (1770855, 0), (1653652, 0), (1623271, 0), (1677961, 0), (1676342, 0), (1649443, 0), (1677600, 0), (1709604, 0), (1731488, 0), (1740718, 0), (1631333, 0), (1780866, 0), (1757041, 0), (1737841, 0), (1790235, 0), (1794909, 0), (1642023, 1), (1658839, 1), (1641979, 0), (1716965, 0), (1781053, 0), (1730260, 0), (1803365, 0), (1727028, 0), (1669843, 0), (1798146, 0), (1802525, 0), (1764431, 0), (1731276, 0), (1709317, 0), (1693578, 0), (1653707, 0), (1718775, 0), (1654218, 0), (1659307, 0), (1717985, 0), (1777219, 0), (1783406, 0), (1722962, 0), (1631763, 0), (1655470, 1), (1777103, 0), (1706875, 0), (1711440, 0), (1774030, 0), (1690568, 1), (1671390, 0), (1737662, 1), (1691312, 0), (1796051, 0), (1690018, 0), (1778305, 1), (1639402, 0), (1690058, 0), (1801934, 0), (1779154, 0), (1651862, 0), (1645235, 0), (1780283, 0), (1718056, 0), (1719505, 0), (1776105, 0), (1651839, 0), (1791111, 0), (1671948, 0), (1763020, 0), (1656798, 0), (1643846, 0), (1725580, 0), (1727646, 0), (1780944, 0), (1803196, 0), (1659817, 1), (1655940, 0), (1705400, 0), (1636179, 0), (1759998, 0), (1727132, 0), (1706743, 0), (1777099, 0), (1787776, 0)]\n"
     ]
    }
   ],
   "source": [
    "positive_cutoff = len(all_data[\"positives\"])//10\n",
    "negative_cutoff = len(all_data[\"negatives\"])//10\n",
    "\n",
    "testing_positives = all_data[\"positives\"][-positive_cutoff:]\n",
    "testing_negatives = all_data[\"negatives\"][-negative_cutoff:]\n",
    "\n",
    "testing_data = [(index, 1) for index in testing_positives]\n",
    "testing_data.extend([(index, 0) for index in testing_negatives])\n",
    "\n",
    "np.random.shuffle(testing_data)\n",
    "print(testing_data[:100])\n",
    "\n",
    "training_positives = all_data[\"positives\"][:-positive_cutoff]\n",
    "training_negatives = all_data[\"negatives\"][:-negative_cutoff]\n",
    "\n",
    "_data = all_data[\"all_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:36.837504Z",
     "start_time": "2019-05-02T07:28:36.834494Z"
    }
   },
   "outputs": [],
   "source": [
    "max_comment_length = 120 #99th percentile of comment lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:38.785915Z",
     "start_time": "2019-05-02T07:28:36.839876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 120]) torch.Size([12, 120]) torch.Size([12, 120]) torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "def genBatch(bs=8, testing = False, cuda=True):\n",
    "    \n",
    "    _positive_data = training_positives\n",
    "    _negative_data = training_negatives\n",
    "    \n",
    "    _batch = None\n",
    "    if (not testing):\n",
    "        #during training always present a balanced training set\n",
    "        positive_samples = bs//2\n",
    "        negative_samples = bs - positive_samples\n",
    "        _p = np.random.randint(0, len(_positive_data), (positive_samples,))\n",
    "        _n = np.random.randint(0, len(_negative_data), (negative_samples,))\n",
    "\n",
    "        _batch = [(tokenizer.convert_tokens_to_ids(_data[index][\"comment_text\"]), 1) \n",
    "                  for index in _p]\n",
    "        _batch.extend([(tokenizer.convert_tokens_to_ids(_data[index][\"comment_text\"]), 0) \n",
    "                       for index in _n])\n",
    "    else:\n",
    "        _batch = []\n",
    "        _indices = np.random.randint(0, len(testing_data), (bs,))\n",
    "        for list_index in _indices:\n",
    "            _index = testing_data[list_index][0]\n",
    "            _class = testing_data[list_index][1]\n",
    "            _batch.append((tokenizer.convert_tokens_to_ids(_data[_index][\"comment_text\"]), _class))\n",
    "        \n",
    "    np.random.shuffle(_batch)\n",
    "    _docs = [dp[0] for dp in _batch]\n",
    "    _y = [dp[1] for dp in _batch]\n",
    "    \n",
    "    docs = []\n",
    "    for _doc in _docs:\n",
    "        _doc.insert(0, 101)\n",
    "        while (len(_doc) < max_comment_length):\n",
    "            _doc.append(0)\n",
    "        docs.append(_doc[:max_comment_length])\n",
    "        \n",
    "    docs = np.asarray(docs)\n",
    "    segments = np.zeros(docs.shape)\n",
    "    y = np.asarray(_y)\n",
    "    if (cuda):\n",
    "        docs = torch.LongTensor(docs).cuda()\n",
    "        segments = torch.LongTensor(segments).cuda()\n",
    "        y = torch.FloatTensor(y).cuda()\n",
    "    else:\n",
    "        docs = torch.LongTensor(docs)\n",
    "        segments = torch.LongTensor(segments)\n",
    "        y = torch.FloatTensor(y)\n",
    "    mask = docs > 0\n",
    "    \n",
    "    return docs, segments, mask, y\n",
    "\n",
    "d, se, m, y = genBatch(bs=12, testing=False, cuda=True)\n",
    "print(d.size(), se.size(), m.size(), y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:44.181370Z",
     "start_time": "2019-05-02T07:28:44.176066Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, dim=64, bert_model = \"bert-base-uncased\"):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.bert_dim = 768\n",
    "        if (\"large\" in bert_model):\n",
    "            self.bert_dim = 1024\n",
    "        self.w = torch.nn.Linear(self.bert_dim, dim)\n",
    "        self.v = torch.nn.Linear(dim,1)\n",
    "        self.o = torch.nn.Linear(self.bert_dim, dim)\n",
    "    \n",
    "    def forward(self, _d):\n",
    "        _att = torch.tanh(self.w(_d))\n",
    "        _att = self.v(_att)\n",
    "        _att = F.softmax(_att, dim=1)\n",
    "        _o = _d * _att\n",
    "        _o = torch.sum(_o, dim=1)\n",
    "        return self.o(_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:44.195807Z",
     "start_time": "2019-05-02T07:28:44.183539Z"
    }
   },
   "outputs": [],
   "source": [
    "class multiHeadedClassifier(torch.nn.Module):\n",
    "    def __init__(self, attention_heads = 1, \n",
    "                 attention_head_dim=512,\n",
    "                 bert_model = \"bert-base-uncased\",\n",
    "                 output_dims = 1\n",
    "                ):\n",
    "        super(multiHeadedClassifier, self).__init__()\n",
    "        self.attentions = torch.nn.ModuleList([])\n",
    "        self.bert_dim = 768\n",
    "        if (\"large\" in bert_model):\n",
    "            self.bert_dim = 1024\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\").cuda()\n",
    "        for i in range(attention_heads):\n",
    "            self.attentions.append(AttentionHead(dim=attention_head_dim, bert_model=bert_model))\n",
    "        self.output = torch.nn.Linear(self.bert_dim + attention_heads*attention_head_dim, 1)\n",
    "    \n",
    "    def forward(self, d, se, m):\n",
    "        _d, _ = self.bert(d, se, m, output_all_encoded_layers=False)\n",
    "        _d = _d*m.unsqueeze(-1).float()\n",
    "        att_outs = []\n",
    "        for i in range(len(self.attentions)):\n",
    "            head_out = self.attentions[i](_d)\n",
    "            att_outs.append(head_out)\n",
    "        att_outs = torch.cat(att_outs, dim=-1).unsqueeze(1)\n",
    "        _out = torch.tanh(torch.cat([_d[:,0,:].unsqueeze(1), att_outs], dim=-1))\n",
    "        return self.output(_out).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:49.596830Z",
     "start_time": "2019-05-02T07:28:44.196890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiHeadedClassifier(\n",
       "  (attentions): ModuleList(\n",
       "    (0): AttentionHead(\n",
       "      (w): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (o): Linear(in_features=768, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (output): Linear(in_features=1280, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = multiHeadedClassifier()\n",
    "network.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:49.602046Z",
     "start_time": "2019-05-02T07:28:49.598264Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optim = torch.optim.SGD(network.parameters(), lr=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:52.218625Z",
     "start_time": "2019-05-02T07:28:51.973475Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "def validate(network, batches=10, bs=64):\n",
    "    print(\"\\tValidating...\")\n",
    "    y_preds = []\n",
    "    y_acts = []\n",
    "    for i in range(batches):\n",
    "        with torch.no_grad():\n",
    "            d, se, m, y = genBatch(bs=12, testing=True, cuda=True)\n",
    "            y_pred = network.forward(d, se, m)\n",
    "            if (len(y_preds) == 0):\n",
    "                y_preds = y_pred\n",
    "                y_acts = y\n",
    "            else:\n",
    "                y_preds = torch.cat([y_preds, y_pred], dim=0)\n",
    "                y_acts = torch.cat([y_acts, y])\n",
    "\n",
    "    loss = loss_fn(y_preds, y_acts.unsqueeze(-1))\n",
    "    score = roc(y_acts.cpu().numpy(), y_preds.cpu().numpy())\n",
    "    print(\"\\t\", np.round(loss.data.item(), 5), np.round(score,2))\n",
    "    return loss.data.item(), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:53.203733Z",
     "start_time": "2019-05-02T07:28:53.193357Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_losses = []\n",
    "epoch_losses = []\n",
    "validation_rocs = []\n",
    "\n",
    "def _save(network, cause):\n",
    "    print(\"\\tSaving model for cause:\", cause)\n",
    "    torch.save(network.state_dict(), \"./UnbiasedToxicity_\" + cause + \".h5\")\n",
    "    _trgcycle = {\n",
    "        \"training_losses\":epoch_losses,\n",
    "        \"validation_losses\":validation_losses,\n",
    "        \"roc_auc\":validation_rocs\n",
    "    }\n",
    "    with open(\"./TrainingCycle_UbiasedToxicity_\" + cause + \".json\", \"w\") as f:\n",
    "        f.write(json.dumps(_trgcycle))\n",
    "        f.close()\n",
    "    \n",
    "def saveModel(network):\n",
    "    loss, roc_auc = validate(network)\n",
    "    validation_losses.append(loss)\n",
    "    validation_rocs.append(roc_auc)\n",
    "    \n",
    "    if (np.min(epoch_losses) == epoch_losses[-1]):\n",
    "        _save(network,\"BestTrainingLoss\")\n",
    "    \n",
    "    if (np.min(validation_losses) == validation_losses[-1]):\n",
    "        _save(network, \"BestValidationLoss\")\n",
    "    \n",
    "    if (np.max(validation_rocs) == validation_rocs[-1]):\n",
    "        _save(network, \"BestRoCAUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:28:54.272032Z",
     "start_time": "2019-05-02T07:28:54.258302Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer=None, loss_function=None, epochs=2, batches_per_epoch=10, bs=12):\n",
    "    for k in range(epochs):\n",
    "        batch_losses = []\n",
    "        batch_rocs = []\n",
    "        for j in range(batches_per_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            d, se, m, y = genBatch(bs=bs, testing=False, cuda=True)\n",
    "            y_pred = network.forward(d, se, m)\n",
    "            loss = loss_function(y_pred, y.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y = y.detach().cpu().numpy()\n",
    "            y_ = torch.sigmoid(y_pred).detach().cpu().numpy()\n",
    "            batch_losses.append(loss.data.item())\n",
    "            roc_ = roc(y,y_)\n",
    "            batch_rocs.append(roc_)\n",
    "            _str = \"Epoch: \" + str(k + 1) + \"; Batch: (\" + str(j+1) + \"/\" + str(batches_per_epoch) + \")\"\n",
    "            _str = _str + \"\\tLoss: \" + str(np.round(np.mean(batch_losses), 5)) + \\\n",
    "                    \"; AUC:\" + str(np.round(np.mean(batch_rocs), 2))\n",
    "            print(_str, end=\"\\r\")\n",
    "        print(\"\\n\")\n",
    "        epoch_losses.append(np.mean(batch_losses))\n",
    "        saveModel(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T07:36:19.271029Z",
     "start_time": "2019-05-02T07:28:55.171539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Batch: (500/500)\tLoss: 0.64573; AUC:0.68\n",
      "\n",
      "\tValidating...\n",
      "\t 0.59894 0.53\n",
      "\tSaving model for cause: BestTrainingLoss\n",
      "\tSaving model for cause: BestValidationLoss\n",
      "\tSaving model for cause: BestRoCAUC\n",
      "Epoch: 2; Batch: (500/500)\tLoss: 0.61199; AUC:0.72\n",
      "\n",
      "\tValidating...\n",
      "\t 0.74811 0.44\n",
      "\tSaving model for cause: BestTrainingLoss\n",
      "Epoch: 3; Batch: (161/500)\tLoss: 0.60381; AUC:0.72\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-79d2a6ed49de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-98629c00f362>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(network, optimizer, loss, epochs, batches_per_epoch, bs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(network, optimizer=optim, loss=loss_fn, epochs=20, bs=64, batches_per_epoch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
