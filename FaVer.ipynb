{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pickle.load(open(\"../fever_processed.pickle\", \"rb\"))\n",
    "testing_data = np.asarray(processed_data[-len(processed_data)//10:])\n",
    "training_data = np.asarray(processed_data[:-len(processed_data)//10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getSamples(data):\n",
    "    classes = [line[\"label\"] for line in data]\n",
    "    positive_samples = np.asarray(classes) ==\"SUPPORTS\"\n",
    "    positive_samples = np.asarray([i for i in range(len(positive_samples)) if positive_samples[i] == True])\n",
    "    negative_samples = np.asarray(classes) ==\"REFUTES\"\n",
    "    negative_samples = np.asarray([i for i in range(len(negative_samples)) if negative_samples[i] == True])\n",
    "    print(len(positive_samples), len(negative_samples))\n",
    "    return positive_samples, negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_p, tr_n = getSamples(training_data)\n",
    "te_p, te_n = getSamples(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 300\n",
    "max_claim_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(bs = 64, validation = False):\n",
    "    source = training_data\n",
    "    positive_samples = tr_p\n",
    "    negative_samples = tr_n\n",
    "    \n",
    "    if (validation):\n",
    "        source = testing_data\n",
    "        positive_samples = te_p\n",
    "        negative_samples = te_n\n",
    "    \n",
    "    n_samples = bs // 2\n",
    "    p_samples = bs - n_samples\n",
    "    positives = np.random.randint(0, len(positive_samples), (p_samples,))\n",
    "    negatives = np.random.randint(0, len(negative_samples), (n_samples,))\n",
    "    positives = positive_samples[positives]\n",
    "    negatives = negative_samples[negatives]\n",
    "    \n",
    "    all_indices = []\n",
    "    all_indices.extend(positives)\n",
    "    all_indices.extend(negatives)\n",
    "    \n",
    "    _t = []\n",
    "    _s = []\n",
    "    _a = []\n",
    "    _c = []\n",
    "    \n",
    "    for index in all_indices:\n",
    "        _dp = [\"[CLS]\"]\n",
    "        _dp.extend(source[index][\"processed\"][\"claim\"])\n",
    "        _dp.append(\"[SEP]\")\n",
    "        for evid in source[index][\"processed\"][\"evidentiary\"]:\n",
    "            _dp.extend(evid)\n",
    "        _dp.append(\"[SEP]\")\n",
    "        \n",
    "        while (len(_dp) < max_length):\n",
    "            _dp.append(\"[PAD]\")\n",
    "        _dp = _dp[:max_length]\n",
    "        _dp = np.asarray(tokenizer.convert_tokens_to_ids(_dp))\n",
    "        segments = np.ones((max_length,))\n",
    "        segments[:len(source[index][\"processed\"][\"claim\"]) + 2] = 0\n",
    "        _class = 1 if source[index][\"label\"] == \"SUPPORTS\" else 0\n",
    "        att_mask = [1 if _dp[index] >0 else 0 for index in range(len(_dp))]\n",
    "        _t.append(_dp)\n",
    "        _s.append(segments)\n",
    "        _a.append(att_mask)\n",
    "        _c.append(_class)\n",
    "    \n",
    "    text = torch.LongTensor(_t).cuda()\n",
    "    segments = torch.LongTensor(_s).cuda()\n",
    "    att = torch.LongTensor(_a).cuda()\n",
    "    classes = torch.FloatTensor(_c).cuda()\n",
    "    \n",
    "    return text, segments, att, classes\n",
    "    #np.random.shuffle(_data)\n",
    "t, s, a, c = getBatch(bs = 5, validation = False)\n",
    "print(t.size(), s.size(), a.size(), c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from QA_Attentions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "class FaVer(torch.nn.Module):\n",
    "    def __init__(self, bert_model = \"bert-base-uncased\"):\n",
    "        super(FaVer, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.bert_width = 768\n",
    "        if (\"-large-\" in self.bert_model):\n",
    "            self.bert_width = 1024\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.wd = torch.nn.Parameter(torch.FloatTensor(np.random.uniform(0, 1, (3*self.bert_width,))))\n",
    "        #self.innerAttQuery = torch.nn.Parameter(torch.FloatTensor(np.random.uniform(0, 1, (self.bert_width, 512))))\n",
    "        self.innerAttDoc = torch.nn.Parameter(torch.FloatTensor(np.random.uniform(0, 1, (self.bert_width*4, 512))))\n",
    "        self.out = torch.nn.Linear((self.bert_width*4),1)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, t, s, a):\n",
    "        text, pooled = self.bert(t,\n",
    "                        token_type_ids=s, \n",
    "                        attention_mask=a, \n",
    "                        output_all_encoded_layers=False)\n",
    "        \n",
    "        text = self.dropout(text)\n",
    "        cl_ = s == 0\n",
    "        ev_ = s == 1\n",
    "        claims = text * cl_.unsqueeze(-1).float()\n",
    "        claims = claims[:,:max_claim_length, :]\n",
    "        evidences = text * ev_.unsqueeze(-1).float()\n",
    "        evidences = evidences * a.unsqueeze(-1).float()\n",
    "        bdaf, ad2q, aq2d = biDAF(evidences, claims, self.wd)\n",
    "        _f = self.out(InnerAttention(bdaf, self.innerAttDoc))\n",
    "        return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFn = torch.nn.BCEWithLogitsLoss()\n",
    "def getLoss(pred, actual, lossFn, e_weight=0.6, ne_weight=0.4):\n",
    "    loss = lossFn(pred.squeeze(-1), actual)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FaVer().cuda()\n",
    "lr = 3e-5\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t, s, a, y = getBatch(5)\n",
    "    y_ = network.forward(t, s, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "epoch_vals = []\n",
    "epoch_accs = []\n",
    "epoch_evid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:50.337845Z",
     "start_time": "2019-04-17T08:53:50.328101Z"
    }
   },
   "outputs": [],
   "source": [
    "def _save(cause, network):\n",
    "    print(\"\\tSaving Model for Cause:\", cause)\n",
    "    torch.save(network.state_dict(), \"./FaVer_\" + cause + \"_save.h5\")\n",
    "    with open(\"./\" + cause + \"_training_cycle.json\", \"w\") as f:            \n",
    "        f.write(json.dumps(\n",
    "            {\n",
    "                \"training_losses\":epoch_losses,\n",
    "                \"validation_losses\":epoch_vals,\n",
    "                \"validation_accuracy\":epoch_accs,\n",
    "                \"evidence_accuracy\":epoch_evid        \n",
    "            }\n",
    "        ))\n",
    "        f.close()\n",
    "    \n",
    "def chooseModelSave(network):\n",
    "    save = False\n",
    "    if (np.min(epoch_vals) == epoch_vals[-1]):\n",
    "        cause = \"BestValidationLoss\"\n",
    "        _save(cause, network)\n",
    "    \n",
    "    if (np.max(epoch_accs) == epoch_accs[-1]):\n",
    "        cause = \"BestValidationOverallAccuracy\"\n",
    "        _save(cause, network)\n",
    "    \n",
    "    if (np.max(epoch_evid) == epoch_evid[-1]):\n",
    "        cause = \"BestValidationEvidentiaryAccuracy\"\n",
    "        _save(cause, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:51.160509Z",
     "start_time": "2019-04-17T08:53:51.147473Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(network, bs=100, num_batches=5):\n",
    "    \n",
    "    classes = torch.FloatTensor([]).cuda()\n",
    "    preds = torch.FloatTensor([]).cuda()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            et, es, ea, classes_ = getBatch(bs=bs, validation=True)\n",
    "            y_ = network.forward(et, es, ea)\n",
    "            classes = torch.cat([classes, classes_], dim=0)\n",
    "            preds = torch.cat([preds, y_], dim=0)\n",
    "        \n",
    "        evidences = classes >= 1\n",
    "        f_loss = getLoss(preds, classes, lossFn)\n",
    "        pred = torch.round(torch.sigmoid(preds)).squeeze(-1)\n",
    "        acc = torch.sum(pred == classes)\n",
    "        acc = acc.cpu().numpy()/(bs*num_batches)\n",
    "        positives = torch.sum(pred[evidences] == classes[evidences])\n",
    "        \n",
    "        return f_loss.data.item(), acc, positives.cpu().numpy()/torch.sum(evidences).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(network, bs = 5, epochs = 10, batches_per_epoch = 20):\n",
    "\n",
    "    val_min = 1000\n",
    "    for k in range(epochs):\n",
    "        losses = []\n",
    "        for i in range(batches_per_epoch):\n",
    "            t, s, a, y = getBatch(bs)\n",
    "            y_ = network.forward(t, s, a)\n",
    "            loss = getLoss(y_, y, lossFn)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.data.item())\n",
    "            print(\"epoch:\", k+1, \"batch:\", i+1, \"loss:\", np.round(np.mean(losses),5), end=\"\\r\")\n",
    "        epoch_losses.append(np.mean(losses))\n",
    "        val_loss, acc, evid_acc = validate(network, num_batches=10)\n",
    "        epoch_vals.append(val_loss)\n",
    "        epoch_accs.append(acc)\n",
    "        epoch_evid.append(evid_acc)\n",
    "        \n",
    "        print(\"\\n\\tValidation Loss:\", np.round(val_loss,5))\n",
    "        print(\"\\tOverall Validation Accuracy:\", np.round(acc,2), \"; and for evidence only:\", np.round(evid_acc,2))\n",
    "        \n",
    "        if (val_loss < val_min):\n",
    "            val_min = val_loss\n",
    "            \n",
    "        chooseModelSave(network)\n",
    "        \n",
    "        with open(\"./FaVer_training_cycle.json\", \"w\") as f:            \n",
    "            f.write(json.dumps(\n",
    "                {\n",
    "                    \"training_losses\":epoch_losses,\n",
    "                    \"validation_losses\":epoch_vals,\n",
    "                    \"validation_accuracy\":epoch_accs,\n",
    "                    \"evidence_accuracy\":epoch_evid        \n",
    "                }\n",
    "            ))\n",
    "            f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train(network, bs=16, batches_per_epoch = 1000, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
