{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline Generation - PyTorch Implementation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pwd\n",
    "!ls ./sumdata/train\n",
    "!gunzip ./sumdata/train/train.article.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*Not really using fastai for this particular notebook. Import to explore Fastai options to the same things.*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "from fastai.fastai.imports import *\n",
    "from fastai.fastai.torch_imports import *\n",
    "from fastai.fastai.core import *\n",
    "from fastai.fastai.model import fit\n",
    "from fastai.fastai.dataset import *\n",
    "from fastai.fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pprint\n",
    "from random import randint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "config = []\n",
    "with open(\"./gigaword_attn_config.json\") as f:\n",
    "    config = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples (x,y): 3803957 3803957\n",
      "Validation Samples (x,y): 189651 189651\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "val_inputs = []\n",
    "val_outputs = []\n",
    "counter = 0\n",
    "_dict = {}\n",
    "\n",
    "path = config[\"data_path\"]\n",
    "input_path = config[\"training\"][\"inputs\"]\n",
    "output_path = config[\"training\"][\"outputs\"]\n",
    "\n",
    "validation_input_path = config[\"validation\"][\"inputs\"]\n",
    "validation_output_path = config[\"validation\"][\"outputs\"]\n",
    "\n",
    "with open(path + input_path, \"r\") as f:\n",
    "    inputs = f.readlines()\n",
    "with open(path + output_path, \"r\") as f:\n",
    "    outputs = f.readlines()\n",
    "with open(path + validation_input_path, \"r\") as f:\n",
    "    val_inputs = f.readlines()\n",
    "with open(path + validation_output_path, \"r\") as f:\n",
    "    val_outputs = f.readlines()\n",
    "        \n",
    "print(\"Training Samples (x,y):\",len(inputs), len(outputs))\n",
    "print(\"Validation Samples (x,y):\", len(val_inputs), len(val_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Rebuild Dictionary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "def buildDictWordTokens(all_sources):\n",
    "    _dict = {}\n",
    "    for k in range(len(all_sources)):\n",
    "        counter = 0\n",
    "        for line in all_sources[k]:\n",
    "            counter += 1\n",
    "            if (counter %1000 == 0):\n",
    "                _str = str(k) + \"\\t\" + str(counter) + \"/\" + str(len(all_sources[k])) + \"\\r\"\n",
    "                sys.stdout.write(_str)\n",
    "                sys.stdout.flush()\n",
    "            for word in line.split():\n",
    "                if (word not in _dict):\n",
    "                    _dict[word] = 0\n",
    "                _dict[word] += 1\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "_dict = buildDictWordTokens([inputs, outputs])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "pickle.dump(_dict, open(config[\"dictionary_path\"], \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ...or load from disk instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "_dict = pickle.load(open(config[\"dictionary_path\"], \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_dict = sorted([(word, _dict[word]) for word in _dict], key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "set config[\"vocab_size\"]:-1 to use all words in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "add_tokens = config[\"dictionary_tokens\"]\n",
    "for i in range(len(add_tokens)):\n",
    "    _dict.insert(i, (add_tokens[i], 1))\n",
    "f_dict = {}\n",
    "for i in range(len(_dict)):\n",
    "    f_dict[_dict[i][0]] = i \n",
    "r_dict = [_dict[i][0] for i in range(len(_dict))]\n",
    "vocab_size = len(r_dict) if config[\"vocab_size\"] == -1 else config[\"vocab_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaffolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_S_', '_E_', '_P_', '_OOV_', 'the', 'to', ',', '.', 'in', 'of', 'a', 'on', 'and', \"'s\", 'for', 'said', 'with', 'at', 'that', '##', 'as', '<unk>', 'from', 'an', 'by', 'new', 'has', 'his', 'after', 'tuesday', 'wednesday', 'thursday', 'is', 'its']\n"
     ]
    }
   ],
   "source": [
    "print([r_dict[w] for w in range(34)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_token = config[\"oov_token\"]\n",
    "padding_token = config[\"padding_token\"]\n",
    "\n",
    "def generateBatch(x_source, y_source, input_ts=30, output_ts = 10, bs=64):\n",
    "    dont_copy_index = input_ts-1\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "    u_ = []\n",
    "    u_ind = []\n",
    "    while len(x_) < bs:\n",
    "        _u = []\n",
    "        _u_ind = []\n",
    "        l = randint(0, len(x_source)-1)\n",
    "        x = [f_dict[t] for t in x_source[l].split()]\n",
    "        y = [f_dict[t] for t in y_source[l].split()]\n",
    "        x = [t if t < vocab_size else f_dict[oov_token] for t in x]\n",
    "        y = [t if t < vocab_size else f_dict[oov_token] for t in y] \n",
    "        x = x[:input_ts]\n",
    "        while (len(x) < input_ts):\n",
    "            x.insert(0,f_dict[padding_token])\n",
    "        for i in range(len(y)):\n",
    "            word = y[i]   \n",
    "            if (word in x and word != oov_token):\n",
    "                _u.append(1)\n",
    "                _u_ind.append(x.index(word))\n",
    "                #y[i] = vocab_size + x.index(word)\n",
    "            else:\n",
    "                _u.append(0)\n",
    "                _u_ind.append(dont_copy_index)\n",
    "                \n",
    "        while (len(y) < output_ts):\n",
    "            y.append(f_dict[padding_token])\n",
    "            _u.append(0)\n",
    "            _u_ind.append(dont_copy_index)\n",
    "            \n",
    "        _u_ind = _u_ind[:output_ts]\n",
    "        _u = _u[:output_ts]\n",
    "        y = y[:output_ts]\n",
    "\n",
    "        \n",
    "        x_.append(x)\n",
    "        y_.append(y)\n",
    "        u_ind.append(_u_ind)\n",
    "        u_.append(_u)\n",
    "\n",
    "    return np.array(x_), np.array(y_), np.array(u_), np.array(u_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y,u,ind = generateBatch(val_inputs, val_outputs, input_ts=20, output_ts=10)\n",
    "x.shape, y.shape, u.shape, ind.shape\n",
    "np.max(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function **flattens along the batch and timesteps dimensions** and computes a Loss for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfn_multi(outputs, acts, criterion, input_ts = 30):\n",
    "    acts = acts.transpose(0,1).contiguous().view(-1)\n",
    "    outputs = outputs.view(-1, vocab_size)\n",
    "    return criterion(outputs, acts.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, lossfn, criterion, num_batches = 10, bs = 128, output_ts=10):\n",
    "    t_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        x, y, u, u_indices = generateBatch(val_inputs, val_outputs, output_ts = output_ts)\n",
    "        y = torch.LongTensor(y).cuda()\n",
    "        h = m.reinitInputHiddenState(1)\n",
    "        w,h,u = model(torch.from_numpy(x).cuda(), h, output_ts)\n",
    "        l = lossfn(w,y,criterion)\n",
    "        t_loss += l.item()\n",
    "    return t_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBatch(x,y,u,u_indices, \n",
    "               model, \n",
    "               optimizer, \n",
    "               criterion, \n",
    "               bs, \n",
    "               use_tf = False, \n",
    "               output_ts=10):\n",
    "    loss = 0\n",
    "\n",
    "    #print(y) \n",
    "    y = torch.LongTensor(y).cuda()\n",
    "    u = torch.LongTensor(u).cuda()\n",
    "    #u_app = torch.zeros_like(u).unsqueeze(0).cuda().permute(1,2,0)\n",
    "    \n",
    "    u_indices = torch.LongTensor(u_indices).cuda()\n",
    "    h = m.reinitInputHiddenState(bs)\n",
    "    w,h,u_pred = model(torch.from_numpy(x).cuda(), h, output_ts, y_acts=y, use_tf=True)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    \"\"\"\n",
    "    l1 = lossfn_u(u, u_pred)\n",
    "    loss += l1\n",
    "    l1.backward(retain_graph=True)\n",
    "    #optimizer.step()\n",
    "    \"\"\"\n",
    "    #optimizer.zero_grad()\n",
    "    l2 = lossfn_uind(u_indices, h, criterion)\n",
    "    loss += l2\n",
    "    l2.backward(retain_graph=True)\n",
    "    #optimizer.step()\n",
    "    \n",
    "    #optimizer.zero_grad()\n",
    "    l = lossfn_multi(w,y,criterion)\n",
    "    loss += l\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), l.item(), 0, l2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, generate_words = 10, print_attn=False):\n",
    "    x, y, u, u_ind = generateBatch(val_inputs, val_outputs, output_ts =generate_words)\n",
    "    h = m.reinitInputHiddenState(1)\n",
    "    outputs, atts, uts = model(torch.from_numpy(x).cuda(), h, generate_words, print_attn=print_attn)\n",
    "    outputs = outputs.exp()\n",
    "    words = torch.max(outputs, -1)[1].view(-1,outputs.size()[1]).permute(1,0)\n",
    "    samples = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_words = \" \".join([r_dict[word.item()] for word in x[i]])\n",
    "        y_act_words = \" \".join([r_dict[word.item()] for word in y[i]])\n",
    "        y_words = \" \".join([r_dict[word] for word in words[i]])\n",
    "        \"\"\"\n",
    "        if (print_attn):\n",
    "            print(\"ATTNS:\")\n",
    "            print(atts[i])\n",
    "        \"\"\"\n",
    "        f_ = {\n",
    "            \"text\": {\n",
    "                \"source\":x_words, \n",
    "                \"actual\":y_act_words, \n",
    "                \"predicted\":y_words\n",
    "            },\n",
    "            \"attention\":F.softmax(atts[i], dim=-1).cpu().detach().numpy().tolist()\n",
    "        }\n",
    "        samples.append(f_)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "network_losses = []\n",
    "ut_losses = []\n",
    "att_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=1, \n",
    "          batches=128, \n",
    "          optim=None, \n",
    "          criterion = None, \n",
    "          bs = 64, \n",
    "          output_ts=20, \n",
    "          use_tf=False, \n",
    "          lr=1e-3, \n",
    "          num_valid_batches=10):\n",
    "    \n",
    "    if optim == None:\n",
    "        optim = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    if criterion == None:\n",
    "        criterion = nn.NLLLoss()\n",
    "    for e in range(epochs):\n",
    "        rolling_loss = 0\n",
    "        rolling_time = 0\n",
    "        rolling_network = 0\n",
    "        rolling_ut = 0\n",
    "        rolling_att = 0\n",
    "        print(\"\\n\")\n",
    "        for b in range(batches):\n",
    "            b_start = time.time()\n",
    "            loss, network_loss, ut_loss, att_loss = trainBatch(*generateBatch(inputs, outputs, output_ts = output_ts, bs=bs), \n",
    "                              model, \n",
    "                              optim, \n",
    "                              criterion, \n",
    "                              bs, \n",
    "                              output_ts=output_ts,\n",
    "                              use_tf = use_tf)\n",
    "            rolling_loss += loss\n",
    "            rolling_network += network_loss\n",
    "            rolling_ut += ut_loss\n",
    "            rolling_att += att_loss\n",
    "            b_stop = time.time()\n",
    "            rolling_time += b_stop-b_start\n",
    "            avg_time = rolling_time/(b+1)\n",
    "            eta = (batches-b)*avg_time\n",
    "            _str = \"e\" + str(e+1) + \", batch: \" + \\\n",
    "                    str(b+1) + \"\\tloss:\" + \\\n",
    "                    \"{:10.3f}\".format(rolling_loss/(b+1)) + \\\n",
    "                    \" (\" + \\\n",
    "                    \"{:10.3f}\".format(rolling_network/(b+1)) + \\\n",
    "                    \",\" + \\\n",
    "                    \"{:10.3f}\".format(rolling_ut/(b+1)) + \\\n",
    "                    \",\" + \\\n",
    "                    \"{:10.3f}\".format(rolling_att/(b+1)) + \\\n",
    "                    \") \" + \\\n",
    "                    \" \\t\\teta: \" +  \\\n",
    "                    \"{:5.1f}\".format(eta) + \"s\\t\" + \\\n",
    "                    \"{:1.2f}\".format(avg_time) + \"s/batch\\r\"\n",
    "            sys.stdout.write(_str)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        losses.append(rolling_loss/batches)\n",
    "        #validate\n",
    "        valid_loss = validate(model, lossfn_multi, criterion, bs=bs, num_batches=num_valid_batches, output_ts=output_ts)        \n",
    "        print(\"\\n\")\n",
    "        print(\"validation loss:\", \"{:3.2f}\".format(valid_loss))\n",
    "\n",
    "        doSave = False        \n",
    "        if (len(val_losses) == 0):\n",
    "            doSave = True\n",
    "        elif (np.min(val_losses) > valid_loss):\n",
    "            doSave = True            \n",
    "        if (doSave):\n",
    "            print(\"Saving Model:\", config[\"save_model_path\"])\n",
    "            torch.save(model, config[\"save_model_path\"])    \n",
    "        val_losses.append(valid_loss)\n",
    "        network_losses.append(network_loss)\n",
    "        ut_losses.append(ut_loss)\n",
    "        att_losses.append(att_loss)\n",
    "        \n",
    "        with open(config[\"save_training_cycle_path\"], \"w\") as f:\n",
    "            f.write(json.dumps( {                \n",
    "                \"training_loss\":losses,\n",
    "                \"validation_loss\":val_losses,\n",
    "                \"network_loss\":network_losses,\n",
    "                \"ut_loss\":ut_losses,\n",
    "                \"att_loss\":att_losses\n",
    "            }))\n",
    "            f.close()\n",
    "        \n",
    "        #sample\n",
    "        samples = sample(m, generate_words=output_ts)  \n",
    "        _l = rolling_loss/batches\n",
    "        _samples = {\n",
    "            \"epochs\":len(losses),\n",
    "            \"used_tf\":use_tf,\n",
    "            \"loss\":_l,\n",
    "            \"val_loss\":valid_loss,\n",
    "            \"samples\":samples\n",
    "        }\n",
    "        with open(config[\"save_samples_path\"] + str(time.time()) + \"_.json\", \"w\") as f:\n",
    "            f.write(json.dumps(_samples, indent=4))\n",
    "            f.close()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_token = f_dict[config[\"start_token\"]]\n",
    "class customGRU(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size=128, \n",
    "                 embed_dim = 100, \n",
    "                 lstm_dim= 90, \n",
    "                 hidden_dim=64, \n",
    "                 bidirec=False, \n",
    "                 lstm_layers = 3,\n",
    "                 start_token = start_token):\n",
    "        super(customGRU, self).__init__()\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx = f_dict[padding_token])\n",
    "        self.start_token = start_token\n",
    "        self.input_lstm = nn.GRU(embed_dim, lstm_dim, num_layers=lstm_layers, dropout=0.1, bidirectional=True)\n",
    "        self.attn_W = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.w2i = nn.Parameter(torch.randn(hidden_dim + lstm_dim*2, hidden_dim))\n",
    "        self.dec_lstm = nn.GRU(embed_dim, hidden_dim)\n",
    "        self.decoder = nn.Parameter(torch.randn(self.hidden_dim + self.lstm_dim*2, embed_dim))\n",
    "        self.ut = nn.Parameter(torch.randn(self.hidden_dim + self.lstm_dim*2, 1))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.norm_softmax = nn.Softmax(dim=-1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, hidden, output_ts, use_tf=False, y_acts=None, train=True, print_attn=False):\n",
    "        bs, ts = x.size()\n",
    "        x_ = x.permute(1,0)\n",
    "        o = self.embed(x_) #b,ts,embed\n",
    "        y_ = None\n",
    "        if (y_acts is not None):\n",
    "            y_ = y_acts.permute(1,0)\n",
    "        i_lh, i_h = self.input_lstm(o)\n",
    "        h = i_lh[-1, :, :].unsqueeze(0)\n",
    "        attn_ = torch.zeros(1, 1, self.lstm_dim*2)\n",
    "        pw = np.zeros((bs,1))\n",
    "        pw[:,0] = start_token\n",
    "        pw = torch.LongTensor(pw).cuda().permute(1,0)\n",
    "        o_wh = []\n",
    "        atts_ = []\n",
    "        uts_ = []\n",
    "        for i in range(output_ts):\n",
    "            pw = self.embed(pw)\n",
    "            h,h_ = self.dec_lstm(pw, h)    \n",
    "            #Attention Calculations\n",
    "            a = torch.matmul(h, self.attn_W) #1, b, hidden_dim\n",
    "            a = a.permute(1,0,2) #b,n,m <- b, 1, hidden_dim\n",
    "            b = i_lh.permute(1,2,0) #b,m,p <- b, hidden_dim, ts\n",
    "            e = torch.bmm(a,b) #b,n,p <= b, 1, ts\n",
    "            alpha = F.softmax(e, dim=-1)\n",
    "            \n",
    "            if (print_attn):\n",
    "                print(\"ALPHA:\", alpha.size())\n",
    "                print(\"ALPHA[0] SUM:\",torch.sum(alpha,-1)[0])\n",
    "                print(\"ALPHA[0]:\",alpha[0])\n",
    "                print(\"E:\", e.size())\n",
    "                print(\"E[0]:\",e[0])\n",
    "                print(\"I_LH_PERM:\", b.size())\n",
    "                print(\"A_PERM:\", a.size())\n",
    "                print(\"a_matmul\", a.size())\n",
    "                print(\"w_attn:\", self.attn_W)\n",
    "                print(\"h[0]\", h[0])\n",
    "                print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "                #print(alpha)\n",
    "                \n",
    "            #atts_.append(e.view(-1, ts+1)) \n",
    "            atts_.append(e.view(-1,ts))\n",
    "            #alpha = alpha #ts, b\n",
    "            \n",
    "            #output creation\n",
    "            #alpha_mult = alpha[:,:,:ts]\n",
    "            att_out = torch.sum(i_lh*alpha.view(-1, x.size()[0], 1),0) #b, hidden_dim\n",
    "            att_out = att_out.unsqueeze(0) #1, b, hidden_dim\n",
    "            h_att = torch.cat([h, att_out], -1) \n",
    "            \n",
    "            \n",
    "            ut = torch.matmul(h_att, self.ut)\n",
    "            uts_.append(self.sigmoid(ut))\n",
    "            \n",
    "            \n",
    "            w_proj = torch.matmul(h_att, self.decoder)\n",
    "            w_ = self.log_softmax(torch.matmul(w_proj, torch.transpose(self.embed.weight, 0, 1)))\n",
    "            #f_out = torch.cat([w_, alpha.permute(1,0,2)], -1)\n",
    "\n",
    "            o_wh.append(w_)\n",
    "            \n",
    "            #GRU State Management\n",
    "            pw = torch.max(w_,-1)[1]\n",
    "            if ((use_tf and torch.randint(11, (1,))[0] > 3)):\n",
    "                pw = y_[i,:].unsqueeze(0)\n",
    "            \n",
    "        o_wh = torch.stack(o_wh, 0).squeeze(1)\n",
    "        atts_ = torch.stack(atts_, 0)\n",
    "        atts_ = atts_.permute(1, 0, 2)\n",
    "        uts_ = torch.stack(uts_, 0).squeeze(1).permute(1,0,2)\n",
    "        return o_wh, atts_, uts_\n",
    "    \n",
    "    def reinitInputHiddenState(self,bs):\n",
    "        return torch.zeros((self.lstm_layers, bs, self.lstm_dim)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m = customGRU(vocab_size = vocab_size, \n",
    "              hidden_dim = 256, \n",
    "              embed_dim=300, \n",
    "              lstm_dim = 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples = sample(m, print_attn=False)    # test that sampling works without errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfn_u(outputs_u, acts):\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "    acts = acts.contiguous().view(-1, acts.size(-1))\n",
    "    outputs = outputs_u.view(-1,1)\n",
    "    l = loss(outputs.float(), acts)\n",
    "    return l\n",
    "    \n",
    "def lossfn_uind(acts_ind, outputs_ind, criterion):\n",
    "    loss2 = nn.CrossEntropyLoss()\n",
    "    acts_ind = acts_ind.transpose(0,1).contiguous().view(-1)\n",
    "    outputs_ind = outputs_ind.permute(1,0,2)\n",
    "    outputs_ind = outputs_ind.contiguous().view(-1, outputs_ind.size()[-1])\n",
    "    l2 = loss2(outputs_ind, acts_ind)\n",
    "    return l2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "10 epoch(s) (2500 batches of 128 samples each.) Teacher Forcing: True\n",
      "\n",
      "\n",
      "e1, batch: 2500\tloss:    20.150 (    17.744,     0.000,     2.406)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 6.86\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aman/miniconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type customGRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e2, batch: 2500\tloss:     9.192 (     7.221,     0.000,     1.971)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 6.60\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e3, batch: 2500\tloss:     7.664 (     5.877,     0.000,     1.787)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 6.05\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e4, batch: 2500\tloss:     7.186 (     5.452,     0.000,     1.734)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.60\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e5, batch: 2500\tloss:     6.873 (     5.168,     0.000,     1.705)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.49\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e6, batch: 2500\tloss:     6.634 (     4.959,     0.000,     1.674)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.24\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e7, batch: 2500\tloss:     6.430 (     4.793,     0.000,     1.637)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.35\n",
      "\n",
      "\n",
      "e8, batch: 2500\tloss:     6.286 (     4.673,     0.000,     1.613)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.13\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e9, batch: 2500\tloss:     6.176 (     4.585,     0.000,     1.591)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.20\n",
      "\n",
      "\n",
      "e10, batch: 2500\tloss:     6.068 (     4.508,     0.000,     1.560)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.14\n",
      "\n",
      "\n",
      "10 epoch(s) (2500 batches of 128 samples each.) Teacher Forcing: True\n",
      "\n",
      "\n",
      "e1, batch: 2500\tloss:     5.978 (     4.437,     0.000,     1.541)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.02\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e2, batch: 2500\tloss:     5.903 (     4.378,     0.000,     1.525)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.11\n",
      "\n",
      "\n",
      "e3, batch: 2500\tloss:     5.838 (     4.328,     0.000,     1.510)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.12\n",
      "\n",
      "\n",
      "e4, batch: 2500\tloss:     5.775 (     4.277,     0.000,     1.498)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.10\n",
      "\n",
      "\n",
      "e5, batch: 2500\tloss:     5.724 (     4.240,     0.000,     1.484)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.00\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e6, batch: 2500\tloss:     5.673 (     4.203,     0.000,     1.471)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.75\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e7, batch: 2500\tloss:     5.640 (     4.177,     0.000,     1.463)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.95\n",
      "\n",
      "\n",
      "e8, batch: 2500\tloss:     5.597 (     4.143,     0.000,     1.453)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.01\n",
      "\n",
      "\n",
      "e9, batch: 2500\tloss:     5.555 (     4.113,     0.000,     1.442)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.85\n",
      "\n",
      "\n",
      "e10, batch: 2500\tloss:     5.520 (     4.084,     0.000,     1.436)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.99\n",
      "\n",
      "\n",
      "10 epoch(s) (2500 batches of 128 samples each.) Teacher Forcing: True\n",
      "\n",
      "\n",
      "e1, batch: 2500\tloss:     5.493 (     4.062,     0.000,     1.430)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.85\n",
      "\n",
      "\n",
      "e2, batch: 2500\tloss:     5.455 (     4.032,     0.000,     1.423)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.88\n",
      "\n",
      "\n",
      "e3, batch: 2500\tloss:     5.433 (     4.015,     0.000,     1.418)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 5.05\n",
      "\n",
      "\n",
      "e4, batch: 2500\tloss:     5.419 (     4.002,     0.000,     1.417)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.80\n",
      "\n",
      "\n",
      "e5, batch: 2500\tloss:     5.379 (     3.970,     0.000,     1.409)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.94\n",
      "\n",
      "\n",
      "e6, batch: 2500\tloss:     5.371 (     3.963,     0.000,     1.407)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.73\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e7, batch: 2500\tloss:     5.342 (     3.940,     0.000,     1.402)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.97\n",
      "\n",
      "\n",
      "e8, batch: 2500\tloss:     5.312 (     3.916,     0.000,     1.397)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.84\n",
      "\n",
      "\n",
      "e9, batch: 2500\tloss:     5.306 (     3.911,     0.000,     1.395)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.65\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e10, batch: 2500\tloss:     5.287 (     3.896,     0.000,     1.391)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.78\n",
      "\n",
      "\n",
      "10 epoch(s) (2500 batches of 128 samples each.) Teacher Forcing: True\n",
      "\n",
      "\n",
      "e1, batch: 2500\tloss:     5.257 (     3.875,     0.000,     1.382)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.83\n",
      "\n",
      "\n",
      "e2, batch: 2500\tloss:     5.244 (     3.861,     0.000,     1.383)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.77\n",
      "\n",
      "\n",
      "e3, batch: 2500\tloss:     5.224 (     3.849,     0.000,     1.376)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.96\n",
      "\n",
      "\n",
      "e4, batch: 2500\tloss:     5.217 (     3.841,     0.000,     1.376)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.96\n",
      "\n",
      "\n",
      "e5, batch: 2500\tloss:     5.211 (     3.836,     0.000,     1.375)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.63\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e6, batch: 2500\tloss:     5.186 (     3.817,     0.000,     1.369)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.75\n",
      "\n",
      "\n",
      "e7, batch: 2500\tloss:     5.166 (     3.802,     0.000,     1.364)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.73\n",
      "\n",
      "\n",
      "e8, batch: 2500\tloss:     5.156 (     3.794,     0.000,     1.362)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.65\n",
      "\n",
      "\n",
      "e9, batch: 2500\tloss:     5.150 (     3.787,     0.000,     1.364)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.61\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e10, batch: 2500\tloss:     5.122 (     3.768,     0.000,     1.355)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.74\n",
      "\n",
      "\n",
      "10 epoch(s) (2500 batches of 128 samples each.) Teacher Forcing: True\n",
      "\n",
      "\n",
      "e1, batch: 2500\tloss:     5.123 (     3.767,     0.000,     1.356)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.78\n",
      "\n",
      "\n",
      "e2, batch: 2500\tloss:     5.107 (     3.756,     0.000,     1.351)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.75\n",
      "\n",
      "\n",
      "e3, batch: 2500\tloss:     5.100 (     3.748,     0.000,     1.351)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.66\n",
      "\n",
      "\n",
      "e4, batch: 2500\tloss:     5.078 (     3.732,     0.000,     1.346)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.93\n",
      "\n",
      "\n",
      "e5, batch: 2500\tloss:     5.063 (     3.721,     0.000,     1.342)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.68\n",
      "\n",
      "\n",
      "e6, batch: 2500\tloss:     5.067 (     3.724,     0.000,     1.343)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.79\n",
      "\n",
      "\n",
      "e7, batch: 2500\tloss:     5.056 (     3.715,     0.000,     1.341)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.78\n",
      "\n",
      "\n",
      "e8, batch: 2500\tloss:     5.038 (     3.702,     0.000,     1.336)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.71\n",
      "\n",
      "\n",
      "e9, batch: 2500\tloss:     5.038 (     3.703,     0.000,     1.335)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.50\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e10, batch: 2500\tloss:     5.023 (     3.691,     0.000,     1.331)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.79\n",
      "\n",
      "\n",
      "10 epoch(s) (2500 batches of 128 samples each.) Teacher Forcing: True\n",
      "\n",
      "\n",
      "e1, batch: 2500\tloss:     5.013 (     3.684,     0.000,     1.330)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.55\n",
      "\n",
      "\n",
      "e2, batch: 2500\tloss:     5.003 (     3.677,     0.000,     1.326)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.54\n",
      "\n",
      "\n",
      "e3, batch: 2500\tloss:     5.007 (     3.679,     0.000,     1.328)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.81\n",
      "\n",
      "\n",
      "e4, batch: 2500\tloss:     4.993 (     3.668,     0.000,     1.325)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.78\n",
      "\n",
      "\n",
      "e5, batch: 2500\tloss:     4.974 (     3.655,     0.000,     1.319)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.67\n",
      "\n",
      "\n",
      "e6, batch: 2500\tloss:     4.971 (     3.652,     0.000,     1.318)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.88\n",
      "\n",
      "\n",
      "e7, batch: 2500\tloss:     4.961 (     3.647,     0.000,     1.314)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.61\n",
      "\n",
      "\n",
      "e8, batch: 2500\tloss:     4.958 (     3.641,     0.000,     1.316)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.58\n",
      "\n",
      "\n",
      "e9, batch: 2500\tloss:     4.946 (     3.635,     0.000,     1.311)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.76\n",
      "\n",
      "\n",
      "e10, batch: 2500\tloss:     4.935 (     3.626,     0.000,     1.309)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.82\n",
      "\n",
      "\n",
      "10 epoch(s) (2500 batches of 128 samples each.) Teacher Forcing: True\n",
      "\n",
      "\n",
      "e1, batch: 2500\tloss:     4.940 (     3.628,     0.000,     1.312)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.80\n",
      "\n",
      "\n",
      "e2, batch: 2500\tloss:     4.927 (     3.622,     0.000,     1.305)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.55\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e3, batch: 2500\tloss:     4.910 (     3.608,     0.000,     1.302)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.72\n",
      "\n",
      "\n",
      "e4, batch: 2500\tloss:     4.904 (     3.604,     0.000,     1.300)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.96\n",
      "\n",
      "\n",
      "e5, batch: 2500\tloss:     4.908 (     3.605,     0.000,     1.303)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.79\n",
      "\n",
      "\n",
      "e6, batch: 2500\tloss:     4.879 (     3.586,     0.000,     1.293)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.81\n",
      "\n",
      "\n",
      "e7, batch: 2500\tloss:     4.896 (     3.598,     0.000,     1.299)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.63\n",
      "\n",
      "\n",
      "e8, batch: 2500\tloss:     4.871 (     3.578,     0.000,     1.292)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.66\n",
      "\n",
      "\n",
      "e9, batch: 2500\tloss:     4.878 (     3.584,     0.000,     1.294)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.61\n",
      "\n",
      "\n",
      "e10, batch: 2500\tloss:     4.876 (     3.582,     0.000,     1.294)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.66\n",
      "\n",
      "\n",
      "10 epoch(s) (2500 batches of 128 samples each.) Teacher Forcing: True\n",
      "\n",
      "\n",
      "e1, batch: 2500\tloss:     4.865 (     3.575,     0.000,     1.290)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.61\n",
      "\n",
      "\n",
      "e2, batch: 2500\tloss:     4.867 (     3.577,     0.000,     1.290)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.63\n",
      "\n",
      "\n",
      "e3, batch: 2500\tloss:     4.849 (     3.565,     0.000,     1.284)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.82\n",
      "\n",
      "\n",
      "e4, batch: 2500\tloss:     4.842 (     3.559,     0.000,     1.284)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.64\n",
      "\n",
      "\n",
      "e5, batch: 2500\tloss:     4.849 (     3.562,     0.000,     1.286)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.75\n",
      "\n",
      "\n",
      "e6, batch: 2500\tloss:     4.840 (     3.555,     0.000,     1.284)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.64\n",
      "\n",
      "\n",
      "e7, batch: 2500\tloss:     4.820 (     3.540,     0.000,     1.279)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.46\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e8, batch: 2500\tloss:     4.823 (     3.545,     0.000,     1.278)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.73\n",
      "\n",
      "\n",
      "e9, batch: 2500\tloss:     4.817 (     3.540,     0.000,     1.277)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.55\n",
      "\n",
      "\n",
      "e10, batch: 2500\tloss:     4.791 (     3.522,     0.000,     1.268)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.50\n",
      "\n",
      "\n",
      "10 epoch(s) (2500 batches of 128 samples each.) Teacher Forcing: True\n",
      "\n",
      "\n",
      "e1, batch: 2500\tloss:     4.812 (     3.537,     0.000,     1.275)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.70\n",
      "\n",
      "\n",
      "e2, batch: 2500\tloss:     4.797 (     3.526,     0.000,     1.271)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.50\n",
      "\n",
      "\n",
      "e3, batch: 2500\tloss:     4.788 (     3.519,     0.000,     1.269)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.54\n",
      "\n",
      "\n",
      "e4, batch: 2500\tloss:     4.785 (     3.516,     0.000,     1.268)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.59\n",
      "\n",
      "\n",
      "e5, batch: 2500\tloss:     4.792 (     3.523,     0.000,     1.269)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.48\n",
      "\n",
      "\n",
      "e6, batch: 2500\tloss:     4.783 (     3.514,     0.000,     1.269)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.67\n",
      "\n",
      "\n",
      "e7, batch: 2500\tloss:     4.782 (     3.516,     0.000,     1.266)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.87\n",
      "\n",
      "\n",
      "e8, batch: 2500\tloss:     4.780 (     3.513,     0.000,     1.267)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.69\n",
      "\n",
      "\n",
      "e9, batch: 2500\tloss:     4.764 (     3.499,     0.000,     1.265)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.55\n",
      "\n",
      "\n",
      "e10, batch: 2500\tloss:     4.761 (     3.498,     0.000,     1.263)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.51\n",
      "\n",
      "\n",
      "10 epoch(s) (2500 batches of 128 samples each.) Teacher Forcing: True\n",
      "\n",
      "\n",
      "e1, batch: 2500\tloss:     4.763 (     3.499,     0.000,     1.264)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.53\n",
      "\n",
      "\n",
      "e2, batch: 2500\tloss:     4.749 (     3.489,     0.000,     1.260)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.68\n",
      "\n",
      "\n",
      "e3, batch: 2500\tloss:     4.751 (     3.493,     0.000,     1.259)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.35\n",
      "Saving Model: ./models/pyt_attn.h5\n",
      "\n",
      "\n",
      "e4, batch: 2500\tloss:     4.759 (     3.497,     0.000,     1.262)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.57\n",
      "\n",
      "\n",
      "e5, batch: 2500\tloss:     4.758 (     3.496,     0.000,     1.262)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.55\n",
      "\n",
      "\n",
      "e6, batch: 2500\tloss:     4.738 (     3.482,     0.000,     1.256)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.63\n",
      "\n",
      "\n",
      "e7, batch: 2500\tloss:     4.736 (     3.480,     0.000,     1.256)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.50\n",
      "\n",
      "\n",
      "e8, batch: 2500\tloss:     4.725 (     3.473,     0.000,     1.252)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.76\n",
      "\n",
      "\n",
      "e9, batch: 2500\tloss:     4.719 (     3.468,     0.000,     1.251)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.56\n",
      "\n",
      "\n",
      "e10, batch: 2500\tloss:     4.732 (     3.477,     0.000,     1.255)  \t\teta:   0.1s\t0.12s/batch\n",
      "\n",
      "validation loss: 4.69\n",
      "\n",
      "\n",
      "10 epoch(s) (2500 batches of 128 samples each.) Teacher Forcing: True\n",
      "\n",
      "\n",
      "e1, batch: 627\tloss:     4.720 (     3.473,     0.000,     1.247)  \t\teta: 234.7s\t0.13s/batch\r"
     ]
    }
   ],
   "source": [
    "epochs = [10] \n",
    "batches = [2500, 1000]\n",
    "tf = [True, True]\n",
    "lrs = [1e-3, 1e-4]\n",
    "\n",
    "_output_ts = 10\n",
    "_bs = 128\n",
    "optim = torch.optim.Adam(m.parameters(), lr=1e-3) \n",
    "\n",
    "\n",
    "while True:\n",
    "    for i in range(len(epochs)):\n",
    "        print(\"\\n\")\n",
    "        print(str(epochs[i]) + \" epoch(s) (\" + str(batches[i]) + \" batches of \" + str(_bs) + \" samples each.) Teacher Forcing:\", tf[i])\n",
    "        e = epochs[i]\n",
    "        b = batches[i]\n",
    "        _losses = train(m, \n",
    "                        epochs=e, \n",
    "                        batches=b, \n",
    "                        optim = optim, \n",
    "                        output_ts=_output_ts, \n",
    "                        use_tf=tf[i], \n",
    "                        bs=_bs)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
