{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:51:55.764208Z",
     "start_time": "2019-04-17T08:51:45.065341Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "processed_data = pickle.load(open(\"../../../fever_processed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep for Evidence Accumulation\n",
    "\n",
    "* The significance of a given statement as evidence to a claim/question is modelled as a classification problem\n",
    "* A class is awarded to potential evidence based on the following:\n",
    "    - Class 0, if the evidence tokens do not contribute to answering the question\n",
    "    - Class 1, if the evidence tokens partially or fully answer the question\n",
    "* The evidence and the claim are put through the same bert instance, but are put through separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:51:55.768240Z",
     "start_time": "2019-04-17T08:51:55.765930Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len_claims = 30\n",
    "max_len_evid = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:24.363410Z",
     "start_time": "2019-04-17T08:51:55.770123Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108771 / 1087718771108771108771 108771108771108771108771108771108771 108771108771 108771108771 108771108771108771108771108771 108771108771108771108771108771108771108771/ 108771108771108771108771108771108771108771/ 108771108771108771108771108771108771108771 108771108771108771108771108771 108771108771108771108771108771108771108771108771108771108771108771108771108771108771 108771 108771108771108771 108771108771108771108771108771108771/ 108771108771108771108771108771108771108771 108771108771108771108771 108771108771108771108771108771108771108771108771 108771108771108771 108771108771108771 108771108771108771108771108771108771108771108771108771108771108771108771108771108771108771108771108771108771 108771108771108771 108771108771108771 108771108771108771108771108771108771108771108771108771108771108771/ 108771108771108771108771108771108771108771108771108771 108771108771108771108771108771108771 108771108771 108771108771108771108771108771108771108771108771108771108771108771\r"
     ]
    }
   ],
   "source": [
    "lines = []\n",
    "classes = []\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import json\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def make_data(claim, evidence):\n",
    "    _fctokens = [\"[CLS]\"]\n",
    "    _fctokens.extend(claim)\n",
    "    _fetokens = [\"[CLS]\"]\n",
    "    _fetokens.extend(evidence) \n",
    "    while len(_fctokens) < max_len_claims:\n",
    "        _fctokens.append(\"[PAD]\")\n",
    "    while len(_fetokens) < max_len_evid:\n",
    "        _fetokens.append(\"[PAD]\")\n",
    "    _fctokens = _fctokens[:max_len_claims]\n",
    "    _fetokens = _fetokens[:max_len_evid]\n",
    "    _csegments = np.zeros((max_len_claims,))\n",
    "    _esegments = np.zeros((max_len_evid,))\n",
    "    ctokens = tokenizer.convert_tokens_to_ids(_fctokens)\n",
    "    etokens = tokenizer.convert_tokens_to_ids(_fetokens)\n",
    "    return (ctokens, etokens, _esegments)\n",
    "\n",
    "counter = 0\n",
    "for line in processed_data:\n",
    "    counter += 1\n",
    "    print(counter, \"/\", len(processed_data), end=\"\\r\")\n",
    "    for evidence in line[\"processed\"][\"evidentiary\"]:\n",
    "        lines.append(make_data(line[\"processed\"][\"claim\"], evidence))\n",
    "        if (len(line[\"processed\"][\"evidentiary\"]) >= 1):\n",
    "            classes.append(1)\n",
    "    for evidence in line[\"processed\"][\"non_evidentiary\"]:\n",
    "        lines.append(make_data(line[\"processed\"][\"claim\"], evidence))\n",
    "        classes.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:24.851950Z",
     "start_time": "2019-04-17T08:53:24.365274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points =  1605875\n",
      "Of which evidentiary: 201060\n",
      "Total training data points =  1445287\n",
      "Of which evidentiary: 180997\n",
      "Total testing data points =  160588\n",
      "Of which evidentiary: 20063\n"
     ]
    }
   ],
   "source": [
    "print(\"Total data points = \", len(classes))\n",
    "print(\"Of which evidentiary:\", np.count_nonzero(classes))\n",
    "\n",
    "training_lines = lines[:-len(classes)//10]\n",
    "training_classes = classes[:-len(classes)//10]\n",
    "\n",
    "print(\"Total training data points = \", len(training_classes))\n",
    "print(\"Of which evidentiary:\", np.count_nonzero(training_classes))\n",
    "\n",
    "training_evidentiary_indices = [i for i in range(len(training_classes)) if training_classes[i] > 0 ]\n",
    "training_nonevidentiary_indices = [i for i in range(len(training_classes)) if training_classes[i] == 0]\n",
    "\n",
    "testing_lines = lines[-len(classes)//10:]\n",
    "testing_classes = classes[-len(classes)//10:]\n",
    "\n",
    "testing_evidentiary_indices = [i for i in range(len(testing_classes)) if testing_classes[i] > 0 ]\n",
    "testing_nonevidentiary_indices = [i for i in range(len(testing_classes)) if testing_classes[i] == 0]\n",
    "\n",
    "print(\"Total testing data points = \", len(testing_classes))\n",
    "print(\"Of which evidentiary:\", np.count_nonzero(testing_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Fact Extraction (ReFE)\n",
    "\n",
    "* Each sequence (claim/question and evidence) are put through bert separately. \n",
    "* The output of both is passed through a biDAF layer. \n",
    "* The claim and the output of bidaf are further passed through inner attention layers\n",
    "* The outputs of the inner attention are concatenated and passed through a perceptron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:24.856981Z",
     "start_time": "2019-04-17T08:53:24.853962Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp16 = False\n",
    "if (train_fp16):\n",
    "    from apex import amp\n",
    "    from apex.fp16_utils import FP16_Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:26.950841Z",
     "start_time": "2019-04-17T08:53:26.941580Z"
    }
   },
   "outputs": [],
   "source": [
    "lossFn = torch.nn.BCEWithLogitsLoss()\n",
    "def getLoss(pred, actual, lossFn, e_weight=0.6, ne_weight=0.4):\n",
    "    loss = lossFn(pred.squeeze(-1), actual)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:24.864951Z",
     "start_time": "2019-04-17T08:53:24.858628Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "epoch_vals = []\n",
    "epoch_accs = []\n",
    "epoch_evid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:26.932264Z",
     "start_time": "2019-04-17T08:53:24.867645Z"
    }
   },
   "outputs": [],
   "source": [
    "def getTrainingBatch(bs = 64, validation = False):\n",
    "    \n",
    "    evidentiary = training_evidentiary_indices\n",
    "    non_evidentiary = training_nonevidentiary_indices\n",
    "    source = training_lines\n",
    "    source_classes = training_classes\n",
    "    \n",
    "    if (validation):\n",
    "        evidentiary = testing_evidentiary_indices\n",
    "        non_evidentiary = testing_nonevidentiary_indices\n",
    "        source = testing_lines\n",
    "        source_classes = testing_classes\n",
    "        \n",
    "    ev_total = bs // 2\n",
    "    nev_total = bs - ev_total\n",
    "    x = np.random.randint(0, len(evidentiary), (ev_total))\n",
    "    x = np.asarray(evidentiary)[x]\n",
    "    _base_ctokens = [source[index][0] for index in x]\n",
    "    _base_etokens = [source[index][1] for index in x]\n",
    "\n",
    "    _classes = [source_classes[index] for index in x]\n",
    "    \n",
    "    x = np.random.randint(0, len(non_evidentiary), (nev_total))\n",
    "    x = np.asarray(non_evidentiary)[x]\n",
    "    _base_ctokens_ne = [source[index][0] for index in x]\n",
    "    _base_etokens_ne = [source[index][1] for index in x]\n",
    "    _classes_ne = [source_classes[index] for index in x]\n",
    "    \n",
    "    _base_ctokens.extend(_base_ctokens_ne)\n",
    "    _base_etokens.extend(_base_etokens_ne)\n",
    "    _classes.extend(_classes_ne)\n",
    "        \n",
    "    final_seq = [i for i in range(bs)]\n",
    "    np.random.shuffle(final_seq)\n",
    "    \n",
    "    ctokens = []\n",
    "    etokens = []\n",
    "    csegments = np.zeros((bs, max_len_claims))\n",
    "    esegments = np.zeros((bs, max_len_evid))\n",
    "    classes = []\n",
    "    for index in final_seq:\n",
    "        ctokens.append(_base_ctokens[index])\n",
    "        etokens.append(_base_etokens[index])\n",
    "        classes.append(_classes[index])\n",
    "    \n",
    "    ctokens = torch.LongTensor(ctokens).cuda()\n",
    "    csegments = torch.LongTensor(csegments).cuda()\n",
    "    etokens = torch.LongTensor(etokens).cuda()\n",
    "    esegments = torch.LongTensor(esegments).cuda()\n",
    "    classes = torch.FloatTensor(classes).cuda()\n",
    "    catt_mask = ctokens != 0\n",
    "    eatt_mask = etokens != 0\n",
    "    \n",
    "    return ctokens, csegments, catt_mask, etokens, esegments, eatt_mask, classes\n",
    "    \n",
    "ctokens, csegments, catt_mask, etokens, esegments, eatt_mask, classes = getTrainingBatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:26.939616Z",
     "start_time": "2019-04-17T08:53:26.934281Z"
    }
   },
   "outputs": [],
   "source": [
    "print(ctokens.size(), csegments.size(), catt_mask.size())\n",
    "print(etokens.size(), esegments.size(), eatt_mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:26.963241Z",
     "start_time": "2019-04-17T08:53:26.952075Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from QA_Attentions import *\n",
    "\n",
    "class ReFE(torch.nn.Module):\n",
    "    def __init__(self, bert_model = \"bert-base-uncased\"):\n",
    "        super(ReFE, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.bert_width = 768\n",
    "        if (\"-large-\" in self.bert_model):\n",
    "            self.bert_width = 1024\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.wd = torch.nn.Parameter(torch.FloatTensor(np.random.uniform(0, 1, (3*self.bert_width,))))\n",
    "        self.innerAttQuery = torch.nn.Parameter(torch.FloatTensor(np.random.uniform(0, 1, (self.bert_width, 512))))\n",
    "        self.innerAttDoc = torch.nn.Parameter(torch.FloatTensor(np.random.uniform(0, 1, (self.bert_width*4, 512))))\n",
    "        self.out = torch.nn.Linear((self.bert_width*4),1)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, dt, ds, da, qt, qs, qa):\n",
    "        queries, pooled = self.bert(qt, \n",
    "                         token_type_ids=qs, \n",
    "                         attention_mask=qa, \n",
    "                         output_all_encoded_layers=False)\n",
    "        \n",
    "        documents, pooled = self.bert(dt, \n",
    "                         token_type_ids=ds, \n",
    "                         attention_mask=da, \n",
    "                         output_all_encoded_layers=False)\n",
    "        \n",
    "        if (train_fp16):\n",
    "            queries = self.dropout(queries * qa.unsqueeze(-1).half().cuda())\n",
    "            documents = self.dropout(documents * da.unsqueeze(-1).half().cuda())\n",
    "        else:\n",
    "            queries = self.dropout(queries * qa.unsqueeze(-1).float())\n",
    "            documents = self.dropout(documents * da.unsqueeze(-1).float())\n",
    "        \n",
    "        bdaf, ad2q, aq2d = biDAF(documents, queries, self.wd)\n",
    "        #q = InnerAttention(queries, self.innerAttQuery)\n",
    "        d = InnerAttention(bdaf, self.innerAttDoc)\n",
    "        #_f = torch.cat([q,d],dim=-1)\n",
    "        out_ = self.out(d)\n",
    "        return out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:42.779821Z",
     "start_time": "2019-04-17T08:53:42.776560Z"
    }
   },
   "outputs": [],
   "source": [
    "continue_from_prev = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:48.540785Z",
     "start_time": "2019-04-17T08:53:42.976400Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "network = None\n",
    "if (continue_from_prev):\n",
    "    network = torch.load(\"./ReFE_BestValidationLoss_save.h5\")\n",
    "    tcycle = None\n",
    "    with open(\"./saved_model_training_cycle.json\", \"r\") as f:\n",
    "        tcycle = json.loads(f.read())\n",
    "    epoch_losses = tcycle[\"training_losses\"][:-2]\n",
    "    epoch_vals = tcycle[\"validation_losses\"][:-2]\n",
    "    epoch_accs = tcycle[\"validation_accuracy\"][:-2]\n",
    "    epoch_evid = tcycle[\"evidence_accuracy\"][:-2]\n",
    "else:\n",
    "    network = ReFE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:49.604974Z",
     "start_time": "2019-04-17T08:53:49.598526Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 3e-5\n",
    "\"\"\"\n",
    "if (train_fp16):\n",
    "    lr = 3e-4\n",
    "\"\"\"\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "network.cuda()\n",
    "if (train_fp16):\n",
    "    network, optimizer = amp.initialize(network, optimizer, opt_level=\"O2\")\n",
    "    #reinitialize optimizer with a static loss scale\n",
    "    optimizer = FP16_Optimizer(optimizer, static_loss_scale=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Friends dont let friends use batch sizes > 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:50.337845Z",
     "start_time": "2019-04-17T08:53:50.328101Z"
    }
   },
   "outputs": [],
   "source": [
    "e_trg_losses = []\n",
    "ne_training_losses = []\n",
    "import copy\n",
    "\n",
    "def _save(cause, network):\n",
    "    print(\"\\tSaving Model for Cause:\", cause)\n",
    "    torch.save(network.state_dict(), \"./ReFE_\" + cause + \"_save.h5\")\n",
    "    with open(\"./ReFE\" + cause + \"_training_cycle.json\", \"w\") as f:            \n",
    "        f.write(json.dumps(\n",
    "            {\n",
    "                \"training_losses\":epoch_losses,\n",
    "                \"validation_losses\":epoch_vals,\n",
    "                \"validation_accuracy\":epoch_accs,\n",
    "                \"evidence_accuracy\":epoch_evid        \n",
    "            }\n",
    "        ))\n",
    "        f.close()\n",
    "    \n",
    "def chooseModelSave(network):\n",
    "    save = False\n",
    "    if (np.min(epoch_vals) == epoch_vals[-1]):\n",
    "        cause = \"BestValidationLoss\"\n",
    "        _save(cause, network)\n",
    "    \n",
    "    if (np.max(epoch_accs) == epoch_accs[-1]):\n",
    "        cause = \"BestValidationOverallAccuracy\"\n",
    "        _save(cause, network)\n",
    "    \n",
    "    if (np.max(epoch_evid) == epoch_evid[-1]):\n",
    "        cause = \"BestValidationEvidentiaryAccuracy\"\n",
    "        _save(cause, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:51.160509Z",
     "start_time": "2019-04-17T08:53:51.147473Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(network, bs=100, num_batches=5):\n",
    "    \n",
    "    classes = torch.FloatTensor([]).cuda()\n",
    "    preds = torch.FloatTensor([]).cuda()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            ct, cs, ca, et, es, ea, classes_ = getTrainingBatch(bs=bs, validation=True)\n",
    "            y_ = network.forward(et, es, ea, ct, cs, ca)\n",
    "            classes = torch.cat([classes, classes_], dim=0)\n",
    "            preds = torch.cat([preds, y_], dim=0)\n",
    "        \n",
    "        evidences = classes >= 1\n",
    "        f_loss = getLoss(preds, classes, lossFn)\n",
    "        pred = torch.round(torch.sigmoid(preds)).squeeze(-1)\n",
    "        acc = torch.sum(pred == classes)\n",
    "        acc = acc.cpu().numpy()/(bs*num_batches)\n",
    "        positives = torch.sum(pred[evidences] == classes[evidences])\n",
    "        \n",
    "        return f_loss.data.item(), acc, positives.cpu().numpy()/torch.sum(evidences).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:53:55.662283Z",
     "start_time": "2019-04-17T08:53:52.083816Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def Train(network, bs = 24, epochs=30, batches_per_epoch=10000):\n",
    "    val_min = 1000\n",
    "    if (continue_from_prev):\n",
    "        val_min = np.min(epoch_vals)\n",
    "    for k in range(epochs):\n",
    "        batch_losses = []\n",
    "        for i in range(batches_per_epoch):\n",
    "            ct, cs, ca, et, es, ea, classes = getTrainingBatch(bs=bs)\n",
    "            y_ = network.forward(et, es, ea, ct, cs, ca)\n",
    "            optimizer.zero_grad()\n",
    "            f_loss = getLoss(y_, classes, lossFn)            \n",
    "            if (train_fp16):\n",
    "                with amp.scale_loss(f_loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                    optimizer.step()\n",
    "            else:\n",
    "                f_loss.backward()\n",
    "                optimizer.step()\n",
    "            batch_losses.append(f_loss.data.item())\n",
    "            print(\"Epoch:\", k+1, \n",
    "                  \"Batch:\", i+1, \n",
    "                  \"Loss:\", np.round(np.mean(batch_losses),5), \n",
    "                  end=\"\\r\")\n",
    "        epoch_losses.append(np.mean(batch_losses))\n",
    "        val_loss, acc, evid_acc = validate(network, num_batches=10)\n",
    "        \n",
    "        epoch_vals.append(val_loss)\n",
    "        epoch_accs.append(acc)\n",
    "        epoch_evid.append(evid_acc)\n",
    "        \n",
    "        print(\"\\n\\tValidation Loss:\", np.round(val_loss,5))\n",
    "        print(\"\\tOverall Validation Accuracy:\", np.round(acc,2), \"; and for evidence only:\", np.round(evid_acc,2))\n",
    "        \n",
    "        if (val_loss < val_min):\n",
    "            val_min = val_loss\n",
    "            \n",
    "        chooseModelSave(network)\n",
    "        \n",
    "        with open(\"./ReFE_training_cycle.json\", \"w\") as f:            \n",
    "            f.write(json.dumps(\n",
    "                {\n",
    "                    \"training_losses\":epoch_losses,\n",
    "                    \"validation_losses\":epoch_vals,\n",
    "                    \"validation_accuracy\":epoch_accs,\n",
    "                    \"evidence_accuracy\":epoch_evid        \n",
    "                }\n",
    "            ))\n",
    "            f.close()\n",
    "\n",
    "tot_epochs = 3\n",
    "bs = 32\n",
    "batches_per_epoch = (len(training_evidentiary_indices) * 2) // bs\n",
    "\n",
    "print(\"Batches Per Epoch will be:\", batches_per_epoch)\n",
    "\n",
    "if (train_fp16):\n",
    "    bs = 128\n",
    "#Train(network, bs=5, epochs=3, batches_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train(network, bs=bs, epochs=tot_epochs, batches_per_epoch=batches_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReFE(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=3072, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fp16 = False\n",
    "network = ReFE()\n",
    "network.load_state_dict(torch.load(\"./ReFE_BestValidationEvidentiaryAccuracy_save.h5\"))\n",
    "network.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 30])\n",
      "torch.Size([100, 30])\n",
      "torch.Size([100, 30])\n",
      "torch.Size([100, 90])\n"
     ]
    }
   ],
   "source": [
    "def testingBatch(bs = 100):\n",
    "    source = testing_lines\n",
    "    source_classes = testing_classes\n",
    "    x = np.random.randint(0, len(testing_lines), (bs,))\n",
    "    _ctokens = [source[index][0] for index in x]\n",
    "    _etokens = [source[index][1] for index in x]\n",
    "    csegments = np.zeros((bs, max_len_claims))\n",
    "    esegments = np.zeros((bs, max_len_evid))\n",
    "    _classes = [source_classes[index] for index in x]\n",
    "\n",
    "    ctokens = torch.LongTensor(_ctokens).cuda()\n",
    "    csegments = torch.LongTensor(csegments).cuda()\n",
    "    etokens = torch.LongTensor(_etokens).cuda()\n",
    "    esegments = torch.LongTensor(esegments).cuda()\n",
    "    classes = torch.FloatTensor(_classes).cuda()\n",
    "    catt_mask = ctokens != 0\n",
    "    eatt_mask = etokens != 0\n",
    "    \n",
    "    return ctokens, csegments, catt_mask, etokens, esegments, eatt_mask, classes\n",
    "    \n",
    "ct, cs, ca, et, es, ea, classes_ = testingBatch()\n",
    "print(ct.size())\n",
    "print(cs.size())\n",
    "print(ca.size())\n",
    "print(et.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(network, bs=100, num_batches=32):\n",
    "    \n",
    "    classes = torch.FloatTensor([]).cuda()\n",
    "    preds = torch.FloatTensor([]).cuda()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            ct, cs, ca, et, es, ea, classes_ = testingBatch(bs=bs)\n",
    "            y_ = network.forward(et, es, ea, ct, cs, ca)\n",
    "            classes = torch.cat([classes, classes_], dim=0)\n",
    "            preds = torch.cat([preds, y_], dim=0)\n",
    "        evidences = classes >= 1\n",
    "        f_loss = getLoss(preds, classes, lossFn)\n",
    "        \n",
    "        pred = torch.round(F.sigmoid(preds)).squeeze(-1)\n",
    "        acc = torch.sum(pred == classes)\n",
    "        acc = acc.cpu().numpy()/(bs*num_batches)\n",
    "        positives = torch.sum(pred[evidences] == classes[evidences])\n",
    "        \n",
    "        return f_loss.data.item(), acc, positives.cpu().numpy()/torch.sum(evidences).cpu().numpy(), preds, classes, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8271875\n",
      "Negative Samples: 2273 / 2776 0.82\n",
      "Positive Samples: 374 / 424 0.88\n"
     ]
    }
   ],
   "source": [
    "l, acc, p_acc, y_, y = validate(network)\n",
    "cf = confusion_matrix(y.cpu().numpy(), np.asarray(torch.round(y_).int().cpu().numpy(), dtype=np.int8))\n",
    "y_ = torch.round(F.sigmoid(y_))\n",
    "\n",
    "act_1 = y == 1\n",
    "act_2 = y_ == 1\n",
    "act_5 = y == 0\n",
    "act_6 = y_ == 0\n",
    "a = torch.sum(act_1)\n",
    "b = torch.sum(act_2)\n",
    "e = torch.sum(act_5)\n",
    "f = torch.sum(act_6)\n",
    "print(acc)\n",
    "\n",
    "x_0 = torch.sum(y_[act_5] == 0).cpu().numpy()\n",
    "x__0 = e.cpu().numpy()\n",
    "print(\"Negative Samples:\", x_0, \"/\", x__0, np.round(x_0/x__0, 2))\n",
    "x_1 = torch.sum(y_[act_1] == 1).cpu().numpy()\n",
    "x__1 = a.cpu().numpy()\n",
    "print(\"Positive Samples:\", x_1, \"/\", x__1, np.round(x_1/x__1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y__ = y_.squeeze(-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(y, y__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9a6290a748>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cf, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
