{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:04:25.235467Z",
     "start_time": "2019-04-28T16:04:24.523879Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from Attentions import *\n",
    "import json\n",
    "import pickle\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:04:44.697157Z",
     "start_time": "2019-04-28T16:04:25.237021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "all_data = pickle.load(open(\"./training_0.pickle\", \"rb\"))\n",
    "print(len(all_data))\n",
    "training = all_data[:-len(all_data)//10]\n",
    "testing = all_data[-len(all_data)//10:]\n",
    "\n",
    "max_doc_length = 100\n",
    "max_summary_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:04:44.734994Z",
     "start_time": "2019-04-28T16:04:44.699172Z"
    }
   },
   "outputs": [],
   "source": [
    "_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:04:46.575811Z",
     "start_time": "2019-04-28T16:04:44.736452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100]) torch.Size([5, 100]) torch.Size([5, 100]) torch.Size([5, 20]) torch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "def genBatch(bs = 5, validation = False):\n",
    "    data = training\n",
    "    if (validation):\n",
    "        data = testing\n",
    "    indices = np.random.randint(0, len(data), (bs,))\n",
    "    docs = [data[index][\"story_tokens\"] for index in indices]\n",
    "    _pointers = [data[index][\"pointers\"] for index in indices]\n",
    "    \n",
    "    documents = []\n",
    "    summaries = []\n",
    "    pointers = []\n",
    "    for doc in docs:\n",
    "        doc = doc[1:]\n",
    "        doc.insert(0, 101) #<- 101 is the token id for the CLS token\n",
    "        while (len(doc) < max_doc_length):\n",
    "            doc.append(0)\n",
    "        doc = doc[:max_doc_length]\n",
    "        documents.append(doc)\n",
    "    #print(documents)  \n",
    "    \n",
    "    #print(indices)\n",
    "    sums = [data[index][\"summary_tokens\"] for index in indices]\n",
    "    #print(sums)\n",
    "    for k in range(len(sums)):\n",
    "        summ = sums[k]\n",
    "        _point = _pointers[k]\n",
    "        while (len(summ) < max_summary_length):\n",
    "            summ.append(0)\n",
    "        summ = summ[:max_summary_length]\n",
    "        summaries.append(summ)\n",
    "        points = np.zeros((len(summ),))\n",
    "        _point_choice = np.asarray(_point) < max_summary_length\n",
    "        _point = np.asarray(_point)[_point_choice]\n",
    "        if (len(_point) > 0):\n",
    "            points[_point] = 1\n",
    "        pointers.append(points)\n",
    "        \n",
    "    if _cuda:\n",
    "        documents = torch.LongTensor(documents).cuda()\n",
    "        summaries = torch.LongTensor(summaries).cuda()\n",
    "        segments = torch.zeros_like(documents).cuda()\n",
    "        pointers = torch.FloatTensor(pointers).cuda()\n",
    "    else:\n",
    "        documents = torch.LongTensor(documents)\n",
    "        summaries = torch.LongTensor(summaries)\n",
    "        segments = torch.zeros_like(documents)\n",
    "        pointers = torch.FloatTensor(pointers)\n",
    "    mask = documents > 0\n",
    "    \n",
    "    return documents, segments, mask, summaries, pointers\n",
    "    \n",
    "d, se, m, su, po = genBatch()\n",
    "print(d.size(), se.size(), m.size(), su.size(), po.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:04:46.580634Z",
     "start_time": "2019-04-28T16:04:46.577165Z"
    }
   },
   "outputs": [],
   "source": [
    "def resolvePreviouslyGeneratedText(arr, innerAttentionMatrix, resolutionMatrix):\n",
    "    #_allPrev = torch.cat(arr, dim=1)\n",
    "    _allPrev = arr\n",
    "    prev_ = InnerAttention(_allPrev, innerAttentionMatrix)\n",
    "    if (len(prev_.size()) == 2):\n",
    "        prev_ = prev_.unsqueeze(1)\n",
    "    prev_ = torch.sum(prev_, dim=1)\n",
    "    return torch.matmul(prev_, resolutionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:04:46.608339Z",
     "start_time": "2019-04-28T16:04:46.582691Z"
    }
   },
   "outputs": [],
   "source": [
    "class Summarizer(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 bert_model = \"bert-base-uncased\",\n",
    "                 attention_dim = 512,\n",
    "                 tf = True,\n",
    "                 isCuda = True):\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.bert_width = 768\n",
    "        self.bert_model = bert_model\n",
    "        self.iscuda = isCuda\n",
    "        self.teacherForcing = tf\n",
    "        if (\"-large-\" in self.bert_model):\n",
    "            self.bert_width = 1024\n",
    "        \n",
    "        \"\"\" GRU \"\"\"\n",
    "        self.wz = torch.nn.Parameter(torch.rand((self.bert_width*3, self.bert_width)))\n",
    "        self.wr = torch.nn.Parameter(torch.rand((self.bert_width*3, self.bert_width*3)))\n",
    "        self.w_cand = torch.nn.Parameter(torch.rand((self.bert_width*3, self.bert_width)))\n",
    "        \n",
    "        torch.nn.init.normal_(self.wz)\n",
    "        torch.nn.init.normal_(self.wr)\n",
    "        torch.nn.init.normal_(self.w_cand)\n",
    "        \"\"\" BERT \"\"\"\n",
    "        if (self.iscuda):\n",
    "            self.bert = BertModel.from_pretrained(bert_model).cuda()\n",
    "        else:\n",
    "            self.bert = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "        \"\"\" UaHj, Wa, Va; Weights for the context vector\"\"\"\n",
    "        self.ua = torch.nn.Parameter(torch.rand(self.bert_width, 256))\n",
    "        self.wa = torch.nn.Parameter(torch.rand(self.bert_width, 256))\n",
    "        self.va = torch.nn.Parameter(torch.rand((256,)))\n",
    "        torch.nn.init.normal_(self.ua)\n",
    "        torch.nn.init.normal_(self.wa)\n",
    "        torch.nn.init.normal_(self.va)\n",
    "        \n",
    "        \"\"\" OUTPUT \"\"\"\n",
    "        self.output_ = torch.nn.Parameter(torch.ones((self.bert_width, 30000)))\n",
    "        #torch.nn.init.normal_(self.output_)\n",
    "        self.output_to_network_embedding = torch.nn.Embedding(30000, self.bert_width)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        \n",
    "    def init_hidden_state(self, size):\n",
    "        if (self.iscuda):\n",
    "            _prev_word = torch.LongTensor([[101]]).cuda()#<- this is basically the cls marker\n",
    "        else:\n",
    "            _prev_word = torch.LongTensor([[101]])\n",
    "        _prev_word = _prev_word.repeat(size[0], 1)\n",
    "        \n",
    "        _hs = None\n",
    "        if (self.cuda):\n",
    "            _hs = torch.rand(size).cuda()\n",
    "        else:\n",
    "            _hs = torch.rand(size)\n",
    "        \n",
    "        torch.nn.init.normal_(_hs)\n",
    "        return _hs, [_prev_word]\n",
    "    \n",
    "    def forward(self, docs, segments, masks, output_ts = 75, y = None, tf_prob = 0.25):\n",
    "        pointers = []\n",
    "        atts = []\n",
    "        hs, _output_words = self.init_hidden_state((docs.size()[0],1, self.bert_width))\n",
    "        \n",
    "        _docs, _ = self.bert(docs, segments, masks, output_all_encoded_layers = False)\n",
    "        _docs = _docs * masks.unsqueeze(-1).float()\n",
    "        _docs = self.dropout(_docs)\n",
    "        _x = _docs\n",
    "        _uahj = torch.matmul(_docs, self.ua)\n",
    "        generated_words = []\n",
    "        \n",
    "        for i in range(output_ts):\n",
    "            _generatedContext = None\n",
    "            w = _output_words[-1]\n",
    "            y_in = self.output_to_network_embedding(w)    \n",
    "\n",
    "            #context vector generation for the doc space\n",
    "            _stwa = torch.matmul(hs, self.wa)\n",
    "            _xatt = bahdanauAttention(_uahj, _stwa, self.va)\n",
    "            _dcv = _docs * _xatt.unsqueeze(-1)\n",
    "            doc_context_vector = torch.sum(_dcv, dim=1).unsqueeze(1)\n",
    "\n",
    "            #gru gating\n",
    "            _gru_in = torch.cat([y_in, hs, doc_context_vector], dim=-1)            \n",
    "            z = torch.sigmoid(torch.matmul(_gru_in, self.wz))\n",
    "            r = torch.sigmoid(torch.matmul(_gru_in, self.wr))\n",
    "            \n",
    "            #candidate hidden state and final hidden state for the gru\n",
    "            _cand_in = _gru_in*r\n",
    "            h_cand = torch.tanh(torch.matmul(_cand_in, self.w_cand))\n",
    "            hs = (1-z)*hs + z*h_cand\n",
    "            \n",
    "            #generate the output word\n",
    "            word = torch.matmul(self.dropout(hs), self.output_)\n",
    "            generated_words.append(word)\n",
    "            \n",
    "            _word = F.softmax(word, dim=-1)\n",
    "            _word = torch.max(_word, dim=-1)[1].detach()\n",
    "            if (self.teacherForcing and (y is not None)):\n",
    "                choice = np.random.randint(0, 100, (1,))[0]\n",
    "                if (choice < tf_prob*100):\n",
    "                    _word = y[:,i].unsqueeze(-1).detach()\n",
    "\n",
    "            _output_words.append(_word)\n",
    "            atts.append(_xatt.unsqueeze(1))\n",
    "        \n",
    "        return torch.cat(generated_words, dim=1), torch.cat(atts, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:04:46.615586Z",
     "start_time": "2019-04-28T16:04:46.609883Z"
    }
   },
   "outputs": [],
   "source": [
    "continue_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:04:46.621273Z",
     "start_time": "2019-04-28T16:04:46.617642Z"
    }
   },
   "outputs": [],
   "source": [
    "network = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:04:53.636064Z",
     "start_time": "2019-04-28T16:04:47.963912Z"
    }
   },
   "outputs": [],
   "source": [
    "network = Summarizer(isCuda = _cuda).cuda()\n",
    "\n",
    "continueForCause = \"BestTrainingLoss\"\n",
    "if (_cuda):\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPU(s)...\")\n",
    "        network = torch.nn.DataParallel(network)\n",
    "\n",
    "    epoch_losses = []\n",
    "    epoch_vals = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    if (continue_training):\n",
    "        network.load_state_dict(torch.load(\"./summarizer_\" + continueForCause + \".h5\"))\n",
    "        _training_cylce = None\n",
    "        with open(\"./Summarizer_training_cycle_\" + continueForCause + \".json\") as f:\n",
    "            _training_cylce = json.loads(f.read())\n",
    "            epoch_losses = _training_cylce[\"training_losses\"]\n",
    "            epoch_vals = _training_cylce[\"validation_losses\"]\n",
    "            print(epoch_losses, epoch_vals)\n",
    "\n",
    "    network.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "alpha = 1.\n",
    "beta = 1.\n",
    "gamma = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:04:55.362184Z",
     "start_time": "2019-04-28T16:04:55.351969Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def viz(epoch):\n",
    "    with torch.no_grad():\n",
    "        d, se, m, su, po = genBatch(bs=8)\n",
    "        su = su[:,:10]\n",
    "        words, atts = network.forward(d, se, m, output_ts=10)\n",
    "        l = CoverageLoss(atts)\n",
    "        data = atts.squeeze(-1).cpu().numpy()\n",
    "        for i in range(2):\n",
    "            \"\"\"\n",
    "            if (epoch % 5 == 0):\n",
    "                plt.figure(figsize=(105, 15))\n",
    "                _s = sns.heatmap(data[i], annot=True, vmin=0.01, vmax = 1)\n",
    "                fig = _s.get_figure()\n",
    "                fig.savefig(\"epoch_\" + str(epoch) + \"_\" + str(i) + \".png\")\n",
    "            \"\"\"\n",
    "            words2 = F.softmax(words, dim=1)\n",
    "            print(words2.topk(10, dim=-1)[1][i])\n",
    "            print(su[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:04:58.436776Z",
     "start_time": "2019-04-28T16:04:58.416294Z"
    }
   },
   "outputs": [],
   "source": [
    "def _save(cause):\n",
    "    print(\"\\t\\t...saving model for cause\", cause)\n",
    "    torch.save(network.state_dict(), \"./summarizer_\" + cause + \".h5\")\n",
    "    with open(\"./Summarizer_training_cycle_\"  + cause + \".json\", \"w\") as f:            \n",
    "        f.write(json.dumps(\n",
    "            {\n",
    "                \"training_losses\":epoch_losses,\n",
    "                \"validation_losses\":epoch_vals,\n",
    "                \"validation_accuracy\":epoch_accs,\n",
    "            }\n",
    "        ))\n",
    "        f.close()\n",
    "\n",
    "def ValidateModel(validation_bs = 2, epoch=0):\n",
    "    all_su = None\n",
    "    all_pred = None\n",
    "    \n",
    "    print(\"\\n\\tValidating...\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(10):\n",
    "\n",
    "            d, se, m, su, po = genBatch(bs=validation_bs, validation = True)\n",
    "            su = su[:,:10]\n",
    "            words, atts = network.forward(d, se, m, output_ts=10)\n",
    "            if (all_su is None):\n",
    "                all_su = su\n",
    "                all_pred = words\n",
    "            else:\n",
    "                all_su = torch.cat([all_su, su], dim=0)\n",
    "                all_pred = torch.cat([all_pred, words], dim=0)\n",
    "        val_loss = WordLoss(all_su, all_pred)\n",
    "        print(\"\\tValidation Loss:\", np.round(val_loss.data.item(), 5))\n",
    "        epoch_vals.append(val_loss.data.item())\n",
    "\n",
    "    with open(\"./Summarizer_training_cycle.json\", \"w\") as f:            \n",
    "        f.write(json.dumps(\n",
    "            {\n",
    "                \"training_losses\":epoch_losses,\n",
    "                \"validation_losses\":epoch_vals,\n",
    "                \"validation_accuracy\":epoch_accs,\n",
    "            }\n",
    "        ))\n",
    "        f.close()\n",
    "    \n",
    "    if (np.min(epoch_vals) == epoch_vals[-1]):\n",
    "        _save(\"BestValidationLoss\")        \n",
    "    \n",
    "    if (np.min(epoch_losses) == epoch_losses[-1]):\n",
    "        _save(\"BestTrainingLoss\")\n",
    "    \n",
    "    print(\"\\tVisualizing...\")\n",
    "    viz(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:05:00.617797Z",
     "start_time": "2019-04-28T16:05:00.605933Z"
    }
   },
   "outputs": [],
   "source": [
    "pointerCriterion = torch.nn.BCEWithLogitsLoss()\n",
    "wordCriterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def CoverageLoss(attentions):\n",
    "    \"\"\"\n",
    "    :param attentions: (b, yt, xt)\n",
    "    :param coverages: (b, yt, xt)\n",
    "    \"\"\"\n",
    "    coverage = None\n",
    "    losses = None\n",
    "    if (_cuda):\n",
    "        coverage = torch.zeros((attentions.size()[0], attentions.size()[-1])).cuda()\n",
    "    else:\n",
    "        coverage = torch.zeros((attentions.size()[0], attentions.size()[-1]))\n",
    "    \n",
    "    losses = []\n",
    "    for i in range(attentions.size()[1]):\n",
    "        cov = torch.min(coverage, attentions[:,i,:])\n",
    "        _ts_loss = torch.sum(cov, dim=1)\n",
    "        losses.append(_ts_loss)\n",
    "        coverage = coverage + attentions[:,i,:]\n",
    "    \n",
    "    losses = torch.stack(losses)\n",
    "    _loss = torch.sum(losses)/(attentions.size()[0]*attentions.size()[1])\n",
    "    return _loss\n",
    "\n",
    "def PointerLoss(yPointers, y_Pointers):\n",
    "    return pointerCriterion(y_Pointers, yPointers)\n",
    "\n",
    "def WordLoss(yWords, y_Words):\n",
    "    return wordCriterion(y_Words.view(-1,30000), yWords.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:05:01.898425Z",
     "start_time": "2019-04-28T16:05:01.885608Z"
    }
   },
   "outputs": [],
   "source": [
    "def annealTFRate(epoch):\n",
    "    return 0.75 - epoch/100\n",
    "\n",
    "def Train(epochs, batches_per_epoch, bs):\n",
    "    for k in range(epochs):\n",
    "        batch_losses = []\n",
    "        b_wl = []\n",
    "        b_pl = []\n",
    "        b_cl = []\n",
    "        for j in range(batches_per_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            d, se, m, su, po = genBatch(bs=bs)\n",
    "            su = su[:,:10]\n",
    "            words, atts = network.forward(d, se, m, output_ts=10, y=su, tf_prob=annealTFRate(k))\n",
    "            _word_loss = alpha * WordLoss(su, words)\n",
    "            _coverage_loss = CoverageLoss(atts)\n",
    "            total_loss = _word_loss + _coverage_loss\n",
    "\n",
    "            b_wl.append(_word_loss.data.item())\n",
    "            b_cl.append(_coverage_loss.detach().cpu().numpy())\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_losses.append(total_loss.data.item())\n",
    "            \n",
    "            _str = \"Epoch: \" + str(k+1) + \\\n",
    "            \";  Batch: \" + str(j+1)  + \"/\" + str(batches_per_epoch) + \\\n",
    "            \"; Loss: \" + str(np.round(np.mean(batch_losses), 5)) \n",
    "            \n",
    "            _str = _str + \" (\" + str(np.round(np.mean(b_wl),5))  + \\\n",
    "            \",\" + str(np.round(np.mean(b_cl), 5)) + \")\"\n",
    "            \n",
    "            print(_str, end = \"\\r\")\n",
    "        epoch_losses.append(np.mean(batch_losses))\n",
    "        ValidateModel(validation_bs = bs, epoch=k)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:05:15.607278Z",
     "start_time": "2019-04-28T16:05:15.601500Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = max_summary_length\n",
    "epochs = 40\n",
    "batches_per_epoch = 2560\n",
    "\n",
    "if (_cuda):\n",
    "    if (torch.cuda.device_count() > 1):\n",
    "        bs = 56 * torch.cuda.device_count()\n",
    "    else:\n",
    "        bs = 20\n",
    "else:\n",
    "    bs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T16:05:19.174205Z",
     "start_time": "2019-04-28T16:05:19.169171Z"
    }
   },
   "outputs": [],
   "source": [
    "def bahdanauAttention(uahj, st_wa, va):\n",
    "    _intermediate = torch.tanh(st_wa + uahj)\n",
    "    _att = F.softmax(torch.matmul(_intermediate, va), dim=-1)\n",
    "    return _att "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-28T16:05:24.174Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1;  Batch: 2560/2560; Loss: 8.47417 (7.67243,0.80174)\n",
      "\tValidating...\n",
      "\tValidation Loss: 9.5803\n",
      "\t\t...saving model for cause BestValidationLoss\n",
      "\t\t...saving model for cause BestTrainingLoss\n",
      "\tVisualizing...\n",
      "tensor([[ 2149,  4895,  1005,  7802,  3306,  2158,  8956,  2006, 10991,  2446],\n",
      "        [ 1999,  4895, 23876,  2125, 21931,  1005,  2000, 15549,  2395,  8408],\n",
      "        [ 1999,  1996,  5712,  2034,  5618,  4380,  2149,  2125,  2005,  2859],\n",
      "        [13492, 15549,  2102, 20351,  2154,  4030,  4119, 16558,  8366,  6335],\n",
      "        [11096,  3926,  2110,  2391,  5375,  6921,  4552,  4001,  2645,  2772],\n",
      "        [ 3771, 11074, 14995,  9739, 20129,  2733, 12532,  3030, 16181, 12913],\n",
      "        [11074, 24636, 12944, 23902,  2733,  9368, 26775, 13525, 13317, 27105],\n",
      "        [    0,  2733,  2311, 21329,  2007,  2504,  3204,  4399, 11110,  3357],\n",
      "        [    0,  2923,  8858, 24168,  2007,  2733,  2504,  3864,  9109,  1999],\n",
      "        [    0,  2504,  8858,  2622,  2733,  2095,  4945,  2007,  4332,  2663]],\n",
      "       device='cuda:0')\n",
      "tensor([ 7269, 17654,  2015,  2007,  2149,  2000, 11147, 10130, 12135,     0],\n",
      "       device='cuda:0')\n",
      "tensor([[15768,  6629,  8661,  4524,  2605,  6746, 10819,  2610,  3000, 13085],\n",
      "        [28588,  7986, 25644, 23307, 10819, 10448, 14625,  2906,  3048,  5363],\n",
      "        [ 1055,  3048,  6715,  6956,  8858,  2313,  4590,  6792, 28588,  8661],\n",
      "        [18798, 13747, 15595, 13351,  7502,  6715, 17578, 25871, 16282,  6087],\n",
      "        [18798,  5172, 10560,  6715, 15595, 12999, 17420, 12185, 28334, 24117],\n",
      "        [ 2007,  2000,  1043,  1996,  1005,     0,  1026,  1037,  2022,  2025],\n",
      "        [20614, 23822, 10309,  8378, 25317, 21841,  8462,  4174, 16534, 13905],\n",
      "        [ 9109, 21841,  7620,  2906,  2251, 21868, 17238, 17420,  5266,  8270],\n",
      "        [17309,  9881, 12071, 25317, 14246,  8268, 10330, 21663, 18256, 20614],\n",
      "        [14575,  9881, 21663, 17309,  5413, 19807,  8462,  5266,  7174,  6968]],\n",
      "       device='cuda:0')\n",
      "tensor([7802, 4610, 7502, 1001, 1012, 1001, 1001, 3867, 1999, 2034],\n",
      "       device='cuda:0')\n",
      "\n",
      "\n",
      "Epoch: 2;  Batch: 2560/2560; Loss: 8.48683 (7.6021,0.88473))\n",
      "\tValidating...\n",
      "\tValidation Loss: 9.69758\n",
      "\tVisualizing...\n",
      "tensor([[15195,  7509,  4811,  8026,  7409,  7802, 17359, 14887,  9976, 13018],\n",
      "        [ 8840,  7409,  2916, 19115,  7206,  2038,  7666,  2080,  7230, 12403],\n",
      "        [ 5222,  1012,  7698,  9407, 20746, 20203,  2743,  7033, 16336,  2015],\n",
      "        [20644,  2171, 25167,  7666,  2383, 12672, 26399, 13492, 18881,  4630],\n",
      "        [ 3736,  5826, 13363, 14887,  5261,  4559,  7065,  4499,  3857,  3428],\n",
      "        [ 5299, 13363,  9407,  8583,  3450, 23709,  9061,  9930,  5971, 26347],\n",
      "        [22747,  6591, 13363,  2046, 18524,  4509,  4179,  2743, 20833,  4550],\n",
      "        [ 2615,  8619,  3204, 10370,  2290, 12076,  9407, 20142,  3314, 29441],\n",
      "        [15172,  7913,  3244, 23806, 12502, 19696, 20927, 23356,  2769, 17795],\n",
      "        [21868, 10603, 23143, 12128, 22017,  4898, 10693,  5510,  4528,  6054]],\n",
      "       device='cuda:0')\n",
      "tensor([ 2605, 16393,  2015,  2176,  2454,  6363,  2005, 12899, 19472, 12801],\n",
      "       device='cuda:0')\n",
      "tensor([[12429,  3841,  5849, 16485,  3960, 14699, 13154, 18137,  6134, 18563],\n",
      "        [ 5468, 26981,  9497,  4003, 17456, 26473,  7285, 20481, 11302,  5881],\n",
      "        [28992, 27942, 16308,  5468,  6702,  3314, 15302,  6134, 10228,  3099],\n",
      "        [ 1061,  1040,  1025,  1037,  1002,     0,  1009,  1024,  1996,  2000],\n",
      "        [ 7452, 15302, 10502,  4200, 12474, 26039, 11302,  5762, 16531, 24642],\n",
      "        [ 8159,  4305,  3902,  2393,  5339,  6781, 16986,  5762,  9103,  2239],\n",
      "        [16871,  7197,  5876, 24642,  3760,  2949, 18762, 10964,  7773,  7118],\n",
      "        [ 2689, 14505,  5231,  4313, 10131, 15061, 24848,  3148,  5881, 27955],\n",
      "        [16782,  9497, 15061,  3489,  9386, 20390, 16784, 20486, 16864, 15430],\n",
      "        [15061,  3489, 14505, 16782,  6847,  8741,  6372,  4157, 16158,  9497]],\n",
      "       device='cuda:0')\n",
      "tensor([ 5673,  2417, 13699,  4135,  7274, 10558,  2000, 22889, 18163,  2005],\n",
      "       device='cuda:0')\n",
      "\n",
      "\n",
      "Epoch: 3;  Batch: 2560/2560; Loss: 8.54662 (7.66083,0.88579)\n",
      "\tValidating...\n",
      "\tValidation Loss: 9.51013\n",
      "\t\t...saving model for cause BestValidationLoss\n",
      "\tVisualizing...\n",
      "tensor([[ 5713, 13146,  6395,  3735,  9042, 17060, 24550, 21626,  7643,  9303],\n",
      "        [ 3262, 17837, 16356, 19857,  3314, 11529,  7926, 23439, 15710, 28923],\n",
      "        [22043, 15710, 23555, 19857,  3219, 10831,  4424, 21626,  4372,  9461],\n",
      "        [15710, 18296, 19079, 16599,  2719,  2049,  6831,  2166, 12706,  5142],\n",
      "        [12706, 15710, 16599, 19079,  3582,  5937, 22459, 13362, 13436,  3335],\n",
      "        [18074, 28308, 15710, 19542,  5639,  5937,  3110, 17482,  7884, 12706],\n",
      "        [ 3219,  6153, 13102,  3002,  3210,  2653, 16454, 21197,  6818, 14328],\n",
      "        [ 3219, 18074,  2566, 24610, 26878, 22964,  4942, 10992, 11464,  5936],\n",
      "        [ 2152, 17364,  1043,  2682, 26887,  2869,  2452,  8040,  8174,  2845],\n",
      "        [ 1011,  1010,  1008,  1009,  1001,     0,  1004,  1005,  1012,  1013]],\n",
      "       device='cuda:0')\n",
      "tensor([23564,  2099, 19062,  9148,  2177,  4447, 13952,  5920,  2886,     0],\n",
      "       device='cuda:0')\n",
      "tensor([[    7,     6,     4,     5,     1,     0,     2,     3,     8,     9],\n",
      "        [19701,  6238,  4198,  3114, 13340,  3444, 16871,  6485,  8559, 15089],\n",
      "        [19966, 12172,  2851, 17808,  8496,  8377,  7666,  8493, 20357,  6238],\n",
      "        [ 2851,  4257,  6238,  6082, 17808, 15085, 12430, 12871, 16059, 12228],\n",
      "        [12228, 22589,  3268, 10938, 12430,  7863, 11110, 26010, 17808, 12871],\n",
      "        [ 6238,  8559, 16059, 16402,  7316, 19701, 20225, 12682, 28675, 24403],\n",
      "        [22589, 28644, 27972,  5593, 17808, 10938,  3876, 20225, 16853,  7604],\n",
      "        [16970, 11981,  4551, 16653,  7473, 17137,  7604,  2851, 15905, 17808],\n",
      "        [ 3867,     0,  4551,  7473,  2454,  6363,  1011, 11626,  1001,  5841],\n",
      "        [    0,  3867,  6363,  2851,  4551,  7473,  3193, 16653, 22878, 17137]],\n",
      "       device='cuda:0')\n",
      "tensor([14434,  1005,  1055,  7659,  4388,  7092,  1005,  1055,  4923,  8599],\n",
      "       device='cuda:0')\n",
      "\n",
      "\n",
      "Epoch: 4;  Batch: 2560/2560; Loss: 8.60245 (7.71556,0.88689)\n",
      "\tValidating...\n",
      "\tValidation Loss: 9.99304\n",
      "\tVisualizing...\n",
      "tensor([[ 3928, 17726, 14355, 12280, 24829, 21416,  7109, 25983,  5772,  7198],\n",
      "        [    7,     6,     4,     5,     1,     0,     2,     3,     8,     9],\n",
      "        [12071,  9094,  2835,  2851,  5763, 16652,  2438,  8516,  3043,  2680],\n",
      "        [ 2552,  9857,  2005,  2835,  2039,  2713,  2954,  2006,  7566,  2096],\n",
      "        [ 9377, 14527, 14355, 21351,  8753, 10616,  2835,  9097, 18417, 28357],\n",
      "        [10616, 21351,  2669,  8795, 12038, 28002,  5918, 19901, 16165, 12226],\n",
      "        [19901, 28002, 13786, 10616, 12019, 21708, 24043, 17177, 14321, 14676],\n",
      "        [ 3687, 23659, 24934, 23896, 28002, 11589, 13918, 27300, 29232,  9766],\n",
      "        [27887, 26287,  9983, 24934, 19510, 24193, 20801, 17560, 16165,  3687],\n",
      "        [10616, 23337, 27887, 18195,  6225, 18041,  9983,  7228, 29232,  3687]],\n",
      "       device='cuda:0')\n",
      "tensor([10692,  8090,  2000,  3607,  2058, 29357, 11371,     0,     0,     0],\n",
      "       device='cuda:0')\n",
      "tensor([[18326, 12443,  5846, 22093, 25460, 24737, 20944, 13043,  7723, 27734],\n",
      "        [15166,  7015, 23845, 13602, 26246, 18884, 21383, 18219, 13948,  9268],\n",
      "        [ 7015, 17524, 11297, 14386, 13052,  6198, 23597, 12923,  7134, 22093],\n",
      "        [22058,  8252,  6651, 11877,  4313, 10736, 14191,  6987,  3555,  5459],\n",
      "        [29216, 17516,  6099,  8980,  4558,  2192,  5072,  5847,  1001,  1012],\n",
      "        [24386,  5994, 17419, 21119,  5357, 17985, 13376, 26662,  8765, 12348],\n",
      "        [17732, 23615, 12348, 23714,  5459, 12053, 23522, 11073, 28236, 28083],\n",
      "        [17152,  4181,  4887,  5302, 21759,  6260, 10057, 16485,  5917, 21723],\n",
      "        [28083, 24610, 12768,  5558, 21382, 17482, 25540, 28579,  5459, 14690],\n",
      "        [20099, 11626, 13948,  8029, 22377,  5302, 17152,  9217, 10502,  5558]],\n",
      "       device='cuda:0')\n",
      "tensor([ 2250,  2710,  2000,  4965,  9738,  2013, 29143,  7861, 10024,  2121],\n",
      "       device='cuda:0')\n",
      "\n",
      "\n",
      "Epoch: 5;  Batch: 2560/2560; Loss: 8.39675 (7.6666,0.73014))\n",
      "\tValidating...\n",
      "\tValidation Loss: 8.75476\n",
      "\t\t...saving model for cause BestValidationLoss\n",
      "\t\t...saving model for cause BestTrainingLoss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVisualizing...\n",
      "tensor([[26614, 14611,  8248, 19667, 25510, 19722, 13376,  9600, 23790, 23637],\n",
      "        [17673, 22177, 15436,  7330, 26646, 14611,  2761, 28052,  5625, 13494],\n",
      "        [27597, 28052, 16369, 22834, 11191, 20081, 13754, 23959, 25510, 25119],\n",
      "        [13494,  4102, 16482, 14459, 14611,  5720, 28227,  9584, 12246, 20871],\n",
      "        [15007, 25750, 14202, 29534, 12228,  6076, 15985, 16482,  8399, 23646],\n",
      "        [12228, 28227, 28176,  8399, 15007,  6087, 19737,  5625, 13302, 15285],\n",
      "        [24168, 13112, 20486, 10747,  8621,  9250, 16260, 14540, 28227, 16096],\n",
      "        [24947,  5543, 19737, 26868, 28227, 18257, 12071, 27922, 21549,  5162],\n",
      "        [    7,     6,     4,     5,     1,     0,     2,     3,     8,     9],\n",
      "        [12910,  5738, 13619, 27178, 11687, 26614,  3170,  8248, 11409,  6131]],\n",
      "       device='cuda:0')\n",
      "tensor([ 2093,  9302, 17671,  2915,  2757,  1999,  2642, 14474,     0,     0],\n",
      "       device='cuda:0')\n",
      "tensor([[21575,  8695, 11783, 16096, 17060, 10665, 19145, 10192,  9111, 26665],\n",
      "        [11522, 12871,  4253, 22214,  6035, 10615,  4397, 18966, 23054,  7592],\n",
      "        [24153, 24976, 19003, 10043, 15964, 18681, 15061, 23054, 22214, 29191],\n",
      "        [23054,  5510, 17752, 20710,  9816, 16966, 24059, 10992, 17883, 23646],\n",
      "        [23054,  5510, 13808, 28052,  3420, 24322, 12233, 20771, 23801, 21803],\n",
      "        [17667,  5299, 23054,  4216, 15964, 19244, 23801, 15396, 13989, 22531],\n",
      "        [23054,  7354,  6038, 23801,  6279, 12237, 29169,  4188, 10285, 15680],\n",
      "        [24759,  3011, 11522,  2401,  6805,  5699,  3582, 19145, 22690,  7870],\n",
      "        [ 1999,  1998,  1055,  1997,  1001,     0,  1025,  1028,  2000,  2005],\n",
      "        [ 6054, 10704, 11335,  9310,  6647,  7103, 13867,  9863,  8542,  8887]],\n",
      "       device='cuda:0')\n",
      "tensor([ 2148,  4759,  4049,  2096,  2108, 18948,  2185,  2013, 13249,     0],\n",
      "       device='cuda:0')\n",
      "\n",
      "\n",
      "Epoch: 6;  Batch: 2560/2560; Loss: 7.36146 (7.02561,0.33586)\n",
      "\tValidating...\n",
      "\tValidation Loss: 8.52182\n",
      "\t\t...saving model for cause BestValidationLoss\n",
      "\t\t...saving model for cause BestTrainingLoss\n",
      "\tVisualizing...\n",
      "tensor([[18325,  3505, 19420, 10993, 19695,  8903,  4963, 16782, 26911, 25176],\n",
      "        [14287, 28845,  3734, 24979,  4430, 20229, 14834,  9834,  5378, 19431],\n",
      "        [11705, 14287,  3734, 14393, 15946, 16481, 20735,  8782, 10893, 19420],\n",
      "        [13278, 19737, 27686,  6701, 23921, 15946, 13051, 21910, 28471,  5801],\n",
      "        [ 7043, 11961,  3193, 20735, 14287, 10380, 18433, 13341, 28464,  2837],\n",
      "        [ 9013, 15599,  3271, 26869, 14459,  7959, 14148, 16586,  5007,  7541],\n",
      "        [ 5104,  2407,  9382,  2904, 20807, 14287,  2223, 15863,  3556, 17680],\n",
      "        [ 7589, 25555, 25748, 24444, 24059, 10431, 15395, 13124,  4664, 22802],\n",
      "        [11915,  7548,  3178, 18886,  2454,  5335, 18895,  6415, 27257, 10372],\n",
      "        [17364,  3556, 24647,  3151,  2523,  2291, 14933, 15322,  7043,  3696]],\n",
      "       device='cuda:0')\n",
      "tensor([26075,  4599,  5307,  2088,  2452, 26717,     0,     0,     0,     0],\n",
      "       device='cuda:0')\n",
      "tensor([[    7,     6,     4,     5,     1,     0,     2,     3,     8,     9],\n",
      "        [25832, 25047, 21939,  7661, 23394,  8061,  2768, 11038, 10213, 22837],\n",
      "        [ 6820,  3762, 20800,  2083,  5833, 11434,  2464,  8847,  6930,  5270],\n",
      "        [18142, 10797,  9611,  4847, 18622, 20528,  7949, 17298, 10197,  3251],\n",
      "        [ 2378, 27775,  8684, 17771,  9032,  4077, 18142, 19939, 19943, 29107],\n",
      "        [18622,  2951,  3119,     0, 14144, 11823, 15016,  4221,  3070,  5905],\n",
      "        [17298, 14574,     0,  7599,  5095,  3119, 16613,  2094,  3070, 28289],\n",
      "        [18622, 16613,  7389,     0, 27065,  2951,  4399, 24633,  8993,  3119],\n",
      "        [    0, 23194, 17057,  3989, 18622,  7974, 25805, 17170, 12502,  2525],\n",
      "        [17057,  2951,     0,  4276,  5095, 15291,  7974,  7577,  2763, 20142]],\n",
      "       device='cuda:0')\n",
      "tensor([ 2149, 25740,  2015,  3433,  2000,  3534, 19267,     0,     0,     0],\n",
      "       device='cuda:0')\n",
      "\n",
      "\n",
      "Epoch: 7;  Batch: 481/2560; Loss: 7.26071 (6.92128,0.33943)\r"
     ]
    }
   ],
   "source": [
    "Train(epochs, batches_per_epoch, bs = bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T15:11:02.090127Z",
     "start_time": "2019-04-28T14:57:36.195Z"
    }
   },
   "outputs": [],
   "source": [
    "_save(\"LastForcedSave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T14:50:27.375026Z",
     "start_time": "2019-04-28T14:50:07.789Z"
    }
   },
   "outputs": [],
   "source": [
    "network = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T14:50:27.375798Z",
     "start_time": "2019-04-28T14:50:09.164Z"
    }
   },
   "outputs": [],
   "source": [
    "network = Summarizer()\n",
    "if (torch.cuda.device_count() > 1):\n",
    "    network = torch.nn.DataParallel(network)\n",
    "network.load_state_dict(torch.load(\"./summarizer_BestTrainingLoss.h5\"))\n",
    "network.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T12:58:00.040303Z",
     "start_time": "2019-04-28T12:57:58.945185Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "with torch.no_grad():\n",
    "    d, se, m, su, po = genBatch(bs=8)\n",
    "    words, atts = network.forward(d, se, m, output_ts=20)\n",
    "    words2 = F.softmax(words, dim=1)\n",
    "    print(words2.topk(10, dim=-1)[1][0])\n",
    "    print(su[0])\n",
    "    \n",
    "    #print(su)\n",
    "    #print(words.size())\n",
    "\n",
    "    w = torch.max(words2, dim=-1)[1]\n",
    "    #print(w)\n",
    "    _pred = tokenizer.convert_ids_to_tokens(w.cpu().numpy()[0])\n",
    "    _act = tokenizer.convert_ids_to_tokens(su.cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T03:40:39.681194Z",
     "start_time": "2019-04-27T03:40:39.676705Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\" \".join(_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T03:40:43.107281Z",
     "start_time": "2019-04-27T03:40:43.102802Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\" \".join(_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
