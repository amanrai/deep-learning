{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T13:01:59.075690Z",
     "start_time": "2019-04-24T13:01:56.014064Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = pickle.load(open(\"../../../Data/DMQA/cnn_tokenized.pickle\", \"rb\"))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T13:28:08.372273Z",
     "start_time": "2019-04-24T13:28:07.877273Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from Attentions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T13:28:35.484598Z",
     "start_time": "2019-04-24T13:28:35.479724Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "max_doc_length = 100\n",
    "max_summary_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T13:28:36.325645Z",
     "start_time": "2019-04-24T13:28:36.290105Z"
    }
   },
   "outputs": [],
   "source": [
    "def genBatch(bs = 5):\n",
    "    indices = np.random.randint(0, len(data), (bs,))\n",
    "    docs = [data[index][\"story_tokens\"] for index in indices]\n",
    "    _pointers = [data[index][\"pointers\"] for index in indices]\n",
    "    \n",
    "    documents = []\n",
    "    summaries = []\n",
    "    pointers = []\n",
    "    for doc in docs:\n",
    "        doc.insert(0, 101) #<- 101 is the token id for the CLS token\n",
    "        while (len(doc) < max_doc_length):\n",
    "            doc.append(0)\n",
    "        doc = doc[:max_doc_length]\n",
    "        documents.append(doc)\n",
    "        \n",
    "    sums = [data[index][\"summary_tokens\"] for index in indices]\n",
    "    for k in range(len(sums)):\n",
    "        summ = sums[k]\n",
    "        _point = _pointers[k]\n",
    "        summ.insert(0, 101)\n",
    "        while (len(summ) < max_summary_length):\n",
    "            summ.append(0)\n",
    "        summ = summ[:max_summary_length]\n",
    "        summaries.append(summ)\n",
    "        points = np.zeros((len(summ),))\n",
    "        _point_choice = np.asarray(_point) < max_summary_length\n",
    "        _point = np.asarray(_point)[_point_choice]\n",
    "        if (len(_point) > 0):\n",
    "            points[_point] = 1\n",
    "        pointers.append(points)\n",
    "    \n",
    "    documents = torch.LongTensor(documents)\n",
    "    summaries = torch.LongTensor(summaries)\n",
    "    segments = torch.zeros_like(documents)\n",
    "    pointers = torch.FloatTensor(pointers)\n",
    "    mask = documents > 0\n",
    "    \n",
    "    return documents, segments, mask, summaries, pointers\n",
    "    \n",
    "d, se, m, su, po = genBatch()\n",
    "print(d.size(), se.size(), m.size(), su.size(), po.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T13:28:38.373235Z",
     "start_time": "2019-04-24T13:28:38.366615Z"
    }
   },
   "outputs": [],
   "source": [
    "def resolvePreviouslyGeneratedText(arr, innerAttentionMatrix, resolutionMatrix):\n",
    "    _allPrev = torch.cat(arr, dim=1)\n",
    "    prev_ = InnerAttention(_allPrev, innerAttentionMatrix)\n",
    "    if (len(prev_.size()) == 2):\n",
    "        prev_ = prev_.unsqueeze(1)\n",
    "    prev_ = torch.sum(prev_, dim=1)\n",
    "    return torch.matmul(prev_, resolutionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T13:28:39.198620Z",
     "start_time": "2019-04-24T13:28:39.174537Z"
    }
   },
   "outputs": [],
   "source": [
    "class Summarizer(torch.nn.Module):\n",
    "    def __init__(self, bert_model = \"bert-base-uncased\"):\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.bert_width = 768\n",
    "        self.bert_model = bert_model\n",
    "        if (\"-large-\" in self.bert_model):\n",
    "            self.bert_width = 1024\n",
    "        \n",
    "        #self.bertToModel = torch.nn.Linear(768, self.bert_width)\n",
    "        self.wz = torch.nn.Parameter(torch.zeros((self.bert_width*2, self.bert_width)))\n",
    "        self.wr = torch.nn.Parameter(torch.zeros((self.bert_width*2, self.bert_width*2)))\n",
    "        self.w_cand = torch.nn.Parameter(torch.zeros((self.bert_width*4, self.bert_width)))\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.innerXAttention = torch.nn.Parameter(torch.zeros((self.bert_width, 512)))\n",
    "        \n",
    "        self.innerPrevAttention = torch.nn.Parameter(torch.zeros((30000, 512)))\n",
    "        self.prevToWidth = torch.nn.Parameter(torch.zeros((30000, self.bert_width)))\n",
    "        self.attention_weights = torch.nn.Parameter(torch.zeros((self.bert_width, self.bert_width)))\n",
    "        self.output_ = torch.nn.Parameter(torch.zeros(self.bert_width, 30000))\n",
    "        \n",
    "        self.pointer_out = torch.nn.Linear(self.bert_width * 3, 1)\n",
    "        \n",
    "    def init_hidden_state(self, size):\n",
    "        _prev_word = self.output_[101] #<- this is basically the cls marker\n",
    "        _prev_word = _prev_word.repeat(size[0], 1).unsqueeze(1)\n",
    "        return torch.empty(size).uniform_(-1,1), [_prev_word]\n",
    "    \n",
    "    def forward(self, docs, segments, masks, output_ts = 75):\n",
    "        coverages = []\n",
    "        pointers = []\n",
    "        atts = []\n",
    "        hs, generated_words = self.init_hidden_state((docs.size()[0],1, self.bert_width))\n",
    "        \n",
    "        _docs, _ = self.bert(docs, segments, masks, output_all_encoded_layers = False)\n",
    "        #_docs = self.bertToModel(_docs)\n",
    "        _docs = _docs * masks.unsqueeze(-1).float()\n",
    "        _x = InnerAttention(_docs, self.innerXAttention).unsqueeze(1)\n",
    "        \n",
    "        coverage = torch.zeros((docs.size()[0],docs.size()[1]))        \n",
    "        for i in range(output_ts):\n",
    "            #self attention and context vector generation of all previously generated words\n",
    "            _generatedContext = resolvePreviouslyGeneratedText(generated_words, \n",
    "                                                  self.innerPrevAttention, \n",
    "                                                  self.prevToWidth)\n",
    "            \n",
    "            #gru gating\n",
    "            _gru_in = torch.cat([_x, hs], dim=-1)\n",
    "            z = torch.sigmoid(torch.matmul(_gru_in, self.wz))\n",
    "            r = torch.sigmoid(torch.matmul(_gru_in, self.wr))\n",
    "            \n",
    "            #context vector generation for the doc space\n",
    "            att = dotProductAttention(_docs, hs, self.attention_weights)\n",
    "            doc_context_vector = torch.sum(_docs * att, dim=1).unsqueeze(1)\n",
    "            \n",
    "            #candidate hidden state and final hidden state for the gru\n",
    "            _cand_in = torch.cat([_gru_in*r, doc_context_vector, _generatedContext.unsqueeze(1)], dim=-1)\n",
    "            h_cand = torch.tanh(torch.matmul(_cand_in, self.w_cand))\n",
    "            hs = (1-z)*hs + z*h_cand\n",
    "            \n",
    "            #pointer architecture\n",
    "            _pointer_in = torch.cat([hs, doc_context_vector, _generatedContext.unsqueeze(1)], dim=-1)\n",
    "            pointer = self.pointer_out(_pointer_in)\n",
    "            \n",
    "            #generate the output word\n",
    "            word = torch.matmul(hs, self.output_)\n",
    "            \n",
    "            generated_words.append(word)\n",
    "            pointers.append(pointer)\n",
    "            \n",
    "            coverage = coverage + att.squeeze(-1)\n",
    "            add_cov = coverage.clone().unsqueeze(1)\n",
    "            coverages.append(add_cov)\n",
    "            atts.append(att.transpose(-2,-1))\n",
    "        \n",
    "        return torch.cat(generated_words[1:], dim=1), torch.cat(coverages, dim=1), torch.cat(pointers, dim=1).squeeze(-1), torch.cat(atts, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T13:28:39.909122Z",
     "start_time": "2019-04-24T13:28:39.899120Z"
    }
   },
   "outputs": [],
   "source": [
    "pointerCriterion = torch.nn.BCEWithLogitsLoss()\n",
    "wordCriterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def CoverageLoss(attentions, coverages):\n",
    "    \"\"\"\n",
    "    :param attentions: (b, yt, xt)\n",
    "    :param coverages: (b, yt, xt)\n",
    "    \"\"\"\n",
    "    l = torch.min(attentions, coverages) #b, yt, xt\n",
    "    l = torch.sum(l, dim=-1) #eliminate the xt\n",
    "    l = torch.sum(l, dim=-1) #eliminate the yt\n",
    "    l = torch.sum(l)/(l.size()[0]) #eliminate the b\n",
    "    return l\n",
    "\n",
    "def PointerLoss(yPointers, y_Pointers):\n",
    "    return pointerCriterion(y_Pointers, yPointers)\n",
    "\n",
    "def WordLoss(yWords, y_Words):\n",
    "    return wordCriterion(y_Words.view(-1,30000), yWords.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T13:31:45.551374Z",
     "start_time": "2019-04-24T13:28:40.569913Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = 75\n",
    "epochs = 10\n",
    "batches_per_epoch = 5\n",
    "s = Summarizer()\n",
    "optimizer = torch.optim.Adam(s.parameters(), lr=1e-3)\n",
    "\n",
    "alpha = 1.\n",
    "beta = 1.\n",
    "gamma = 1.\n",
    "\n",
    "for k in range(epochs):\n",
    "    for j in range(batches_per_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        d, se, m, su, po = genBatch(bs=16)\n",
    "        words, coverage, pointers, atts = s.forward(d, se, m, output_ts=ts)\n",
    "        l = gamma * CoverageLoss(atts, coverage)\n",
    "        l2 = beta * PointerLoss(po[:,:ts], pointers)\n",
    "        l3 = alpha * WordLoss(su[:,:ts], words)\n",
    "        total_loss = l3 + l2 + l\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        print(total_loss.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
