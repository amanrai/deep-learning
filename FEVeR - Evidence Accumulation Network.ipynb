{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T15:15:00.225333Z",
     "start_time": "2019-04-13T15:14:49.205657Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "processed_data = pickle.load(open(\"../fever_processed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep for Evidence Accumulation\n",
    "\n",
    "* The significance of a given statement as evidence to a claim/question is modelled as a classification problem\n",
    "* Any length of text (such as a sentence from a document), is appended to the claim/question in the usual way. \"CLS\" <claim/question tokens> \"SEP\" <potential/evidence tokens> \"SEP\"\n",
    "* A class is awarded to the combined string based on the following:\n",
    "    - Class 0, if the evidence tokens do not contribute to answering the question\n",
    "    - Class 1, if the evidence tokens partially answer the question\n",
    "    - Class 2, if the evidence tokens completely answer the question\n",
    "* len(claim) + len(evidence) + 3 should be <= 96 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T15:15:00.229576Z",
     "start_time": "2019-04-13T15:15:00.226955Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T15:15:50.421270Z",
     "start_time": "2019-04-13T15:15:00.231258Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "lines = []\n",
    "classes = []\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def make_data(claim, evidence):\n",
    "    _ftokens = [\"[CLS]\"]\n",
    "    _ftokens.extend(claim)\n",
    "    _ftokens.append(\"[SEP]\")\n",
    "    _ftokens.extend(evidence)\n",
    "    _ftokens.append(\"[SEP]\")\n",
    "    while(len(_ftokens) < max_len):\n",
    "        _ftokens.append(\"[PAD]\")\n",
    "    _ftokens = _ftokens[:max_len]\n",
    "    segments = np.ones((max_len,))\n",
    "    segments[:len(claim) + 2] = 0\n",
    "    tokens = tokenizer.convert_tokens_to_ids(_ftokens)\n",
    "    return (tokens, segments)\n",
    "    \n",
    "for line in processed_data:\n",
    "    for evidence in line[\"processed\"][\"evidentiary\"]:\n",
    "        lines.append(make_data(line[\"processed\"][\"claim\"], evidence))\n",
    "        if (len(line[\"processed\"][\"evidentiary\"]) == 1):\n",
    "            classes.append(1)\n",
    "        else:\n",
    "            classes.append(2)\n",
    "    for evidence in line[\"processed\"][\"non_evidentiary\"]:\n",
    "        lines.append(make_data(line[\"processed\"][\"claim\"], evidence))\n",
    "        classes.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T15:15:50.783232Z",
     "start_time": "2019-04-13T15:15:50.422998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points =  1605875\n",
      "Of which evidentiary: 201060\n",
      "Total training data points =  1445287\n",
      "Of which evidentiary: 180997\n",
      "Total testing data points =  160588\n",
      "Of which evidentiary: 20063\n"
     ]
    }
   ],
   "source": [
    "print(\"Total data points = \", len(classes))\n",
    "print(\"Of which evidentiary:\", np.count_nonzero(classes))\n",
    "\n",
    "training_lines = lines[:-len(classes)//10]\n",
    "training_classes = classes[:-len(classes)//10]\n",
    "\n",
    "print(\"Total training data points = \", len(training_classes))\n",
    "print(\"Of which evidentiary:\", np.count_nonzero(training_classes))\n",
    "\n",
    "training_evidentiary_indices = [i for i in range(len(training_classes)) if training_classes[i] > 0 ]\n",
    "training_nonevidentiary_indices = [i for i in range(len(training_classes)) if training_classes[i] == 0]\n",
    "\n",
    "testing_lines = lines[-len(classes)//10:]\n",
    "testing_classes = classes[-len(classes)//10:]\n",
    "\n",
    "print(\"Total testing data points = \", len(testing_classes))\n",
    "print(\"Of which evidentiary:\", np.count_nonzero(testing_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Fact Extraction (ReFE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T15:15:56.267034Z",
     "start_time": "2019-04-13T15:15:56.262800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T15:15:58.964692Z",
     "start_time": "2019-04-13T15:15:56.945967Z"
    }
   },
   "outputs": [],
   "source": [
    "#Aim below is to always present a balanced training set. \n",
    "\n",
    "def getTrainingBatch(bs = 64):\n",
    "    ev_total = bs // 8\n",
    "    nev_total = bs - ev_total\n",
    "    x = np.random.randint(0, len(training_evidentiary_indices), (ev_total))\n",
    "    x = np.asarray(training_evidentiary_indices)[x]\n",
    "    _base_tokens = [training_lines[index][0] for index in x]\n",
    "    _segment_tokens = [training_lines[index][1] for index in x]\n",
    "    _classes = [training_classes[index] for index in x]\n",
    "    \n",
    "    x = np.random.randint(0, len(training_nonevidentiary_indices), (nev_total))\n",
    "    x = np.asarray(training_nonevidentiary_indices)[x]\n",
    "    _base_tokens_ne = [training_lines[index][0] for index in x]\n",
    "    _segment_tokens_ne = [training_lines[index][1] for index in x]\n",
    "    _classes_ne = [training_classes[index] for index in x]\n",
    "    \n",
    "    _base_tokens.extend(_base_tokens_ne)\n",
    "    _segment_tokens.extend(_segment_tokens_ne)\n",
    "    _classes.extend(_classes_ne)\n",
    "        \n",
    "    final_seq = [i for i in range(bs)]\n",
    "    np.random.shuffle(final_seq)\n",
    "    \n",
    "    tokens = []\n",
    "    segments = []\n",
    "    classes = []\n",
    "    for index in final_seq:\n",
    "        tokens.append(_base_tokens[index])\n",
    "        segments.append(_segment_tokens[index])\n",
    "        classes.append(_classes[index])\n",
    "    \n",
    "    tokens = torch.LongTensor(tokens).cuda()\n",
    "    segments = torch.LongTensor(segments).cuda()\n",
    "    classes = torch.LongTensor(classes).cuda()\n",
    "    att_mask = tokens != 0\n",
    "    \n",
    "    return tokens, segments, att_mask, classes\n",
    "    \n",
    "tokens, segments, att_mask, classes = getTrainingBatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T15:16:01.538854Z",
     "start_time": "2019-04-13T15:16:01.529066Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReFE(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                num_bert_layers=1,\n",
    "                backprop_thru_bert=False,\n",
    "                internal_dim = 256                \n",
    "                ):\n",
    "        super(ReFE, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.out = torch.nn.Linear(768,3)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, inputs, segments, attention_masks):\n",
    "        f, _ = self.bert(inputs, \n",
    "                         token_type_ids=segments, \n",
    "                         attention_mask=attention_masks, \n",
    "                         output_all_encoded_layers=False)\n",
    "        out_ = self.out(self.dropout(f[:,0,:]))        \n",
    "        return out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T15:16:07.964348Z",
     "start_time": "2019-04-13T15:16:02.426051Z"
    }
   },
   "outputs": [],
   "source": [
    "network = ReFE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Friends dont let friends use batch sizes > 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-13T15:16:08.840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 2219 Loss: 0.63763\r"
     ]
    }
   ],
   "source": [
    "lossFn = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=3e-5)\n",
    "def Train(network, bs = 24, epochs=30, batches_per_epoch=10000):\n",
    "    epoch_losses = []\n",
    "    for k in range(epochs):\n",
    "        batch_losses = []\n",
    "        for i in range(batches_per_epoch):\n",
    "            tokens, segments, att_mask, classes = getTrainingBatch(bs=bs)\n",
    "            y_ = network.forward(tokens, segments, att_mask)\n",
    "            evidences = classes >= 1\n",
    "            non_evidences = classes == 0\n",
    "            optimizer.zero_grad()\n",
    "            loss1 = lossFn(F.log_softmax(y_[evidences], dim=-1), classes[evidences].cuda())\n",
    "            loss2 = lossFn(F.log_softmax(y_[non_evidences], dim = -1), classes[non_evidences].cuda())\n",
    "            f_loss = 0.75*loss1 + 0.25*loss2\n",
    "            batch_losses.append(f_loss.data.item())\n",
    "            print(\"Epoch:\", k+1, \n",
    "                  \"Batch:\", i+1, \n",
    "                  \"Loss:\", np.round(np.mean(batch_losses),5), \n",
    "                  end=\"\\r\")\n",
    "            f_loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_losses.append(np.mean(batch_losses))\n",
    "\n",
    "Train(network.to(\"cuda\"), bs=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
